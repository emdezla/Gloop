
# 2025-03-25 21:45:48.798662
+y

# 2025-03-25 21:56:54.957961
+/add offline_training.ipynb helpers.py

# 2025-03-25 21:57:40.141801
+Cell 4 in offline_training.ipynb is not working. Please adapt both files accordingly so that it works

# 2025-03-25 22:02:24.539649
+/exit

# 2025-03-25 22:06:32.493707
+/add offline_training.py helpers.py

# 2025-03-25 22:07:38.493400
+Create a code_documentation.md file that explains how the code works in terms of RL algorithm and neural networks, so that outsiders can understand the functionality of the code

# 2025-03-25 22:08:02.533073
+y

# 2025-03-25 22:08:10.843296
+d

# 2025-03-25 22:08:47.332742
+/exit

# 2025-03-25 22:09:29.091906
+/add offline_training.py

# 2025-03-25 22:09:38.108635
+/add helpers.py

# 2025-03-25 22:10:59.493033
+There are several problems in the code. Please rewrite the code in a more efficient way and make sure it works. Move the big loop from offline_training.py into a function that can be imported from helpers.py

# 2025-03-25 22:13:53.594735
+n

# 2025-03-25 22:15:56.819903
+y

# 2025-03-25 22:20:02.685113
+/add training_stats.csv

# 2025-03-25 22:20:26.445181
+/drop training_stats.csv

# 2025-03-25 22:20:51.955782
+/exit

# 2025-03-25 22:21:09.675741
+/add offline_training.py helpers.py

# 2025-03-25 22:21:35.613063
+/add training_stats.csv

# 2025-03-25 22:22:58.403498
+I have run offline_training but I can see from training_stats.csv that the actor loss is increasing instead of decreasing. Please have a look at training_stats.csv, try to see if some data does not male sense and propose changes to the code so that the training is succesful

# 2025-03-25 22:26:11.290771
+y

# 2025-03-25 22:35:03.793690
+/exit

# 2025-03-25 22:35:50.306816
+/add offline_training.py helpers.py training_stats.csv 

# 2025-03-25 22:37:09.985052
+The training stats are still not optimal. The actor loss is increaing and the TD loss is also increasing. Please suggest solutions. What is happening? Also, in our previous interaction you suggesed these recommendations (implement them)  2 Monitor gradient norms using TensorBoard
+ 3 Consider adding weight decay (1e-4) to critic networks
+ 4 Try increasing batch size to 512 if memory permits

# 2025-03-25 22:39:10.914298
+n

# 2025-03-25 22:39:13.250196
+y

# 2025-03-25 22:41:12.528977
+/drop

# 2025-03-25 22:41:24.170011
+/add helpers.py offline_training.py training_stats.csv

# 2025-03-25 22:43:00.464335
+Analyse training_stats.csv, paying attention to the trend of all the columns. There must be problems in the way my RL algorithm works because the actor loss is still increasing. Please fix the codeto solve this problem. Be careful and make sure that it works

# 2025-03-25 22:44:59.495989
+y

# 2025-03-25 22:47:16.777723
+/drop training_stats.csv

# 2025-03-25 22:47:21.850015
+/add training_stats.csv

# 2025-03-25 22:49:32.089234
+Analyse training_stats.csv again, paying attention to all columns. Is the RL algorithm working this time? Why CQL penalty is fixed to 5? Keep improving the code and create an offline_testing.py file that we can use to test the trained model with test data. What offline testing techniques do you propose?

# 2025-03-25 22:51:35.376824
+n

# 2025-03-25 22:53:20.163496
+Please edit the files as you suggested.

# 2025-03-25 22:55:16.345384
+y

# 2025-03-25 22:58:58.041300
+/drop training_stats.csv

# 2025-03-25 22:59:02.481580
+/add training_stats.csv

# 2025-03-25 23:00:24.194260
+Analyse training_stats.csv and provide explanations for the trends we are seeing. Propose changes in the RL algorithm or in the neural networks architecture to improve the machine learning stats

# 2025-03-25 23:01:57.800139
+y

# 2025-03-25 23:04:02.738064
+$ python offline_training.py
+Training:   0%|                                                                                                                                                                          | 0/60 [00:00<?, ?it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\offline_training.py", line 18, in <module>
+    trained_model = train_offline(
+                    ^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\helpers.py", line 396, in train_offline
+    alpha_loss.backward()
+  File "C:\Users\emili\procoding\Gloop\venv\Lib\site-packages\torch\_tensor.py", line 626, in backward
+    torch.autograd.backward(
+  File "C:\Users\emili\procoding\Gloop\venv\Lib\site-packages\torch\autograd\__init__.py", line 347, in backward
+    _engine_run_backward(
+  File "C:\Users\emili\procoding\Gloop\venv\Lib\site-packages\torch\autograd\graph.py", line 823, in _engine_run_backward
+    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

# 2025-03-25 23:06:00.288735
+y

# 2025-03-25 23:07:43.209290
+/drop training_stats.csv

# 2025-03-25 23:07:46.985862
+/add training_stats.csv

# 2025-03-25 23:08:05.794365
+Analyse training_stats.csv and provide explanations for the trends we are seeing. Propose changes in the RL algorithm or in the neural networks architecture to improve the machine learning stats if needed

# 2025-03-25 23:10:07.359609
+y

# 2025-03-25 23:13:16.857958
+/drop training_stats.csv

# 2025-03-25 23:13:24.248945
+/add training_stats.csv

# 2025-03-25 23:13:30.938772
+Analyse training_stats.csv and provide explanations for the trends we are seeing. Propose changes in the RL algorithm or in the neural networks architecture to improve the machine learning stats if needed

# 2025-03-25 23:15:48.782365
+y

# 2025-03-25 23:20:55.456407
+/drop training_stats.csv

# 2025-03-25 23:20:59.753189
+/add training_stats.csv

# 2025-03-25 23:21:04.014047
+Analyse training_stats.csv and provide explanations for the trends we are seeing. Propose changes in the RL algorithm or in the neural networks architecture to improve the machine learning stats if needed

# 2025-03-25 23:22:34.917693
+y

# 2025-03-25 23:27:31.584519
+/drop training_stats.csv

# 2025-03-25 23:27:35.390862
+/add training_stats.csv

# 2025-03-25 23:27:38.878882
+Analyse training_stats.csv and provide explanations for the trends we are seeing. Propose changes in the RL algorithm or in the neural networks architecture to improve the machine learning stats if needed

# 2025-03-25 23:29:37.848479
+y

# 2025-03-25 23:31:47.376819
+/drop training_stats.csv

# 2025-03-25 23:32:20.216285
+/add code_documentation.md 

# 2025-03-25 23:35:38.357779
+I feel like we need to take a step back because I dont think we are making any progress. Please update code_documentation and justify all the RL algorithm and NN architecture design choices. Have a critical spirit and include possible simplifications of the RL algorithm that would maybe achieve a similar result. Then create a new file basic_offline_training.py that implements those simplication to the most extreme (kaybe you can remove CQL and just keep a very simple SAC for example) but I want a working RL algorithm please.

# 2025-03-25 23:37:59.104659
+y

# 2025-03-25 23:42:19.873836
+n

# 2025-03-25 23:42:22.122352
+y

# 2025-03-25 23:48:03.208316
+/add basic_offline_training.py

# 2025-03-25 23:48:10.471377
+/drop code_documentation.md

# 2025-03-25 23:48:20.841376
+/drop helpers.py

# 2025-03-25 23:49:41.532430
+/exit

# 2025-03-25 23:50:53.104827
+/add basic_offline_training.py

# 2025-03-25 23:51:47.004588
+Do this stats make sense? Identify trends and fix the code to improve the machine learning Epoch,Iteration,TD Loss,BC Loss,Total Loss,Q Value,Action_Mean,Action_Std
+0,0,8.950296401977539,0.13066498935222626,8.963362693786621,-0.19490160048007965,-0.09380251169204712,0.07348636537790298
+0,10,8.541267681121827,0.14694472402334213,8.55596227645874,-0.267664860188961,-0.1194750800728798,0.07370817512273789
+0,20,7.850835275650025,0.1571185529232025,7.866547155380249,-0.4090842127799988,-0.1368427559733391,0.06795779317617416
+0,30,7.184277868270874,0.15595390200614928,7.199873256683349,-0.5477275669574737,-0.13409120589494705,0.06623937636613846
+0,40,6.504998731613159,0.14041101261973382,6.519039821624756,-0.7194223940372467,-0.12733580246567727,0.06929306015372276
+0,50,5.8436517238616945,0.141470268368721,5.857798671722412,-0.9025322079658509,-0.14077457338571547,0.08069369420409203
+1,0,5.360071182250977,0.12915268540382385,5.372986316680908,-1.0575841665267944,-0.15291136503219604,0.08594882488250732
+1,10,5.032812309265137,0.14611667841672898,5.047423934936523,-1.1532777786254882,-0.1458269715309143,0.08768777698278427
+1,20,4.288223505020142,0.14299944117665292,4.302523422241211,-1.395797848701477,-0.1395374447107315,0.09056402817368507
+1,30,3.6240737199783326,0.14384171068668367,3.638457918167114,-1.6597859621047975,-0.1342798575758934,0.09480489641427994
+1,40,2.9384706020355225,0.13078006803989412,2.9515486001968383,-1.9764665484428405,-0.14354704469442367,0.09721880108118057
+1,50,2.344327425956726,0.13891282975673674,2.3582186937332152,-2.302554726600647,-0.15598493963479995,0.1033067062497139
+2,0,1.970399260520935,0.13681627810001373,1.9840809106826782,-2.593576431274414,-0.15228866040706635,0.11010012030601501
+2,10,1.8622913241386414,0.14085550084710122,1.8763768792152404,-2.7119155168533324,-0.1493399664759636,0.1065354272723198
+2,20,1.490064787864685,0.1348442628979683,1.5035492181777954,-3.0698448419570923,-0.140818852186203,0.11186520233750344
+2,30,1.116623091697693,0.13840730488300323,1.1304638385772705,-3.432330584526062,-0.15125758647918702,0.1154591754078865
+2,40,1.0595729410648347,0.13282241076231002,1.0728551864624023,-3.851771426200867,-0.15037642121315004,0.11967753395438194
+2,50,0.9651105642318726,0.1290081784129143,0.9780113816261291,-4.1127478837966915,-0.1513456016778946,0.12333096712827682
+3,0,0.9770373106002808,0.14581315219402313,0.9916186332702637,-4.193244934082031,-0.15496014058589935,0.12583361566066742
+3,10,0.8155168831348419,0.12445947825908661,0.8279628157615662,-4.380940675735474,-0.15599340349435806,0.14091354757547378
+3,20,0.8248690843582154,0.12502326369285582,0.8373714029788971,-4.608437395095825,-0.15867202281951903,0.14304724782705308
+3,30,0.8047974646091461,0.1382189981639385,0.8186193645000458,-4.815942096710205,-0.15361946374177932,0.1444853127002716
+3,40,0.6974112212657928,0.13092561587691307,0.7105037808418274,-5.025177526473999,-0.13795616552233697,0.14148151278495788
+3,50,0.7011082708835602,0.1277014784514904,0.7138784170150757,-5.269627571105957,-0.16027760207653047,0.1391115218400955
+4,0,0.857384204864502,0.144005686044693,0.8717847466468811,-5.501716613769531,-0.16749492287635803,0.13058514893054962
+4,10,0.7153849124908447,0.12435372546315193,0.7278202772140503,-5.59095458984375,-0.1516701340675354,0.1413308247923851
+4,20,0.7308351993560791,0.12523750066757203,0.743358951807022,-5.819942140579224,-0.1432180166244507,0.14761483073234558
+4,30,0.6317923486232757,0.13094322085380555,0.6448866665363312,-5.956050205230713,-0.15873595029115678,0.15183811634778976
+4,40,0.6217978000640869,0.12276294752955437,0.6340741008520127,-6.234735584259033,-0.1533759817481041,0.14923710227012635
+4,50,0.6102848321199417,0.12723550647497178,0.6230083912611007,-6.442995023727417,-0.15176159739494324,0.1485717177391052
+5,0,0.7295333743095398,0.13244697451591492,0.7427780628204346,-6.6436896324157715,-0.14888018369674683,0.14823713898658752
+5,10,0.6474842607975007,0.12929039150476457,0.6604132890701294,-6.794244718551636,-0.1475772440433502,0.15541268289089202
+5,20,0.6631000310182571,0.1269736744463444,0.6757973968982697,-6.993229675292969,-0.13418593108654023,0.15141348093748092
+5,30,0.6087394773960113,0.12185336425900459,0.6209248214960098,-7.215664100646973,-0.1627127468585968,0.15866373479366302
+5,40,0.6749951541423798,0.11508944183588028,0.6865040957927704,-7.455690526962281,-0.175639009475708,0.16065984666347505
+5,50,0.7746717184782028,0.1271228902041912,0.7873840093612671,-7.726993370056152,-0.13807272389531136,0.16056414991617202
+6,0,1.224534511566162,0.10528790950775146,1.2350633144378662,-7.640315532684326,-0.1379505693912506,0.15739014744758606
+6,10,0.6750634640455246,0.1250602424144745,0.6875694870948792,-7.9062823295593265,-0.15264563262462616,0.1631227657198906
+6,20,0.7242922157049179,0.12153729200363159,0.7364459455013275,-8.222848224639893,-0.1555650368332863,0.16594089269638063
+6,30,0.7828684270381927,0.12131381332874298,0.7949997961521149,-8.485133266448974,-0.1563224747776985,0.16322925835847854
+6,40,0.6389003664255142,0.12596368193626403,0.6514967322349549,-8.719405364990234,-0.14741723239421844,0.16236536651849748
+6,50,0.7513089835643768,0.12226616963744164,0.7635355949401855,-8.949606895446777,-0.150090728700161,0.1658017337322235
+7,0,1.144338846206665,0.1290285587310791,1.157241702079773,-9.088140487670898,-0.14536353945732117,0.15661969780921936
+7,10,0.8208917945623397,0.120953718572855,0.8329871743917465,-9.220771503448486,-0.14642932415008544,0.16357698887586594
+7,20,0.6446791619062424,0.12098570913076401,0.6567777305841446,-9.467544555664062,-0.15270159393548965,0.1631649389863014
+7,30,0.7751643806695938,0.12055736184120178,0.7872201174497604,-9.690361213684081,-0.1519227609038353,0.17209436297416686
+7,40,0.7415052652359009,0.11770271584391594,0.7532755345106125,-9.975134086608886,-0.15745682567358016,0.17019924223423005
+7,50,0.8388285279273987,0.12704568952322007,0.8515330851078033,-10.25555772781372,-0.15535419881343843,0.16483601182699203
+8,0,0.8266245126724243,0.11352953314781189,0.8379774689674377,-10.292396545410156,-0.1441000998020172,0.17177999019622803
+8,10,0.8681661665439606,0.11711903586983681,0.8798780828714371,-10.470866394042968,-0.1466100037097931,0.16573806554079057
+8,20,0.6591612130403519,0.1210943840444088,0.6712706476449967,-10.706106662750244,-0.15402466356754302,0.16908998191356658
+8,30,0.9779401600360871,0.12306515201926231,0.9902466833591461,-11.016128253936767,-0.15427399277687073,0.16943850964307786
+8,40,0.9022465467453002,0.11821319237351417,0.9140678644180298,-11.190534496307373,-0.14929135143756866,0.16803182661533356
+8,50,0.8724325656890869,0.12258795201778412,0.8846913695335388,-11.519298553466797,-0.1567289873957634,0.17204443514347076
+9,0,0.7835699915885925,0.12535488605499268,0.7961055040359497,-11.67227554321289,-0.13972769677639008,0.17687495052814484
+9,10,0.6666890472173691,0.11656493470072746,0.6783455371856689,-11.754414939880371,-0.14576046913862228,0.16862535178661348
+9,20,0.8203152626752853,0.12350052371621131,0.8326653271913529,-12.001883029937744,-0.15431361198425292,0.17397976517677308
+9,30,1.0422940313816071,0.11990151181817055,1.0542841911315919,-12.278933238983154,-0.14304047524929048,0.17066936194896698
+9,40,0.969949996471405,0.12304861918091774,0.9822548538446426,-12.436337661743163,-0.14616430401802064,0.16729280948638917
+9,50,1.2058300405740738,0.1155391976237297,1.2173839658498764,-12.796424674987794,-0.1496364176273346,0.171675643324852
+10,0,0.468056321144104,0.1197182685136795,0.4800281524658203,-12.814544677734375,-0.14702008664608002,0.1738845705986023
+10,10,0.9658135563135147,0.12279975265264512,0.9780935376882554,-12.965559768676759,-0.14875103831291198,0.17494605630636215
+10,20,0.8943901270627975,0.1163235180079937,0.9060224801301956,-13.323575878143311,-0.14218066707253457,0.17916036248207093
+10,30,1.1214757770299912,0.11861218512058258,1.1333369761705399,-13.521122550964355,-0.16378433108329774,0.17369164973497392
+10,40,0.865828862786293,0.12029867172241211,0.8778587251901626,-13.818437385559083,-0.15615200400352477,0.1712891936302185
+10,50,1.3362539380788803,0.11818799674510956,1.3480727285146714,-14.032256698608398,-0.14267957657575608,0.17678477764129638
+11,0,0.4110890328884125,0.13304105401039124,0.4243931472301483,-14.114274978637695,-0.1514085829257965,0.1760932356119156
+11,10,1.0579685777425767,0.11792177110910415,1.0697607457637788,-14.373380947113038,-0.15310686975717544,0.17642529755830766
+11,20,1.1561798930168152,0.11546486094594002,1.1677263855934144,-14.610587310791015,-0.15558137893676757,0.17881989628076553
+11,30,0.9274028897285461,0.12006163373589515,0.9394090473651886,-14.82845516204834,-0.1539518192410469,0.17345510125160218
+11,40,1.5938778936862945,0.12074449509382248,1.6059523463249206,-15.010893440246582,-0.15604315996170043,0.17190211713314058
+11,50,0.9311866343021393,0.11746912524104118,0.9429335296154022,-15.32615795135498,-0.1444791465997696,0.17122444063425063
+12,0,1.0907578468322754,0.10896173864603043,1.101654052734375,-15.54450511932373,-0.14481906592845917,0.1782616525888443
+12,10,1.0395623356103898,0.1152447834610939,1.0510868191719056,-15.66469087600708,-0.1470668599009514,0.17606099992990493
+12,20,0.9164648920297622,0.11455992236733437,0.9279208868741989,-15.832586288452148,-0.15814174264669417,0.17525174617767333
+12,30,1.5362354189157486,0.11827481612563133,1.5480629056692123,-16.188892555236816,-0.16240642219781876,0.17932456582784653
+12,40,1.5552095830440522,0.1188333623111248,1.5670928925275802,-16.323422813415526,-0.15238492786884308,0.17645401805639266
+12,50,1.264551517367363,0.12242059856653213,1.2767935782670974,-16.55775718688965,-0.1436084181070328,0.1782798573374748
+13,0,0.5115490555763245,0.1467810869216919,0.5262271761894226,-16.641159057617188,-0.14442037045955658,0.1748255491256714
+13,10,1.6030009478330611,0.11439616978168488,1.6144405275583267,-16.956199073791502,-0.14723986238241196,0.1757332742214203
+13,20,1.2810525447130203,0.1173654966056347,1.2927890747785569,-17.160214614868163,-0.15370470732450486,0.1785050705075264
+13,30,1.1779354333877563,0.11473594978451729,1.189409014582634,-17.378949546813963,-0.15434414595365525,0.17901498079299927
+13,40,1.4722426921129226,0.11768914237618447,1.484011608362198,-17.605879020690917,-0.14899091720581054,0.18125382363796233
+13,50,1.4691619724035263,0.12091796845197678,1.481253743171692,-17.92014961242676,-0.1493370622396469,0.1809677243232727
+14,0,1.58400559425354,0.12412189692258835,1.596417784690857,-17.990293502807617,-0.16380852460861206,0.18342000246047974
+14,10,2.018328958749771,0.11674358695745468,2.0300033509731295,-18.11725444793701,-0.14953298717737198,0.1803433209657669
+14,20,1.7160124391317368,0.11984412744641304,1.727996852993965,-18.38062572479248,-0.14340087473392488,0.17981548309326173
+14,30,0.6840183645486831,0.10904487743973731,0.6949228405952453,-18.646673965454102,-0.15310909748077392,0.17729941457509996
+14,40,1.5009327977895737,0.11708901077508926,1.5126416951417923,-18.996517944335938,-0.15773859322071077,0.1805223435163498
+14,50,1.6873329997062683,0.11985519006848336,1.699318516254425,-19.16788673400879,-0.15025270134210586,0.17893807590007782
+15,0,0.3930297791957855,0.10620374977588654,0.403650164604187,-19.40721893310547,-0.16019153594970703,0.17776240408420563
+15,10,1.7984725177288055,0.11352053210139275,1.8098245590925217,-19.435100173950197,-0.14800001978874205,0.18104129135608674
+15,20,1.9800689935684204,0.1175239346921444,1.991821402311325,-19.625704956054687,-0.1547419011592865,0.1763349562883377
+15,30,1.1781872808933258,0.12117599099874496,1.1903048932552338,-20.051853752136232,-0.1529953435063362,0.1855720430612564
+15,40,1.8327947854995728,0.1156035229563713,1.8443551063537598,-20.173206520080566,-0.1446732833981514,0.18294016569852828
+15,50,1.7807722449302674,0.11583372727036476,1.7923556298017502,-20.45310688018799,-0.1556400939822197,0.18414208441972732
+16,0,3.376401662826538,0.10577639937400818,3.386979341506958,-20.732545852661133,-0.16761068999767303,0.1814974844455719
+16,10,1.8152904689311982,0.1199672132730484,1.8272871762514113,-20.63254508972168,-0.1523662567138672,0.18334739953279494
+16,20,2.098991283774376,0.11270400881767273,2.1102616816759108,-20.979433822631837,-0.1447292074561119,0.18289246857166291
+16,30,2.198545590043068,0.11423601508140564,2.209969201683998,-21.162478065490724,-0.15396131128072738,0.18219543546438216
+16,40,0.8819159805774689,0.12382862716913223,0.8942988634109497,-21.495058059692383,-0.15071755796670913,0.18066100627183915

# 2025-03-25 23:53:07.009179
+y

# 2025-03-26 00:08:11.926844
+I think the training is not working correctly because the stats converge since the epoch 100 and I dont notice anymore changes 107,30,25.814393930137157,0.0844486527144909,25.84515034109354,-60.43198986053467,-0.15326178148388864,0.2473332792520523
+107,40,16.86539780944586,0.09014859944581985,16.898229686915876,-60.389649391174316,-0.1592405155301094,0.2426484763622284
+107,50,19.382991361618043,0.08941775411367417,19.415557272732258,-60.480018615722656,-0.13719189912080765,0.24621674716472625
+108,0,24.856454849243164,0.10564108937978745,24.89499282836914,-60.57416915893555,-0.13257451355457306,0.256681352853775
+108,10,11.871536161005498,0.08330670148134231,11.901926298439502,-60.53645896911621,-0.15046445280313492,0.24626195132732392
+108,20,22.655588373541832,0.08766434267163277,22.687568524479865,-60.54848766326904,-0.1478385016322136,0.24265104681253433
+108,30,15.731967632472514,0.08549199625849724,15.76315509825945,-60.50073432922363,-0.14682523906230927,0.24814409464597703
+108,40,34.06713416203856,0.08948609530925751,34.099778186529875,-60.572014045715335,-0.14766775518655778,0.25280075520277023
+108,50,22.25163892954588,0.09182531088590622,22.285136684775352,-60.47613410949707,-0.15641362369060516,0.25488794445991514
+109,0,47.83441925048828,0.07858298718929291,47.86313247680664,-60.45003700256348,-0.16733022034168243,0.25844189524650574
+109,10,5.023276913166046,0.08444873690605163,5.0541344925761225,-60.49300270080566,-0.16333532482385635,0.23731517046689987
+109,20,29.292863654345275,0.0851468339562416,29.32397662848234,-60.5850133895874,-0.14817530661821365,0.24440036714076996
+109,30,18.329199296236037,0.09163782894611358,18.362683339416982,-60.54346599578857,-0.14053990989923476,0.24674080163240433
+109,40,32.01566059887409,0.08564590960741043,32.0469558686018,-60.49607124328613,-0.14946429580450057,0.24329936057329177
+109,50,16.513313960283995,0.08724712505936623,16.545194108784198,-60.52059078216553,-0.15236736387014388,0.24633748084306717
+110,0,0.3119219243526459,0.09892208874225616,0.34812742471694946,-60.57286262512207,-0.14821235835552216,0.23640181124210358
+110,10,21.18264347463846,0.08284662142395974,21.21296580210328,-60.559678077697754,-0.15353256911039354,0.24688381105661392498,10,15.536858646571636,0.07085902653634549,15.579288855195045,-61.13438129425049,-0.14305489584803582,0.28237380981445315
+498,20,24.4765890404582,0.07624302953481674,24.522243300080298,-61.17610626220703,-0.16630825847387315,0.27772184610366824
+498,30,10.568601067364217,0.064016929641366,10.606934371590615,-61.15374183654785,-0.15879391580820085,0.27659052014350893
+498,40,17.801031228899955,0.06736046373844147,17.841366784274577,-61.16569595336914,-0.16527659595012664,0.2719685137271881
+498,50,19.436451475322247,0.06853189766407013,19.477488598227502,-61.17786407470703,-0.16053328514099122,0.2822564601898193
+499,0,109.41374969482422,0.06491521000862122,109.4526596069336,-61.058448791503906,-0.1510874480009079,0.25881877541542053
+499,10,21.764549255371094,0.0675486471503973,21.805037711560725,-61.113159370422366,-0.13501738905906677,0.27607733607292173
+499,20,10.652670694887638,0.0690606463700533,10.694065798819064,-61.15472259521484,-0.15921604186296462,0.27426445484161377
+499,30,21.501647470891477,0.06451660841703415,21.540318578481674,-61.09954395294189,-0.14019627571105958,0.27156794518232347
+499,40,13.520855244249105,0.07296685129404068,13.564591582119466,-61.19746131896973,-0.18034228086471557,0.27608913481235503
+499,50,13.218786080181598,0.07156323045492172,13.26168096959591,-61.181709098815915,-0.1707102134823799,0.2844369143247604

# 2025-03-26 00:12:11.264083
+y

# 2025-03-26 00:21:58.707944
+/coder

# 2025-03-26 00:22:00.778771
+/code

# 2025-03-26 00:22:20.696788
+/drop

# 2025-03-26 00:22:26.401174
+/add .gitignore

# 2025-03-26 00:22:42.164365
+include the pth files in the gitignore. 

# 2025-03-26 00:23:15.878963
+/exit

# 2025-03-26 00:25:48.968702
+/add notebooks/sacRLH4

# 2025-03-26 00:26:22.985248
+/add helpers.py

# 2025-03-26 00:34:09.201901
+Following the guidelines below and the code from the files as examples, create a new file SAC_training.py that takes a DiabetesDataset as data and trains a SAC model for reinforcement learning. To implement your custom SAC model following the G2P2C architecture and parameters, use the following guidelines:
+
+Network Architecture
+Feature Extractors (EΠ and EV):
+
+Single-layer LSTM with 16 hidden units (processes a 12-step state history).
+
+Actor Network (Πθ):
+
+After EΠ, use 3 dense layers (32 units each, ReLU).
+
+Output mean (unbounded, no activation) and log standard deviation (unbounded, no activation).
+
+Apply an invertible squashing function (tanh) to sampled actions (per Haarnoja et al.).
+
+Critic Networks (Qϕ1, Qϕ2 and target networks):
+
+After EV, concatenate the LSTM output with the action.
+
+Process through 3 dense layers (32 units each, ReLU).
+
+Output a single Q-value (linear activation).
+
+Use two Q-networks and two target networks.
+
+Key SAC-Specific Details
+Unbounded Gaussian Policy: Sample actions from the Gaussian distribution before squashing with tanh.
+
+Entropy Coefficient: Initialize to 0.1 (adjust if using automatic tuning).
+
+Dual Q-Networks: Train two critics to mitigate overestimation bias.
+
+Hyperparameters
+Parameter	Value
+Replay buffer size	100,000
+Discount factor (γ)	0.997
+Batch size	256
+Learning rate	3  10
+Target smoothing (τ)	0.005
+Target update interval	Every 1 step
+Gradient steps per update	1
+Optimizer	Adam
+Initial entropy coefficient	0.1
+Implementation Steps
+Actor:
+
+LSTM (16 units)  3  Dense (32, ReLU)  Output mean/log_std.
+
+Apply tanh squashing to actions post-sampling.
+
+Critics:
+
+For each Q-network: LSTM (16 units)  Concatenate action  3  Dense (32, ReLU)  Q-value.
+
+Training:
+
+Update critics using MSE loss on TD targets.
+
+Update actor to maximize Q-value + entropy term.
+
+Update targets via Polyak averaging (τ = 0.005).
+
+Code Skeleton (PyTorch)
+python
+Copy
+import torch
+import torch.nn as nn
+import torch.optim as optim
+
+class Actor(nn.Module):
+    def __init__(self, state_dim, action_dim):
+        super().__init__()
+        self.lstm = nn.LSTM(state_dim, 16, batch_first=True)
+        self.dense = nn.Sequential(
+            nn.Linear(16, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU()
+        )
+        self.mean = nn.Linear(32, action_dim)
+        self.log_std = nn.Linear(32, action_dim)
+
+    def forward(self, state):
+        lstm_out, _ = self.lstm(state)
+        features = self.dense(lstm_out[:, -1, :])  # Use last LSTM output
+        mean = self.mean(features)
+        log_std = self.log_std(features)
+        return mean, log_std
+
+class Critic(nn.Module):
+    def __init__(self, state_dim, action_dim):
+        super().__init__()
+        self.lstm = nn.LSTM(state_dim, 16, batch_first=True)
+        self.dense = nn.Sequential(
+            nn.Linear(16 + action_dim, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 1)
+        )
+
+    def forward(self, state, action):
+        lstm_out, _ = self.lstm(state)
+        state_features = lstm_out[:, -1, :]
+        x = torch.cat([state_features, action], dim=1)
+        return self.dense(x)
+
+# Initialize networks
+actor = Actor(state_dim, action_dim)
+critic1 = Critic(state_dim, action_dim)
+critic2 = Critic(state_dim, action_dim)
+target_critic1 = Critic(state_dim, action_dim)
+target_critic2 = Critic(state_dim, action_dim)
+
+# Optimizers
+actor_optim = optim.Adam(actor.parameters(), lr=3e-4)
+critic_optim = optim.Adam(
+    list(critic1.parameters()) + list(critic2.parameters()), lr=3e-4
+)
+Notes
+Clip gradients if necessary for stability.
+
+Use a replay buffer to store transitions.
+
+Apply tanh to actions before executing in the environment.

# 2025-03-26 00:37:11.475638
+y

# 2025-03-26 00:41:35.015984
+/drop

# 2025-03-26 00:41:46.592046
+/exit

# 2025-03-26 00:42:12.392389
+/add SAC_training.py

# 2025-03-26 00:42:17.103373
+/add helpers.py

# 2025-03-26 00:43:44.047234
+/add offline_training.py

# 2025-03-26 00:44:11.754317
+create sac_pipeline.py that I can call to train a model using data from a csv file (similar as in offline_training.py)

# 2025-03-26 00:46:09.897522
+n

# 2025-03-26 00:46:11.799583
+y

# 2025-03-26 00:51:48.767560
+/add sac_pipeline.py

# 2025-03-26 00:52:32.079445
+I dont want any CLI argument on sac_pipeline.py. Include everything on the actual script, in the same way that it is included in offline_training.py

# 2025-03-26 00:53:55.153253
+y

# 2025-03-26 00:59:56.758118
+emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio)
+$ python SAC_pipeline.py 
+
+ Starting SAC training with configuration:
+  dataset_path   : datasets/processed/563-train.csv
+  epochs         : 1000
+  batch_size     : 256
+  save_path      : sac_final.pth
+  device         : cpu
+  checkpoint_freq: 100
+  hidden_size    : 16
+  action_scale   : 1.0
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 50, in <module>
+    main()
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 29, in main
+    os.makedirs(os.path.dirname(config["save_path"]), exist_ok=True)
+  File "<frozen os>", line 225, in makedirs
+FileNotFoundError: [WinError 3] El sistema no puede encontrar la ruta especificada: ''

# 2025-03-26 01:00:58.110107
+y

# 2025-03-26 01:02:33.100308
+where can i say the training log? emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio)
+$ python SAC_pipeline.py 
+
+ Starting SAC training with configuration:
+  dataset_path   : datasets/processed/563-train.csv
+  epochs         : 1000
+  batch_size     : 256
+  save_path      : sac_final.pth
+  device         : cpu
+  checkpoint_freq: 100
+  hidden_size    : 16
+  action_scale   : 1.0
+Filling replay buffer with dataset...
+100%|| 52/52 [00:00<00:00, 259.69it/s]
+Training SAC agent for 1000 epochs...
+ 10%|                                                                                                                                                        | 97/1000 [00:01<00:10, 85.59it/s]Epoch 100/1000
+  Critic Loss: 7.5419
+  Actor Loss: -0.8815
+  Alpha: 0.9687
+ 10%|                                                                                                                                                        | 99/1000 [00:01<00:11, 81.07it/s] 
+
+ Training failed: [WinError 3] El sistema no puede encontrar la ruta especificada: ''
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 52, in <module>
+    main()
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 35, in main
+    trained_agent = train_sac_offline(
+                    ^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\SAC_training.py", line 296, in train_sac_offline
+    agent.save(f"{save_path}_epoch{epoch+1}.pt")
+  File "C:\Users\emili\procoding\Gloop\SAC_training.py", line 217, in save
+    os.makedirs(os.path.dirname(path), exist_ok=True)
+  File "<frozen os>", line 225, in makedirs
+FileNotFoundError: [WinError 3] El sistema no puede encontrar la ruta especificada: ''
+(venv) 

# 2025-03-26 01:07:31.811210
+/add sac_pipeline.py

# 2025-03-26 01:07:44.937368
+emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio)
+$ python SAC_pipeline.py 
+
+ Starting SAC training with configuration:
+  dataset_path   : datasets/processed/563-train.csv
+  epochs         : 1000
+  batch_size     : 256
+  save_path      : sac_final.pth
+  device         : cpu
+  checkpoint_freq: 100
+  hidden_size    : 16
+  action_scale   : 1.0
+Filling replay buffer with dataset...
+100%|| 52/52 [00:00<00:00, 248.85it/s]
+Training SAC agent for 1000 epochs...
+ 10%|                                                                                                                                                                             | 98/1000 [00:01<00:10, 88.82it/s]Epoch 100/1000
+  Critic Loss: 10.2795
+  Actor Loss: -0.6218
+  Alpha: 0.9669
+ 10%|                                                                                                                                                                             | 99/1000 [00:01<00:10, 86.36it/s]
+
+ Training failed: [WinError 3] El sistema no puede encontrar la ruta especificada: ''
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 52, in <module>
+    main()
+  File "C:\Users\emili\procoding\Gloop\SAC_pipeline.py", line 35, in main
+    trained_agent = train_sac_offline(
+                    ^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\SAC_training.py", line 296, in train_sac_offline
+    agent.save(f"{save_path}_epoch{epoch+1}.pt")
+  File "C:\Users\emili\procoding\Gloop\SAC_training.py", line 217, in save
+    os.makedirs(os.path.dirname(path), exist_ok=True)
+  File "<frozen os>", line 225, in makedirs
+FileNotFoundError: [WinError 3] El sistema no puede encontrar la ruta especificada: ''

# 2025-03-26 01:07:48.126569
+n

# 2025-03-26 01:08:57.023941
+y

# 2025-03-26 18:41:47.810758
+create two scripts that I can call (without any CLI argument) to train and test a simple SAC RL agent following this blueprint ---
+
+### **Complete Blueprint for SAC Implementation**  
+**Framework**: PyTorch  
+**Algorithm**: Soft Actor-Critic (SAC)  
+**Assumptions**:  
+- CSV contains pre-engineered temporal features (e.g., `glu_d`, `glu_t`).  
+- **Action space**: Continuous `[-1, 1]` (mapped to insulin rates via ` = 5  ^(4(1))`).  
+- **Reward**: Based on Risk Index (RI) with hypoglycemia penalty.  
+
+---
+
+### **1. Data Preprocessing**
+#### **CSV Columns**:
+| Column    | Description                          | Usage          |
+|-----------|--------------------------------------|----------------|
+| `glu_raw` | Raw glucose (mg/dL)                 | Compute reward |
+| `basal`   | Normalized basal action (historical) | Action input   |
+| `bolus`   | Normalized bolus action (historical) | Action input   |
+
+#### **Steps**:
+1. **Load CSV**:  
+   ```python
+   import pandas as pd
+   df = pd.read_csv("your_data.csv")
+   ```
+
+2. **Define State, Action, Reward**:  
+   ```python
+   # State: Engineered features (8D)
+   state_columns = ['glu', 'glu_d', 'glu_t', 'hr', 'hr_d', 'hr_t', 'iob', 'hour']
+   states = df[state_columns].values  # Shape: [num_samples, 8]
+
+   # Action: Normalized basal/bolus (from CSV)
+   actions = df[['basal', 'bolus']].values  # Shape: [num_samples, 2]
+
+   # Reward: Compute based on next glucose value
+   def calculate_reward(next_glucose_mgdl: float) -> float:
+       if next_glucose_mgdl <= 39:
+           return -15.0
+       else:
+           ri = calculate_risk_index(next_glucose_mgdl)  # Implement RI from [75]
+           return -1.0 * ri  # Normalized to [0, -1]
+
+   # Compute rewards for all transitions
+   rewards = [calculate_reward(df['glu_raw'].iloc[i+1]) for i in range(len(df)-1)]
+   rewards.append(0.0)  # Handle last row
+   dones = df['done'].values
+   ```
+
+---
+
+### **2. Replay Buffer**
+Store transitions `(state, action, reward, next_state, done)` for offline training:  
+```python
+class ReplayBuffer:
+    def __init__(self, max_size=100_000):
+        self.buffer = deque(maxlen=max_size)
+    
+    def add(self, state, action, reward, next_state, done):
+        self.buffer.append((state, action, reward, next_state, done))
+    
+    def sample(self, batch_size):
+        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
+        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])
+        return (
+            torch.FloatTensor(states),
+            torch.FloatTensor(actions),
+            torch.FloatTensor(rewards),
+            torch.FloatTensor(next_states),
+            torch.FloatTensor(dones)
+        )
+
+# Load data into buffer
+buffer = ReplayBuffer()
+for i in range(len(df) - 1):
+    buffer.add(states[i], actions[i], rewards[i], states[i+1], dones[i])
+```
+
+---
+
+### **3. Network Architecture**
+#### **Actor** (Gaussian Policy):  
+```python
+class Actor(nn.Module):
+    def __init__(self, state_dim=8, action_dim=2):
+        super().__init__()
+        self.net = nn.Sequential(
+            nn.Linear(state_dim, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU()
+        )
+        self.mean = nn.Linear(32, action_dim)
+        self.log_std = nn.Linear(32, action_dim)
+
+    def forward(self, state):
+        x = self.net(state)
+        mean = self.mean(x)  # Unbounded
+        log_std = self.log_std(x)
+        return mean, log_std  # Squash with tanh during sampling
+```
+
+#### **Critic** (Twin Q-Networks):  
+```python
+class Critic(nn.Module):
+    def __init__(self, state_dim=8, action_dim=2):
+        super().__init__()
+        self.q1 = nn.Sequential(
+            nn.Linear(state_dim + action_dim, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 1)
+        )
+        self.q2 = nn.Sequential(
+            nn.Linear(state_dim + action_dim, 32), nn.ReLU(),
+            nn.Linear(32, 32), nn.ReLU(),
+            nn.Linear(32, 1)
+        )
+
+    def forward(self, state, action):
+        x = torch.cat([state, action], dim=1)
+        return self.q1(x), self.q2(x)
+```
+
+---
+
+### **4. SAC Training Loop**
+#### **Hyperparameters**:
+| Parameter                  | Value           |
+|----------------------------|-----------------|
+| Batch size                 | 256             |
+| Learning rate              | 3  10        |
+| Discount (γ)               | 0.997           |
+| Target smoothing (τ)       | 0.005           |
+| Entropy coefficient (α)    | 0.1             |
+
+#### **Code**:
+```python
+# Initialize networks
+actor = Actor()
+critic = Critic()
+target_critic = Critic()
+target_critic.load_state_dict(critic.state_dict())
+
+# Optimizers
+actor_optim = optim.Adam(actor.parameters(), lr=3e-4)
+critic_optim = optim.Adam(critic.parameters(), lr=3e-4)
+
+# Loss
+mse_loss = nn.MSELoss()
+
+for epoch in range(800_000 // 256):  # Total steps = 800,000
+    # Sample batch
+    states, actions, rewards, next_states, dones = buffer.sample(256)
+    
+    # Critic Update (Twin Q-learning)
+    with torch.no_grad():
+        next_actions_mean, next_actions_log_std = actor(next_states)
+        next_actions = next_actions_mean + torch.randn_like(next_actions_mean) * torch.exp(next_actions_log_std)
+        next_actions = torch.tanh(next_actions)  # Squash to [-1, 1]
+        target_q1, target_q2 = target_critic(next_states, next_actions)
+        target_q = torch.min(target_q1, target_q2)
+        target = rewards + (1 - dones) * 0.997 * target_q.squeeze()
+    
+    current_q1, current_q2 = critic(states, actions)
+    critic_loss = mse_loss(current_q1.squeeze(), target) + mse_loss(current_q2.squeeze(), target)
+    critic_optim.zero_grad()
+    critic_loss.backward()
+    critic_optim.step()
+    
+    # Actor Update (Maximize Q + Entropy)
+    actions_mean, actions_log_std = actor(states)
+    actions_sampled = actions_mean + torch.randn_like(actions_mean) * torch.exp(actions_log_std)
+    actions_squashed = torch.tanh(actions_sampled)  # [-1, 1]
+    q1, q2 = critic(states, actions_squashed)
+    q = torch.min(q1, q2)
+    actor_loss = -q.mean() + 0.1 * (actions_log_std + 0.5 * (1 + 1e-6 - actions_sampled**2)).mean()
+    actor_optim.zero_grad()
+    actor_loss.backward()
+    actor_optim.step()
+    
+    # Target Update (Polyak Averaging)
+    for param, target_param in zip(critic.parameters(), target_critic.parameters()):
+        target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)
+```
+
+---
+
+### **5. Deployment**  
+Convert SAC actions to insulin rates using:  
+```python
+def action_to_insulin(a: torch.Tensor) -> torch.Tensor:
+    """Maps normalized action [-1, 1] to insulin rate [0, 5] U/min."""
+    I_max = 5.0
+    eta = 4.0
+    return I_max * torch.exp(eta * (a - 1))  # Shape: [batch_size, 2]
+
+# Example usage:
+state = torch.FloatTensor(current_state)  # Shape: [1, 8]
+mean, log_std = actor(state)
+action_sampled = mean + torch.randn_like(mean) * torch.exp(log_std)
+action_squashed = torch.tanh(action_sampled)  # [-1, 1]
+insulin_rates = action_to_insulin(action_squashed)  # [basal_rate, bolus_rate]
+```
+
+---
+
+### **Key Notes**  
+1. **Reward Function**: Implement `calculate_risk_index()` using references [75] and [29].  
+2. **Action Space**: Actor outputs `[-1, 1]`, transformed to insulin rates during deployment.  
+3. **LSTM-Free**: Temporal features (e.g., `glu_d`) replace sequence modeling.  
+4. **Offline Training**: Replay buffer pre-loaded with CSV transitions.  
+
+This blueprint aligns with your CSV structure, reward function, and action space.

# 2025-03-26 18:44:00.372512
+y

# 2025-03-26 18:44:17.202103
+n

# 2025-03-26 18:49:23.088854
+/exit

# 2025-03-26 18:53:38.595186
+/add offline_training.py

# 2025-03-26 18:53:55.858468
+/add SAC_training.py

# 2025-03-26 18:55:05.907001
+Below you can find the complex training stats, they dont seem right. Please modify the code accordingly so that the complex training is effective and all the training markers behave correctly without exploding Epoch,Iteration,TD Loss,CQL Penalty,Critic Loss,Actor Loss,Q1 Value,Q2 Value,Action_Mean,Action_Std,Entropy,Alpha,Alpha_Loss,Q1_Grad,Q2_Grad,Actor_Grad
+0,0,5.07425594329834,0.1765068918466568,5.07567834854126,0.796574592590332,0.025891216471791267,-0.05120444670319557,-0.02327761799097061,0.4707161784172058,0.10000000149011612,7.389056205749512,1.7999999523162842,2.177618980407715,2.1341969966888428,0.025263074785470963
+1,0,5.023379325866699,0.18611003458499908,5.024881839752197,0.825163722038269,-0.006034631282091141,-0.07932865619659424,0.002876142505556345,0.5975875854492188,0.10000000149011612,7.383884906768799,1.7993698120117188,2.1720786094665527,2.1283557415008545,0.012879756279289722
+2,0,4.984624862670898,0.18719857931137085,4.986143589019775,0.8551857471466064,-0.03857855126261711,-0.10979001969099045,0.03348277881741524,0.5943577885627747,0.10000000149011612,7.378716945648193,1.7987397909164429,2.1700599193573,2.127427339553833,0.01632099226117134
+3,0,4.947768211364746,0.19966498017311096,4.949400901794434,0.8905980587005615,-0.07562603801488876,-0.14477859437465668,0.009815610945224762,0.5316969752311707,0.10000000149011612,7.3735527992248535,1.7981096506118774,2.2066524028778076,2.1669981479644775,0.015453548170626163
+4,0,4.923702239990234,0.211983785033226,4.925451755523682,0.9236114025115967,-0.10806258022785187,-0.17796897888183594,0.04276447743177414,0.45360472798347473,0.10000000149011612,7.368392467498779,1.797479510307312,2.1888067722320557,2.15267014503479,0.012884831987321377
+5,0,4.894536018371582,0.23566186428070068,4.896503448486328,0.9606046080589294,-0.14740446209907532,-0.21458373963832855,0.05996514856815338,0.3441781997680664,0.10000000149011612,7.3632354736328125,1.7968494892120361,2.1998252868652344,2.1660821437835693,0.015097687020897865
+6,0,4.860405921936035,0.2500494718551636,4.862527847290039,1.0033917427062988,-0.19420760869979858,-0.2567979693412781,0.08972954005002975,0.23657868802547455,0.10000000149011612,7.358081817626953,1.7962193489074707,2.227179527282715,2.1913814544677734,0.013697375543415546
+7,0,4.826930522918701,0.30576834082603455,4.829559803009033,1.056746006011963,-0.24605314433574677,-0.30839991569519043,0.10177957266569138,0.1665390133857727,0.10000000149011612,7.352932453155518,1.7955892086029053,2.2713966369628906,2.2357966899871826,0.010813705623149872
+8,0,4.788684844970703,0.327506959438324,4.791563987731934,1.1112653017044067,-0.3049921691417694,-0.3621317744255066,0.10862262547016144,0.14118434488773346,0.10000000149011612,7.3477864265441895,1.7949590682983398,2.3026063442230225,2.268781900405884,0.015539636835455894
+9,0,4.745791435241699,0.36901894211769104,4.74910831451416,1.1753836870193481,-0.373405396938324,-0.4239078760147095,0.11323818564414978,0.1442990005016327,0.10000000149011612,7.342643737792969,1.794329047203064,2.368659019470215,2.3276000022888184,0.020852141082286835
+10,0,4.696661472320557,0.4208894371986389,4.700538635253906,1.2508811950683594,-0.44822344183921814,-0.4969220459461212,0.11442247778177261,0.14012488722801208,0.10000000149011612,7.337504863739014,1.7936989068984985,2.4417102336883545,2.4060544967651367,0.023187359794974327
+11,0,4.637019157409668,0.4918119013309479,4.64165735244751,1.3344448804855347,-0.5359901189804077,-0.5759767293930054,0.11464840918779373,0.1296764463186264,0.10000000149011612,7.332369327545166,1.793068766593933,2.5488219261169434,2.4948792457580566,0.026870746165513992
+12,0,4.581727504730225,0.5152994394302368,4.58677864074707,1.4215337038040161,-0.6254845857620239,-0.6592959761619568,0.12092286348342896,0.1282002031803131,0.10000000149011612,7.327237606048584,1.7924387454986572,2.644061326980591,2.581939458847046,0.029581652954220772
+13,0,4.5096435546875,0.6779764890670776,4.516376972198486,1.5393762588500977,-0.7427445650100708,-0.7659728527069092,0.1360505372285843,0.1253385841846466,0.10000000149011612,7.322109699249268,1.7918086051940918,2.834840774536133,2.759640693664551,0.027328629046678543
+14,0,4.430872440338135,0.7227264642715454,4.438408851623535,1.6635645627975464,-0.8693180680274963,-0.8817341923713684,0.16889338195323944,0.13228371739387512,0.10000000149011612,7.316985130310059,1.7911784648895264,3.03462290763855,2.9400196075439453,0.03299373760819435
+15,0,4.308694362640381,0.8534331321716309,4.317925453186035,1.807710886001587,-1.0132362842559814,-1.0106812715530396,0.22238031029701233,0.1480296403169632,0.10000000149011612,7.311864376068115,1.7905484437942505,3.297241687774658,3.191932201385498,0.03971721976995468
+16,0,4.1956281661987305,0.9071230888366699,4.205986499786377,1.9378389120101929,-1.1536226272583008,-1.1204774379730225,0.31424373388290405,0.17616991698741913,0.10000000149011612,7.306746959686279,1.789918303489685,3.5557916164398193,3.417383909225464,0.04502787068486214
+17,0,4.049345970153809,1.0338716506958008,4.061744689941406,2.1023919582366943,-1.3219150304794312,-1.2584798336029053,0.44383156299591064,0.19259372353553772,0.10000000149011612,7.301632881164551,1.7892881631851196,3.86940336227417,3.709439992904663,0.047029413282871246
+18,0,3.8688864707946777,1.2180721759796143,3.884157657623291,2.2913706302642822,-1.510223627090454,-1.394737958908081,0.6117812991142273,0.14873601496219635,0.10000000149011612,7.296522617340088,1.7886580228805542,4.27211856842041,4.074420928955078,0.043271709233522415
+19,0,3.6744306087493896,1.4149434566497803,3.6930558681488037,2.5007059574127197,-1.7158420085906982,-1.5359820127487183,0.7725493311882019,0.07859402894973755,0.10000000149011612,7.291416168212891,1.7880280017852783,4.659894943237305,4.425642490386963,0.03030582144856453
+20,0,3.459163188934326,1.6876914501190186,3.48234224319458,2.7440121173858643,-1.953298568725586,-1.7080106735229492,0.8754026293754578,0.04914211854338646,0.10000000149011612,7.286313056945801,1.787397861480713,5.0843987464904785,4.843973636627197,0.02146209217607975
+21,0,3.226874828338623,1.9040849208831787,3.254917860031128,3.0262694358825684,-2.2308571338653564,-1.9156559705734253,0.9285809993743896,0.03328512981534004,0.10000000149011612,7.281213760375977,1.7867677211761475,5.553655624389648,5.300795078277588,0.014694985002279282
+22,0,2.9673383235931396,2.0981223583221436,3.0006980895996094,3.3252785205841064,-2.5250658988952637,-2.140127420425415,0.9555498957633972,0.02313012257218361,0.10000000149011612,7.27611780166626,1.7861377000808716,6.05569314956665,5.793400287628174,0.012489285320043564
+23,0,2.6748313903808594,2.547628164291382,2.7173750400543213,3.702394723892212,-2.8879055976867676,-2.42366623878479,0.9694580435752869,0.015896957367658615,0.10000000149011612,7.271025657653809,1.7855075597763062,6.430537223815918,6.2319722175598145,0.01138973981142044
+24,0,2.3879570960998535,2.767123222351074,2.438365936279297,4.067262172698975,-3.2492077350616455,-2.6894423961639404,0.9782087802886963,0.010865324176847935,0.10000000149011612,7.265936851501465,1.7848774194717407,7.0008544921875,6.762058734893799,0.011107800528407097
+25,0,1.9894752502441406,3.200282573699951,2.052309513092041,4.519039154052734,-3.6897921562194824,-3.0486109256744385,0.982963502407074,0.008469143882393837,0.10000000149011612,7.2608513832092285,1.7842473983764648,7.215175151824951,7.163510799407959,0.011583086103200912
+26,0,1.6557745933532715,3.571568012237549,1.732297658920288,5.002472877502441,-4.163969039916992,-3.421755075454712,0.9861047267913818,0.006911023054271936,0.10000000149011612,7.255769729614258,1.7836172580718994,7.0086493492126465,7.188100814819336,0.012272360734641552
+27,0,1.3181798458099365,3.912064790725708,1.4107270240783691,5.50710916519165,-4.662274360656738,-3.8326635360717773,0.987729012966156,0.005807442590594292,0.10000000149011612,7.250691890716553,1.782987117767334,6.238987922668457,6.851271152496338,0.013668214902281761
+28,0,0.981452226638794,4.351546764373779,1.0945504903793335,6.114635944366455,-5.259321689605713,-4.296047210693359,0.9883608818054199,0.005406384356319904,0.10000000149011612,7.245616912841797,1.7823569774627686,4.042074680328369,5.456900119781494,0.015356836840510368
+29,0,0.7339397668838501,4.580008029937744,0.8686707615852356,6.814339637756348,-5.958032608032227,-4.865708351135254,0.9888139963150024,0.00524666765704751,0.10000000149011612,7.240546226501465,1.7817269563674927,1.5236819982528687,3.539834976196289,0.015914741903543472
+30,0,0.6750156879425049,4.516140460968018,0.8311267495155334,7.683291912078857,-6.831489562988281,-5.658415794372559,0.988906741142273,0.005137934349477291,0.10000000149011612,7.23547887802124,1.7810968160629272,0.3633095622062683,1.552883267402649,0.013709533959627151
+31,0,0.6001018285751343,4.115861892700195,0.7729398012161255,8.529583930969238,-7.683381080627441,-6.761809825897217,0.9890531301498413,0.005097380839288235,0.10000000149011612,7.230414867401123,1.7804666757583618,1.9390462636947632,0.7525723576545715,0.013128000311553478
+32,0,0.43701228499412537,3.4745728969573975,0.6262270212173462,9.364472389221191,-8.533746719360352,-7.843254566192627,0.9892076253890991,0.005270735360682011,0.10000000149011612,7.225354194641113,1.779836654663086,1.8816279172897339,1.8498485088348389,0.01284834649413824
+33,0,0.43337351083755493,3.6306145191192627,0.6575231552124023,10.273053169250488,-9.427059173583984,-8.935256958007812,0.9885573387145996,0.00691025797277689,0.10000000149011612,7.220297813415527,1.7792065143585205,0.5955414175987244,1.1590968370437622,0.009660030715167522
+34,0,0.3793126344680786,3.4581656455993652,0.634032666683197,11.072334289550781,-10.233565330505371,-9.773441314697266,0.9871006011962891,0.01245318166911602,0.10000000149011612,7.215244293212891,1.778576374053955,0.7582050561904907,0.5783886313438416,0.009658947587013245
+35,0,0.39478880167007446,3.4792606830596924,0.6855044364929199,12.040966987609863,-11.200567245483398,-10.672159194946289,0.9784808158874512,0.033737435936927795,0.10000000149011612,7.2101945877075195,1.7779463529586792,0.7700105905532837,0.5571295022964478,0.02785857766866684
+36,0,0.370413601398468,3.3471736907958984,0.7001765966415405,13.065825462341309,-12.227996826171875,-11.726350784301758,0.9281508326530457,0.14950571954250336,0.10000000149011612,7.205148220062256,1.7773162126541138,0.39447319507598877,0.7322188019752502,0.06246435269713402
+37,0,0.3046747148036957,3.464092254638672,0.6828665733337402,14.080439567565918,-13.220455169677734,-12.895588874816895,0.857744574546814,0.2289915233850479,0.10000000149011612,7.200105667114258,1.7766860723495483,0.318521648645401,0.35412922501564026,0.060389503836631775
+38,0,0.2929304838180542,3.2169876098632812,0.7181237936019897,15.017313003540039,-14.149724006652832,-13.942829132080078,0.751166582107544,0.3302457332611084,0.10000000149011612,7.195066452026367,1.776055932044983,0.608488142490387,0.5012122988700867,0.05922633036971092

# 2025-03-26 18:56:14.626272
+y

# 2025-03-26 18:59:47.114809
+n

# 2025-03-26 19:02:39.209899
+I dont like having the code distributed in three files. Can you create an improved and compact version of the code in a new file SACCQL_training.py? Make the code very readable using classes and functions. Add comments explaining every step of the training so that it is understsanble

# 2025-03-26 20:55:09.160742
+y

# 2025-03-26 20:56:56.367253
+n

# 2025-03-26 20:57:00.062181
+y

# 2025-03-26 21:05:51.565464
+I think you dont understand the data that I am importing very well. The states data is already normalised so we dont need normalisation once it is imported. The actions data is also normalised between -1 and 1 so we dont need normalisation. The glu_raw data is only for the glucose reward computation. Try to remove a bit the complicated parts of the code. You dont need to do a function for everything, be smart about it. The code should be readable. The data looks like this time,glu_raw,glu,glu_d,glu_t,hr,hr_d,hr_t,iob,hour,basal,bolus,done
+2021-10-29 00:00:00,239.0,1.553677881470189,,,-0.0855028996704795,,,-1.271495871418974,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:05:00,238.0,1.5320001828914351,-0.16458086929789967,,-1.0180430677348502,-1.3953604111111517,,-1.256097552468202,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:10:00,235.0,1.4669670871551732,-0.5051341207787045,,-1.095754748406881,-0.11053643218303323,,-1.2407380287207284,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:15:00,233.0,1.4236116899976654,-0.3348574950383021,,-1.095754748406881,0.006265747719522991,,-1.2254420630722747,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:20:00,231.0,1.3802562928401574,-0.3348574950383021,,-0.9403313870628193,0.23987010752463545,,-1.2102335179496275,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:25:00,229.0,1.3369008956826496,-0.3348574950383021,-0.5482555917326529,-0.9403313870628193,0.006265747719522991,-0.5561171792479355,-1.1951353553106387,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:30:00,227.0,1.2935454985251416,-0.3348574950383021,-0.5634833434108576,-1.095754748406881,-0.22733861208558945,0.05374297632903384,-1.180169636644224,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:35:00,222.0,1.1851570056313718,-0.8456873722595094,-0.6396221018018804,-0.9403313870628193,0.23987010752463545,0.13136154158428442,-1.1653575229703643,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:40:00,220.0,1.1418016084738638,-0.3348574950383021,-0.708146984353801,-1.095754748406881,-0.22733861208558945,-0.0016988559961451867,-1.1507192748401045,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:45:00,216.0,1.055090814158848,-0.6754107465191069,-0.8071273702621304,-1.1734664290789119,-0.11053643218303323,-0.19020108590175386,-1.1362742523355551,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:50:00,210.0,0.9250246226863243,-1.0159639979999115,-0.9822465145614827,-0.47406130303063393,1.057485366842529,0.2976870385598214,-1.1220409150698905,0.0,3.1835688690633077,-0.16106604143930775,0
+

# 2025-03-26 21:07:42.471283
+y

# 2025-03-26 21:12:32.866078
+n

# 2025-03-26 21:12:49.367276
+y

# 2025-03-26 21:13:20.864368
+/drop

# 2025-03-26 21:13:26.978228
+/add SACCQL_training.py

# 2025-03-26 21:13:32.158815
+I think you dont understand the data that I am importing very well. The states data is already normalised so we dont need normalisation once it is imported. The actions data is also normalised between -1 and 1 so we dont need normalisation. The glu_raw data is only for the glucose reward computation. Try to remove a bit the complicated parts of the code. You dont need to do a function for everything, be smart about it. The code should be readable. The data looks like this time,glu_raw,glu,glu_d,glu_t,hr,hr_d,hr_t,iob,hour,basal,bolus,done
+2021-10-29 00:00:00,239.0,1.553677881470189,,,-0.0855028996704795,,,-1.271495871418974,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:05:00,238.0,1.5320001828914351,-0.16458086929789967,,-1.0180430677348502,-1.3953604111111517,,-1.256097552468202,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:10:00,235.0,1.4669670871551732,-0.5051341207787045,,-1.095754748406881,-0.11053643218303323,,-1.2407380287207284,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:15:00,233.0,1.4236116899976654,-0.3348574950383021,,-1.095754748406881,0.006265747719522991,,-1.2254420630722747,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:20:00,231.0,1.3802562928401574,-0.3348574950383021,,-0.9403313870628193,0.23987010752463545,,-1.2102335179496275,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:25:00,229.0,1.3369008956826496,-0.3348574950383021,-0.5482555917326529,-0.9403313870628193,0.006265747719522991,-0.5561171792479355,-1.1951353553106387,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:30:00,227.0,1.2935454985251416,-0.3348574950383021,-0.5634833434108576,-1.095754748406881,-0.22733861208558945,0.05374297632903384,-1.180169636644224,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:35:00,222.0,1.1851570056313718,-0.8456873722595094,-0.6396221018018804,-0.9403313870628193,0.23987010752463545,0.13136154158428442,-1.1653575229703643,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:40:00,220.0,1.1418016084738638,-0.3348574950383021,-0.708146984353801,-1.095754748406881,-0.22733861208558945,-0.0016988559961451867,-1.1507192748401045,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:45:00,216.0,1.055090814158848,-0.6754107465191069,-0.8071273702621304,-1.1734664290789119,-0.11053643218303323,-0.19020108590175386,-1.1362742523355551,0.0,3.1835688690633077,-0.16106604143930775,0
+2021-10-29 00:50:00,210.0,0.9250246226863243,-1.0159639979999115,-0.9822465145614827,-0.47406130303063393,1.057485366842529,0.2976870385598214,-1.1220409150698905,0.0,3.1835688690633077,-0.16106604143930775,0
+

# 2025-03-26 21:15:56.819142
+y

# 2025-03-26 21:23:13.778879
+Add the creation of a training log csv file with a lot of information during training. Create a function that is called after the training to analyse the trends of every column in the training log and create a graph displaying them. 

# 2025-03-26 21:28:28.224004
+y

# 2025-03-26 21:30:57.144577
+$ python SACCQL_training.py
+C:\Users\emili\procoding\Gloop\SACCQL_training.py:46: RuntimeWarning: invalid value encountered in power
+  log_term = np.log(glucose_next) ** 1.084
+Training:   0%|                                                                                                              | 0/200 [00:00<?, ?it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 307, in <module>
+    agent = train_sac(
+        dataset_path="datasets/processed/563-train.csv",
+    ...<2 lines>...
+        log_dir="training_logs"
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 200, in train_sac
+    next_actions = agent.act(next_states)
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 137, in act
+    dist = torch.distributions.Normal(mean, std)
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\distributions\normal.py", line 59, in __init__
+    super().__init__(batch_shape, validate_args=validate_args)
+    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\distributions\distribution.py", line 71, in __init__
+    raise ValueError(
+    ...<5 lines>...
+    )
+ValueError: Expected parameter loc (Tensor of shape (256, 2)) of distribution Normal(loc: torch.Size([256, 2]), scale: torch.Size([256, 2])) to satisfy the constraint Real(), but found invalid values:
+tensor([[ 1.8632e-02, -1.5006e-02],
+        [ 7.2262e-02,  4.4353e-02],
+        [ 1.3113e-01, -1.0687e-02],
+        [ 3.9277e-02, -2.7777e-02],
+        [ 3.2060e-01, -3.3996e-02],
+        [ 1.5809e-01, -4.1606e-02],
+        [ 4.7957e-02,  9.8577e-03],
+        [ 1.5447e-02, -6.5393e-02],
+        [ 3.7412e-02, -4.0509e-02],
+        [        nan,         nan],
+        [ 1.6913e-01,  1.6327e-02],
+        [ 7.8805e-02, -3.3828e-02],
+        [ 6.0228e-02, -2.6419e-02],
+        [        nan,         nan],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 9.6446e-02,  4.9962e-02],
+        [ 6.9228e-02, -8.0919e-02],
+        [ 1.9410e-01, -6.1936e-02],
+        [ 4.4907e-02, -6.2703e-02],
+        [ 6.7718e-02, -3.7852e-02],
+        [ 1.0565e-01, -3.3675e-02],
+        [        nan,         nan],
+        [-4.1544e-03, -4.6484e-02],
+        [ 6.3118e-03, -1.1816e-02],
+        [ 2.0457e-01, -8.8497e-02],
+        [ 3.0099e-02, -4.7130e-03],
+        [ 4.2486e-01, -1.4674e-01],
+        [ 2.2654e-04, -1.2416e-02],
+        [ 1.7171e-01,  3.8835e-02],
+        [ 1.9480e-01, -5.9257e-02],
+        [        nan,         nan],
+        [ 6.7625e-02,  5.3857e-03],
+        [ 5.6274e-02, -5.9615e-02],
+        [ 1.6993e-01, -5.9013e-02],
+        [ 2.5547e-01, -1.9588e-02],
+        [        nan,         nan],
+        [ 2.0303e-01,  1.7356e-02],
+        [        nan,         nan],
+        [ 1.4515e-01,  3.6041e-02],
+        [ 2.3158e-01, -2.0768e-01],
+        [ 1.8057e-01, -1.7149e-01],
+        [ 1.7241e-01,  7.8550e-02],
+        [-1.9951e-02, -6.8102e-02],
+        [        nan,         nan],
+        [ 3.5980e-02, -9.7487e-02],
+        [ 9.8638e-02, -9.9655e-03],
+        [ 3.8442e-02,  2.2394e-02],
+        [ 3.9677e-02, -5.6278e-02],
+        [        nan,         nan],
+        [ 1.4042e-01, -4.1280e-02],
+        [ 9.2199e-02,  5.2707e-02],
+        [-7.7944e-02, -2.1122e-01],
+        [        nan,         nan],
+        [-7.1129e-03, -5.0369e-02],
+        [ 1.1148e-01,  3.6929e-02],
+        [ 1.6177e-01,  4.7172e-02],
+        [-2.8292e-02, -1.1370e-01],
+        [ 1.4346e-01,  1.3730e-02],
+        [ 1.4982e-01, -2.0875e-01],
+        [ 4.6464e-02, -2.3569e-02],
+        [        nan,         nan],
+        [ 2.0040e-03, -6.5855e-02],
+        [        nan,         nan],
+        [ 1.5948e-01,  5.1678e-03],
+        [ 6.2267e-02, -2.3687e-02],
+        [ 7.9639e-02, -1.4217e-03],
+        [        nan,         nan],
+        [-4.0010e-02, -1.0899e-01],
+        [ 8.7704e-02, -1.3969e-01],
+        [        nan,         nan],
+        [ 3.5947e-02, -1.3937e-02],
+        [ 5.9930e-02,  3.0547e-02],
+        [ 5.1879e-02, -9.7723e-02],
+        [ 1.8302e-01, -2.5572e-02],
+        [        nan,         nan],
+        [ 1.0162e-01, -6.3140e-02],
+        [        nan,         nan],
+        [ 7.8480e-02, -6.4643e-02],
+        [ 1.2093e-01,  3.5087e-02],
+        [        nan,         nan],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 2.3584e-02, -6.0509e-02],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 1.7113e-01,  1.0694e-02],
+        [ 7.4389e-02,  2.8430e-02],
+        [        nan,         nan],
+        [-1.4897e-02, -5.8522e-02],
+        [ 8.0522e-02, -2.1264e-02],
+        [ 3.2182e-02, -6.0339e-02],
+        [ 9.3621e-02,  1.2805e-02],
+        [ 1.1246e-01,  1.4820e-02],
+        [ 6.7462e-02, -1.9397e-03],
+        [-2.0033e-02, -4.1877e-02],
+        [ 5.0935e-02, -1.2935e-02],
+        [-2.8685e-02, -7.4521e-02],
+        [ 1.1875e-01, -2.4647e-02],
+        [ 2.8053e-02, -4.3701e-02],
+        [ 1.0184e-01, -5.8698e-02],
+        [ 5.1255e-02, -3.6865e-02],
+        [        nan,         nan],
+        [ 2.2642e-03, -1.6437e-01],
+        [        nan,         nan],
+        [ 1.2557e-01, -3.2462e-02],
+        [ 1.8255e-02, -6.1237e-02],
+        [ 5.8435e-02, -1.5580e-02],
+        [ 1.0047e-01,  2.4394e-02],
+        [ 6.4618e-02, -1.2408e-02],
+        [ 1.3471e-01, -2.8436e-04],
+        [        nan,         nan],
+        [-1.4182e-02, -4.8837e-02],
+        [ 7.9445e-02,  3.6405e-02],
+        [ 1.6213e-01, -1.4290e-01],
+        [ 4.6586e-02,  7.5181e-03],
+        [ 2.5024e-02, -2.3803e-02],
+        [ 3.4484e-01,  2.7577e-02],
+        [-4.0911e-02, -1.9633e-01],
+        [ 5.6002e-02,  6.2659e-02],
+        [ 1.5634e-01,  2.4077e-03],
+        [        nan,         nan],
+        [ 4.3763e-02, -1.9008e-03],
+        [ 9.4411e-02, -1.1284e-01],
+        [ 1.3103e-01, -3.6570e-02],
+        [ 1.5494e-01, -9.9267e-02],
+        [ 1.6293e-01,  3.8885e-02],
+        [ 1.5253e-01, -2.6917e-02],
+        [ 3.6743e-02, -1.8482e-01],
+        [ 5.4041e-02,  3.1020e-02],
+        [ 2.0048e-01, -1.7942e-02],
+        [ 1.0830e-01,  2.8220e-02],
+        [ 2.6249e-01,  8.1409e-03],
+        [        nan,         nan],
+        [ 7.9588e-02, -1.1824e-01],
+        [-1.4673e-02, -3.7185e-02],
+        [ 1.2000e-01,  3.9053e-02],
+        [ 2.0895e-02, -1.4359e-01],
+        [-1.2194e-02, -4.9031e-04],
+        [ 8.4642e-03, -8.2866e-02],
+        [ 4.4736e-02, -3.5810e-02],
+        [ 1.9718e-01, -2.6504e-02],
+        [ 8.8473e-02,  8.4492e-02],
+        [ 5.5319e-02, -1.8412e-02],
+        [ 1.3734e-01,  3.3830e-02],
+        [ 1.0218e-01,  2.3468e-03],
+        [-1.9226e-02, -7.7519e-02],
+        [        nan,         nan],
+        [-1.5557e-02, -4.9250e-02],
+        [        nan,         nan],
+        [ 1.8523e-01,  3.1629e-02],
+        [ 2.0775e-01, -1.1862e-01],
+        [        nan,         nan],
+        [ 4.5492e-02,  2.4172e-02],
+        [        nan,         nan],
+        [ 7.8366e-02,  2.6215e-04],
+        [ 1.3567e-01,  3.2426e-02],
+        [ 1.2951e-01,  3.9969e-02],
+        [ 1.1670e-01,  8.4071e-02],
+        [        nan,         nan],
+        [ 1.0854e-01, -2.9514e-02],
+        [ 7.1648e-02, -1.7821e-03],
+        [        nan,         nan],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 3.8998e-02, -5.5920e-02],
+        [ 1.3959e-02, -8.8419e-02],
+        [-6.5872e-04, -4.5551e-02],
+        [ 5.0053e-02,  3.0708e-02],
+        [        nan,         nan],
+        [ 9.4693e-02,  3.2239e-02],
+        [-2.1202e-02, -1.0327e-01],
+        [ 6.0538e-02, -5.3384e-02],
+        [ 1.1259e-01, -4.2468e-02],
+        [        nan,         nan],
+        [ 1.5653e-01,  5.2356e-02],
+        [        nan,         nan],
+        [ 4.9044e-02, -1.1037e-02],
+        [ 1.0084e-01, -1.4251e-02],
+        [ 1.6336e-01,  6.3787e-02],
+        [ 1.0306e-02, -5.1060e-02],
+        [-5.0800e-03, -6.8787e-02],
+        [        nan,         nan],
+        [ 8.9661e-02, -1.6781e-02],
+        [ 1.0273e-01, -1.2700e-01],
+        [        nan,         nan],
+        [-1.5496e-03, -1.5464e-01],
+        [        nan,         nan],
+        [ 7.2806e-02,  2.3343e-02],
+        [        nan,         nan],
+        [ 1.0864e-01,  7.1319e-04],
+        [ 1.5141e-01,  1.9047e-02],
+        [ 1.2531e-01, -8.8819e-02],
+        [-1.8095e-02, -9.1936e-02],
+        [ 1.3212e-01,  3.4041e-02],
+        [ 3.7944e-02, -8.9076e-02],
+        [ 1.1155e-01,  9.4895e-03],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 3.4419e-01, -5.9548e-02],
+        [        nan,         nan],
+        [ 2.1314e-01,  3.0428e-02],
+        [ 4.4045e-02,  4.7795e-06],
+        [ 3.7657e-02, -3.1209e-02],
+        [ 2.2852e-01, -4.5795e-02],
+        [ 3.3886e-02,  2.2049e-02],
+        [        nan,         nan],
+        [ 3.6948e-02, -5.2569e-02],
+        [ 1.1597e-01,  3.1907e-02],
+        [        nan,         nan],
+        [ 1.2113e-01,  8.2805e-02],
+        [ 1.3761e-01,  6.5807e-02],
+        [ 5.6233e-02,  1.1921e-02],
+        [ 6.5750e-02, -6.7704e-02],
+        [ 2.8625e-01,  4.4205e-02],
+        [ 2.0693e-01, -1.0225e-01],
+        [ 5.8367e-02,  4.3011e-02],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 4.7117e-02, -2.0477e-03],
+        [ 7.0682e-02,  1.7424e-02],
+        [ 9.8974e-02,  3.0594e-02],
+        [ 1.0299e-01,  2.6135e-04],
+        [ 9.6655e-02, -1.1500e-02],
+        [        nan,         nan],
+        [ 1.6378e-01, -1.2214e-03],
+        [ 5.5225e-02,  3.2689e-02],
+        [ 4.1465e-02,  2.0341e-02],
+        [ 6.5685e-02, -6.2889e-02],
+        [ 6.0535e-02,  5.0703e-03],
+        [-9.9876e-03, -9.1695e-02],
+        [ 2.2357e-01, -1.1429e-01],
+        [ 4.9345e-01, -2.3235e-02],
+        [ 2.3964e-02,  1.0956e-02],
+        [ 1.3006e-02, -1.3087e-01],
+        [ 8.4723e-02, -3.3547e-02],
+        [ 9.8471e-02, -1.7415e-02],
+        [ 6.5797e-02,  1.3241e-02],
+        [ 9.7279e-02, -2.7069e-02],
+        [ 6.6361e-02, -1.5794e-03],
+        [        nan,         nan],
+        [        nan,         nan],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 2.8809e-01,  7.0878e-02],
+        [ 6.3055e-04, -9.7017e-02],
+        [ 1.3099e-01,  9.3918e-03],
+        [ 1.5119e-01,  2.9185e-02],
+        [-1.0457e-02, -3.3923e-02],
+        [        nan,         nan],
+        [        nan,         nan],
+        [ 4.8900e-02, -2.9883e-02],
+        [ 5.3246e-02,  1.3545e-02],
+        [ 2.8712e-02, -3.0889e-02],
+        [        nan,         nan],
+        [ 8.5701e-02,  3.9211e-02]])
+

# 2025-03-26 21:32:58.160451
+y

# 2025-03-26 21:35:22.788390
+emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio)
+$ python SACCQL_training.py
+C:\Users\emili\procoding\Gloop\SACCQL_training.py:55: RuntimeWarning: invalid value encountered in power
+  log_term = np.log(glucose_next) ** 1.084
+Training:  24%|                              | 47/200 [00:52<02:58,  1.17s/it, Critic Loss=163.279, Actor Loss=-4712.078, Q Values=-41.581]

# 2025-03-26 21:40:34.543369
+y

# 2025-03-26 21:46:26.459001
+Having a look at training logs (that you can find below), many things seem not right. Please fix this without complicating more the code! Dont add weird stuff just try to fix the current building blocks epoch,critic_loss,actor_loss,q1_value,q2_value,action_mean,action_std,entropy,grad_norm
+1,24.89533310670119,-1.320505535373321,-1.7138660629996314,-3.426080183985715,0.006588671410169739,0.6298184188512655,1.42081991984294,10.837632527718178
+2,23.04986968407264,-3.3351325346873355,-4.211609198496892,-5.602144076273992,0.03194096312374593,0.630182500068958,1.4207673531312208,14.010576394888071
+3,23.064897537231445,-5.198942959308624,-5.83621423978072,-6.815948798106267,0.09687596892651457,0.626278253702017,1.4193808000821333,16.13534690783574
+4,22.579363896296574,-7.309831014046302,-7.433027074887202,-7.978060199664189,0.24407787792957747,0.606841639830516,1.416987100472817,17.890964067899265
+5,21.677684013660137,-9.42560206926786,-9.083866027685312,-9.164809465408325,0.44316896624290025,0.5449393374415544,1.4137833485236535,18.62528966023372
+6,20.58064779868493,-11.371783568308903,-10.79379885013287,-10.429586080404428,0.5924106452327508,0.47521922336174893,1.4106762661383703,19.01018993671124
+7,19.39205265045166,-13.215416211348314,-12.591275471907396,-11.781028435780453,0.6924971410861382,0.41113924235105515,1.4081586920298064,19.58085558964656
+8,18.241061834188606,-15.154585489859947,-14.413128320987408,-13.206603141931387,0.7651385435691247,0.351008782019982,1.4056489628094893,20.15837783079881
+9,17.16679341976459,-17.144242946918194,-16.247838020324707,-14.704478172155527,0.8119258708678759,0.3069791951431678,1.4032054658119495,20.725585570702187
+10,16.14723904316242,-19.177145664508526,-18.128770718207726,-16.269523510566124,0.8432387434519254,0.27562573466163415,1.4013581436413984,21.307290370647724
+11,15.157439506970919,-21.315620055565468,-20.077332203204815,-17.901281503530647,0.8710491107060359,0.23940671120698637,1.4002012060238764,21.935354232788086
+12,14.225064992904663,-23.446025885068455,-22.055099927462063,-19.59738144507775,0.8877641340860953,0.22139786957548216,1.3990379847013032,22.566349653097298
+13,13.33490540431096,-25.659682677342342,-24.079393020043007,-21.3580752152663,0.904347110253114,0.1996805381316405,1.3975111842155457,23.210565933814415
+14,12.451111738498394,-27.93913170007559,-26.179241730616642,-23.184818487900955,0.9190854189487604,0.17614793634185424,1.3965744765905233,23.805999682499813
+15,11.619656709524302,-30.25905807201679,-28.308090760157658,-25.076307150033806,0.9243565442470404,0.17201332260782903,1.3962436020374298,24.439327606788048
+16,10.806017490533682,-32.6513715157142,-30.497450351715088,-27.030148102686955,0.9372996447178034,0.14763287010674292,1.3956035123421595,25.08336749443641
+17,10.01671682871305,-35.09944556309627,-32.7395763030419,-29.047360053429237,0.9418809528534229,0.1393930598233755,1.3951000892199004,25.686185580033523
+18,9.242868918638964,-37.615815089299126,-35.03868748591496,-31.127424863668587,0.9470597837979977,0.13228838231701118,1.3948781765424287,26.282570948967567
+19,8.489645444429838,-40.14447637704703,-37.39192471137414,-33.270301011892464,0.9513520747423172,0.12296997096676093,1.3950160650106578,26.930098166832558
+20,7.759696263533372,-42.80263460599459,-39.79107511960543,-35.47433434999906,0.9562982699045768,0.11271236636317693,1.3951913370535924,27.516574676220234
+21,7.043702804125273,-45.45451787801889,-42.24622256939228,-37.73888507256141,0.959841427894739,0.10674792829041298,1.3957823331539447,28.14405793410081
+22,6.3471935253876905,-48.20926196758564,-44.75299307016226,-40.064861004169174,0.9624009762818997,0.10451863906704463,1.396322821195309,28.683104661794808
+23,5.736836204162011,-51.07655290456918,-47.218565720778244,-42.48321349804218,0.9633653771418792,0.09841905433971149,1.397103101015091,28.11028102728037
+24,5.326315824802105,-54.127362764798676,-49.49114748147818,-45.052223499004654,0.966179989851438,0.09289669883079253,1.39757597217193,28.169634708991417
+25,4.986810473295359,-57.25013542175293,-51.77556089254526,-47.70223485506498,0.9685444224339265,0.08997129276394844,1.397998543886038,28.756650704603928
+26,4.67644360432258,-60.4044986137977,-54.1428001843966,-50.40661144256592,0.9695304586337163,0.08864679631705467,1.3986010413903456,29.363124260535606
+27,4.38611960869569,-63.591706642737755,-56.58359131446252,-53.16155961843637,0.9716114653990819,0.0818088694403951,1.3993214070796967,29.997723065889797
+28,4.115664761800033,-66.84751231853778,-59.09891040508564,-55.96854760096623,0.9726906292713605,0.08132599960439481,1.4003005921840668,30.577097159165604
+29,3.869468546830691,-70.11884102454552,-61.67890020517203,-58.82688874464769,0.9733138107336484,0.08154820987524894,1.4010681555821345,31.213156810173622
+30,3.6202026101259084,-73.47488359304575,-64.32193690079909,-61.737108597388634,0.9745936290575907,0.07580555853648828,1.4016007918577928,31.77565860748291
+31,3.3815560661829434,-76.86487300579364,-67.03513365525465,-64.70302867889404,0.9770177958103327,0.06978596617969182,1.402347702246446,32.33032464981079
+32,3.156758876947256,-80.29253049997183,-69.79104775648851,-67.72189169663649,0.9768053889274597,0.07285576894019659,1.4033312843396113,32.94083984081562
+33,2.922534873852363,-83.80384459862343,-72.62167959946852,-70.79381854717548,0.9780388612013596,0.068807196086989,1.40392820422466,33.46117980663593
+34,2.7041770265652585,-87.3402821467473,-75.50030811016376,-73.92118219228891,0.9783945152392755,0.07076429840750419,1.4047313584731176,34.09551847898043
+35,2.479238821909978,-90.9375751201923,-78.44083507244403,-77.09942421546349,0.9797941205593256,0.07075491132071385,1.4050414034953485,34.64739219958965
+36,2.262464323869118,-94.59710223858173,-81.43449167104868,-80.33122840294472,0.9819407004576463,0.06010703681609952,1.4050758251777062,35.18130727914664
+37,2.0452400698111606,-98.31063593350925,-84.48379971430852,-83.61999042217548,0.9825878876906174,0.05690375049240314,1.405887661071924,35.698228909419136
+38,1.8225426650964296,-102.0786258990948,-87.58650897099422,-86.96575751671425,0.9831736603608499,0.05927027475375395,1.4064635336399078,36.21542703188383
+39,1.602269055751654,-105.88550861065204,-90.74942368727464,-90.36774826049805,0.9838736813801986,0.0542287493411165,1.4073652854332557,36.702807646531326
+40,1.4034752135093396,-109.7966847052941,-93.9727479494535,-93.80243110656738,0.9843545372669513,0.052240547294226974,1.4085658834530757,36.56813005300668
+41,1.2765654978843837,-113.86573057908278,-97.26018978999211,-97.17289440448468,0.9852732121944427,0.04906157948649847,1.4096177655916948,35.785797559298004
+42,0.7048577688983642,-118.545622752263,-99.85742202171913,-99.82380969707782,0.9845251761949979,0.05581021394867163,1.4109942821355967,26.154055155240574
+43,0.5740162871530069,-124.64670416025015,-99.98075734652005,-99.99792377765363,0.9851581018704635,0.05115622726197426,1.4129016491082997,24.494481820326584
+44,0.5959762151838871,-130.81241563650278,-99.98166597806491,-99.99663661076472,0.9860255202421775,0.050676870947847,1.414579492348891,24.853726130265457
+45,0.5774647230176085,-136.89000789935773,-99.98986684359036,-99.99640743549054,0.9853347024092307,0.05337812558103066,1.4165302148232093,25.30944013595581
+46,0.5823562136490812,-142.87089362511267,-99.9826768728403,-99.99620393606332,0.9858085306791159,0.05434238316061405,1.4176780879497528,25.875186369969295
+47,0.5733554509598224,-148.82123653705304,-99.99371088468112,-99.99647375253531,0.9869941782492858,0.048297286714212254,1.419133376616698,26.18341324879573
+48,0.5769458957108257,-154.72634740976187,-99.99320499713605,-99.99652040921725,0.9873914030882028,0.04552207869262649,1.4211165469426374,26.62488834674542
+49,0.5782051418970625,-160.60013609666092,-99.9891895881066,-99.9962461911715,0.9871471936886127,0.04907942799708018,1.422629395356545,27.02803296309251
+50,0.5768036022004358,-166.45368693425104,-99.9950988476093,-99.9969951923077,0.9879805995867803,0.04426350072026253,1.42451522900508,27.42838819210346
+51,0.5735504303637964,-172.29422818697415,-99.99060880220853,-99.99615948016827,0.9876846143832574,0.04698421186409318,1.4264754286179175,27.83224582672119
+52,0.5756876460520568,-178.11804815439078,-99.98927512535683,-99.99657366825984,0.9885219805515729,0.04111933858635334,1.428139335834063,28.284394447620098
+53,0.5768388387111302,-183.9469848045936,-99.9895242544321,-99.99602200434758,0.9888577850965353,0.041347321087064653,1.4294086694717407,28.624959578880897
+54,0.5767985214580011,-188.35341145442084,-99.99579268235426,-99.99660477271446,0.9889919356657908,0.043873414827080875,1.4305406694228833,28.891509496248684
+55,0.5722554893615709,-191.2546242934007,-99.99440956115723,-99.99657792311449,0.9887738480017736,0.04326892432828362,1.4313884950601137,29.088501600118782
+56,0.5731029614074102,-194.13857239943283,-99.99068069458008,-99.99611282348633,0.9890288068698003,0.04160094805634939,1.4322241223775423,29.289314380058876
+57,0.5812110193599858,-197.00307611318735,-99.99557993962215,-99.99660579974835,0.9895152300596237,0.03891150105314759,1.4326810836791992,29.534735716306248
+58,0.5775361433378958,-199.86537111722507,-99.9936992938702,-99.99651189950796,0.9895322850117316,0.041479362174868584,1.4334366298638856,29.631644872518685
+59,0.5764109584449145,-202.7077739422138,-99.99451138423039,-99.99602523216835,0.9897598578379705,0.04091219385512746,1.4342754185199738,29.925655181591328
+60,0.5765482990111475,-205.5525280879094,-99.98812514085036,-99.99628008328952,0.9895883042078751,0.043639647075906396,1.4349029339276826,29.997672117673435
+61,0.5723097601529354,-208.37483450082632,-99.99558199368991,-99.99611531771146,0.9898406324478296,0.04181774673410333,1.4357842482053316,30.17536240357619
+62,0.5774079929553688,-210.530272263747,-99.9885328733004,-99.99691537710336,0.9904320985078812,0.038526882202579424,1.4363782291228955,30.307223466726448
+63,0.5754149887999609,-211.91933529193585,-99.99015221228966,-99.99646553626427,0.9901054742244574,0.039631703384937,1.4367111164789934,30.410134388850285
+64,0.5766955439243108,-213.34396010178787,-99.99652833205003,-99.99662883465106,0.9899611175060272,0.03966348516181684,1.4371113960559552,30.45443157049326
+65,0.5720298702441281,-214.7451371413011,-99.99572621859036,-99.99635168222281,0.9903271209735137,0.03763342278006558,1.437566108428515,30.538009130037747
+66,0.576444275848469,-216.14124503502478,-99.99437449528621,-99.99627318749062,0.9904565146336188,0.036329084541648626,1.438027533201071,30.622550927675686
+67,0.5775672616224395,-217.5095839867225,-99.99377646813026,-99.99627098670372,0.9899909106584696,0.0415836598795767,1.4383500172541692,30.78100160452036
+68,0.5756752632553869,-218.9003389798678,-99.99056816101074,-99.99630972055289,0.9901009247853205,0.03998686487858112,1.4384432297486525,30.99513163933387
+69,0.5812200919754105,-220.29722419151892,-99.99264878493089,-99.99646715017465,0.9906425051964246,0.03811385973285024,1.4385920602541704,30.87184260441707
+70,0.5721221818564047,-221.68089000995343,-99.99495594318097,-99.99609022874098,0.9906004965305328,0.03668551354740675,1.4388914979421175,30.952782997718224
+71,0.5728449045832699,-223.0459266075721,-99.99429086538461,-99.99632336543156,0.9907342803019744,0.03493053405187451,1.4392002775118902,31.143548561976505
+72,0.5724038306686382,-224.08175864586462,-99.99540915855995,-99.99654139005222,0.9907566091189017,0.03858383882862444,1.439333989070012,31.492804490602932
+73,0.5769363032752433,-224.7840312077449,-99.99404569772574,-99.99665671128493,0.9909743036215122,0.035022072959691286,1.439332943696242,31.14020167864286
+74,0.5721058803756023,-225.47048451350287,-99.9948077568641,-99.99628580533542,0.9906207987895379,0.03648721815373462,1.439455976853004,31.176769733428955
+75,0.5764123125655505,-226.15485939612756,-99.99505145733173,-99.99627377436711,0.9906470271257254,0.03980984492227435,1.439585337272057,31.216555265279915
+76,0.5718976419627254,-226.83902212289664,-99.99508109459511,-99.99630194443922,0.9908701387735513,0.037517636333807156,1.4397169259878306,31.255158937894382
+77,0.5760660635283924,-227.5200682419997,-99.99492161090558,-99.99617532583383,0.9909136971602073,0.03645129415851373,1.4398479553369374,31.300562455103947
+78,0.5732307026785117,-228.17823674128607,-99.99285023029034,-99.99641506488507,0.9908356678027374,0.037102996586607054,1.439939219218034,31.379925397726204
+79,0.5720250680331866,-228.88159854595477,-99.9952834202693,-99.99621948829063,0.9913934343136274,0.03381567310470228,1.440041381579179,31.374928327707146
+80,0.5763983587429931,-229.53873619666467,-99.99508887070876,-99.9965928884653,0.9905755370855331,0.039651435549156025,1.4401025061423962,31.908519928271954
+81,0.5777463600802898,-230.232419527494,-99.99220862755409,-99.99621171217699,0.9913853143270199,0.03317295826183489,1.4400473970633287,31.478387942680946
+82,0.5763729221059923,-230.90795927781326,-99.99559020996094,-99.99652862548828,0.9907812006198443,0.03880394051352946,1.4401478056724255,31.503835971538837
+83,0.5763651304049955,-231.42153637225812,-99.99553944514348,-99.99617649958684,0.9911019882330527,0.0375625231804756,1.440207662490698,31.520863716418926
+84,0.5719625511835777,-231.75983135516827,-99.99477518521823,-99.99635197566106,0.9911797849031595,0.03465720369982032,1.4402724045973558,31.539963465470535
+85,0.5762550198705867,-232.0977055476262,-99.99550790053148,-99.99633378248949,0.991247588625321,0.035390706320938006,1.4403391893093402,31.559045168069694
+86,0.572297416641959,-232.43491158118616,-99.99371631328876,-99.99573062016414,0.9914720734724631,0.0347767284569832,1.4404073449281545,31.582137988163876
+87,0.571750103960557,-232.77035551804764,-99.9965086716872,-99.996979199923,0.9911933312049279,0.03505853438176788,1.44046089053154,31.61495953339797
+88,0.5740485277044802,-233.10814725435696,-99.99273329514723,-99.99624017568735,0.9916263612417074,0.033471205773261875,1.4405244222054114,31.62615552315345
+89,0.5805209956743056,-233.44437437791092,-99.99578299889198,-99.99630458538348,0.990921655526528,0.03868221359040875,1.4405972682512724,31.63583465722891
+90,0.5763591854103232,-233.77287908700796,-99.9956094301664,-99.99644294151894,0.9914360676820462,0.03568297076540498,1.4406357201246114,31.7549991607666
+91,0.5808049165834046,-234.09514500544623,-99.99646465594952,-99.9964215205266,0.9914875202454053,0.03475344320759177,1.4406506327482371,31.805956767155575
+92,0.5722784448097138,-234.43767606295071,-99.99414517329289,-99.99600571852464,0.9910106624548252,0.03899701921126017,1.4406991440516252,31.818249959212082
+93,0.5734446710412158,-234.7768807044396,-99.99336448082558,-99.99616300142728,0.9916150306279843,0.034811150264711335,1.4407182725576253,31.794562156383808
+94,0.572425347299521,-235.03575662466196,-99.99299533550555,-99.9964112501878,0.9912511041531196,0.03415870566207629,1.4407528042793274,31.731723125164326

# 2025-03-26 21:48:35.734887
+y

# 2025-03-26 21:53:53.466027
+I see some improvement but I believe there are still bugs on the code that is making the training not be effective. Add more logs to gain a better understanding of what is not working. Think about the hyperparameters again. How big is our replay buffer? Here is the data that I have just obtained running the script epoch,critic_loss,actor_loss,q1_value,q2_value,action_mean,action_std,entropy,grad_norm
+1,3.1208647076900187,0.07999568317945187,-0.013389238740693634,-0.017277731826582637,0.003497119643725455,0.474030576646328,0.9163894722094903,3.201055361674382
+2,3.10996550321579,0.05555033561988519,-0.08160319036016098,-0.21215352401710474,0.03302728081157861,0.4873758749319957,0.9111174436715933,3.2404387776668253
+3,3.215599353496845,-0.004369291405265148,-0.15015637315809727,-0.4115088020379727,0.12362078648920242,0.5366833284497261,0.9056236285429734,3.3516498804092407
+4,3.346082655283121,-0.12164227521190277,-0.2228647438952556,-0.6136463479353831,0.28700977162672925,0.570225803897931,0.8997794320950141,3.5508931279182434
+5,3.4618226885795593,-0.3531385872226495,-0.30591411831287235,-0.8166393912755526,0.6083396254823759,0.4094390757381916,0.8936045731489475,3.9253912613942075
+6,3.546765556702247,-0.6333163340504353,-0.4021412959465614,-1.0173992388523543,0.8527504652738571,0.18255452854702106,0.8874799104837271,4.261572654430683
+7,3.5892300055577206,-0.8817421461527164,-0.5068715547139828,-1.2235502715294178,0.9311795773414465,0.09284810850826594,0.882013643017182,4.485906298343952
+8,3.604630681184622,-1.1184717176052241,-0.6190687234585102,-1.4374269522153413,0.9600058415761361,0.055982875351149305,0.877278807071539,4.687127544329717
+9,3.644172104505392,-1.2970002797933726,-0.7073552895050782,-1.606502672800651,0.9702021342057449,0.04336365004285024,0.8739849604093112,4.841149275119488
+10,3.66177729001412,-1.4153812848604643,-0.7672276932459611,-1.7222065788048964,0.9752132216325173,0.037450608833191484,0.8718864871905401,4.935974221963149
+11,3.656008527829097,-1.5332940885653863,-0.8290319775159543,-1.8405342285449688,0.9790517252225143,0.03200903105048033,0.869815816099827,5.031918892493615
+12,3.6469882543270407,-1.6525114820553706,-0.8911954118655279,-1.9618903283889477,0.9823481738567352,0.026827121565404993,0.8677821296911973,5.12325616983267
+13,3.6362613989756656,-1.7737967784588153,-0.9545296728610992,-2.0849598050117493,0.9848969372419211,0.023418142531926815,0.8657786628374686,5.213290535486662
+14,3.623675818626697,-1.8969685893792372,-1.0200660526752472,-2.211452071483319,0.9870254844427109,0.01971548439290088,0.8638118757651403,5.302437553038964
+15,3.6319080728750963,-1.9920306320373828,-1.070682711326159,-2.3083502787810106,0.9882691525495969,0.017704769825706117,0.8623416675971105,5.369306023304279
+16,3.6321802689478946,-2.054343186891996,-1.1044766146403093,-2.3734815304095926,0.9889607635828165,0.016742501336221512,0.8613630762467017,5.4161686805578375
+17,3.624093463787666,-2.1169479397627025,-1.1384154076759632,-2.4404512643814087,0.9895921509999496,0.016247038622028552,0.8603786218624848,5.457758692594675
+18,3.6147877940764794,-2.180520690404452,-1.1736193070044885,-2.505016180185171,0.9906066644650239,0.013965291920332955,0.85939510510518,5.494526175352243
+19,3.6046482783097487,-2.2439474921960096,-1.2079497690384204,-2.572918107876411,0.9912089430368863,0.013545931854213659,0.8584109430129712,5.535742878913879
+20,3.595011504796835,-2.3069483500260572,-1.2435370202247913,-2.641302200464102,0.9916343173155417,0.013517423855284086,0.8574219323121585,5.573334629719074
+21,3.5932688667224,-2.3553186379946194,-1.2707094733531659,-2.693130740752587,0.9921395893280323,0.012238733183879118,0.8566721414144223,5.6062626013389
+22,3.591566168344938,-2.387484357907222,-1.2888913131677187,-2.7278953332167406,0.9924752013041422,0.01130362695011382,0.856173514173581,5.629080405602088
+23,3.587015748023987,-2.4186943540206323,-1.3065452185960917,-2.762478837600121,0.9925545855210378,0.011650008525556097,0.85567056788848,5.645477093183077
+24,3.5796416585262003,-2.4506602470691385,-1.3251385849255781,-2.796543932878054,0.9928861432350599,0.011087582912296057,0.8551625082126031,5.662684834920443
+25,3.5745545763235826,-2.4815985652116628,-1.3425681109611804,-2.832135420579177,0.9930104475754958,0.010944017450898312,0.8546499965282587,5.682682000673735
+26,3.5682539022885837,-2.513285366388468,-1.361298801807257,-2.867460842315967,0.9932581851115594,0.010179743519984186,0.8541353688790247,5.6988932627898
+27,3.571958514360281,-2.5371749767890344,-1.375139940243501,-2.893632146028372,0.9934023802097027,0.01032659752616802,0.853741970199805,5.712156323286203
+28,3.571463956282689,-2.5526892955486593,-1.3844172152189107,-2.91145704801266,0.9934589564800262,0.010429611948963542,0.8534792065620422,5.722324646436251
+29,3.56777769785661,-2.568218034047347,-1.3929050129193525,-2.930242964854607,0.9936593759518403,0.009645692959356193,0.853214219212532,5.73252457838792
+30,3.56274859721844,-2.5839044956060557,-1.4028598757890554,-2.9478755364051232,0.993762378509228,0.00915977367773079,0.8529492914676666,5.737576732268701
+31,3.559795457583207,-2.599171367975382,-1.4120306235093336,-2.9659297237029443,0.9938455178187444,0.00968922649581845,0.8526805559029946,5.744153362054091
+32,3.5577314037543077,-2.6150813423670254,-1.4213418456224294,-2.9829660883316627,0.9939603301194998,0.009181072684721304,0.8524094808560151,5.750865771220281
+33,3.5591786228693447,-2.627323705416459,-1.4281576596773589,-2.9964969800068784,0.9941476170833294,0.00911213357288104,0.8522044958976599,5.756750060961797
+34,3.5575789167330814,-2.6351029185148387,-1.433014683998548,-3.006932882162241,0.9942226524536426,0.008618512459529134,0.8520680849368756,5.7670198403871975
+35,3.5559413387225223,-2.642083566922408,-1.4375420556618617,-3.0150456611926737,0.9940945700957224,0.009417883218982471,0.8519306664283459,5.767565727233887
+36,3.5577978308384237,-2.6501012994692874,-1.4421615646435664,-3.0244752718852115,0.9942451142347776,0.008876163754254006,0.8517913176463201,5.773710580972525
+37,3.5536407507382908,-2.65788197517395,-1.4468159354650056,-3.032455421411074,0.9943280747303596,0.008565337289697848,0.8516515459005649,5.772572086407588
+38,3.552242943873772,-2.6656130873239956,-1.4514653499309833,-3.041697832254263,0.994455771950575,0.008513690850052696,0.8515115460524192,5.778410141284649
+39,3.5535483222741346,-2.6713328820008497,-1.4549500621282137,-3.049101022573618,0.9943735633905117,0.008630618112734877,0.8514051723938721,5.781687727341285
+40,3.5525902693088236,-2.675220177723811,-1.4572134476441603,-3.0537688457048855,0.9944600795324032,0.008538586741241697,0.8513338749225323,5.783372429700998
+41,3.552335881269895,-2.6792798913442173,-1.4597273858693929,-3.057346752056709,0.9945614441083028,0.008325603889086498,0.8512625751587061,5.784073380323557
+42,3.5514016930873575,-2.682681922729199,-1.4620459515314836,-3.0624255492137027,0.994492130783888,0.008286212830661008,0.8511910919959729,5.788143295508164
+43,3.552041475589459,-2.686484795350295,-1.4639832377433777,-3.0674996009239783,0.9944988374526684,0.008526234690529795,0.851119109071218,5.7923925289740925
+44,3.551466396221748,-2.6902759029315066,-1.4667291113963494,-3.071066521681272,0.9944946284477527,0.008440790366596328,0.851046078480207,5.7877117945597725
+45,3.550675855233119,-2.692997244688181,-1.4685051349493174,-3.0749139923315782,0.9945495598591291,0.008580435362931054,0.8509904134732026,5.795851744138277
+46,3.54831915635329,-2.69479448061723,-1.4694482317337623,-3.0767635152890134,0.9945814747076768,0.008495615231088148,0.8509532224673492,5.793445752217219
+47,3.548002852843358,-2.6968411849095273,-1.4704072933930616,-3.0805436143508325,0.994578691629263,0.008236264854741212,0.8509160303152524,5.795134938680208
+48,3.552241252018855,-2.6987283413226786,-1.4717477720517378,-3.0821406107682447,0.9946291630084698,0.008497091010212898,0.8508788370169126,5.794119559801542
+49,3.549870651501876,-2.7006884400661173,-1.472823143005371,-3.084304364827963,0.994631466957239,0.0084289436479314,0.8508416402798432,5.794396868118873
+50,3.5511383093320408,-2.7022172808647156,-1.4741383080299084,-3.0857982497948866,0.9945933314470145,0.00827752474623804,0.8508044492739898,5.797620489047124
+51,3.549210543815906,-2.7039913672667284,-1.474614257995899,-3.0880079177709727,0.9946265048705615,0.008297970975176074,0.8507763697550847,5.798457053991465
+52,3.5508542335950413,-2.704959227488591,-1.4757592448821435,-3.0887190791276784,0.994658852999027,0.00831224186381755,0.8507577742521579,5.800737958688003
+53,3.55255396091021,-2.7061323798619785,-1.4759019659115717,-3.0891780899121213,0.994720370723651,0.007987304581687428,0.850739184480447,5.794455335690425
+54,3.5492297548514147,-2.706780282350687,-1.4765051626242125,-3.0915796802594113,0.9946732016710135,0.007988633131250165,0.8507205889775202,5.7990009142802315
+55,3.5492092829484205,-2.707864114871392,-1.477603561603106,-3.0930998692145715,0.9946850538253784,0.008161526316633591,0.8507019946208367,5.798512651370122
+56,3.549861431121826,-2.708828939841344,-1.4777790560172155,-3.093049691273616,0.9947329988846412,0.008173830493783148,0.8506833842167487,5.800152824475215
+57,3.5497015668795657,-2.709347582780398,-1.4782644051771898,-3.095144748687744,0.9947122816856091,0.00824646505777939,0.8506693519078768,5.801528683075538
+58,3.5499112147551317,-2.709817056472485,-1.4788303627417638,-3.095885533552903,0.9946460437316161,0.008644222675894316,0.8506600570220214,5.803182345170241
+59,3.5509328888012814,-2.709566437281095,-1.478418150773415,-3.0978970481799197,0.9947236214692776,0.00824439713659768,0.8506507552587069,5.802094789651724C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
+  warnings.warn(

# 2025-03-26 21:55:47.975594
+y

# 2025-03-26 21:59:35.413838
+:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
+  warnings.warn(
+Training: 100%|| 200/200 [02:00<00:00,  1.65it/s, Critic Loss=2.099, Actor Loss=-64.734, Q Values=-4.398]
+Training complete. Model saved to sac_model.pth
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 405, in <module>
+    analyze_training_log(
+    ~~~~~~~~~~~~~~~~~~~~^
+        log_path="training_logs/training_log.csv",
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+        output_dir="training_analysis"
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 369, in analyze_training_log
+    ax = axs[idx//3, idx%3]
+         ~~~^^^^^^^^^^^^^^^
+IndexError: index 3 is out of bounds for axis 0 with size 3

# 2025-03-26 22:00:46.772720
+y

# 2025-03-26 22:03:08.545302
+The actor loss keeps going negative and the action_std keeps going zero. Propose solutions to the code just by fixing possible bugs and adapting parameters (do not introduce weird things). 

# 2025-03-26 22:05:19.847937
+y

# 2025-03-26 22:10:05.666120
+I want to test SAC without CQL to issolate the issue. How to Test SAC Without CQL
+Remove the CQL Penalty:
+
+Set α_cql = 0 in the critic loss:
+
+L
+critic
+=
+TD Loss (SAC)
+(
+no CQL term
+)
+L 
+critic
+
+ =TD Loss (SAC)(no CQL term)
+Keep all other components (twin Q-networks, target networks, entropy tuning) intact.
+
+Adjust Hyperparameters:
+
+Increase entropy regularization (higher α or target entropy) to encourage exploration.
+
+Lower the actor learning rate if the policy still collapses too quickly.
+
+Monitor Key Metrics:
+
+Q-values: If they explode (become extremely large), SAC is suffering from overestimation bias.
+
+Entropy/action_std: Ensure they dont collapse to 0.
+
+Actor Loss: Should fluctuate but not plunge to extreme negative values.

# 2025-03-26 22:12:48.737646
+y

# 2025-03-26 22:21:02.097747
+is the training working now? if you see some trends that could still be improved, suggest some changes and justify it. All the changes should be to simplify the code, not increase complexity. epoch,critic_loss,actor_loss,q1_value,q2_value,action_mean,action_std,entropy,grad_norm,log_std_mean,alpha,lr
+1,48.344684013953575,1.8699569197801442,-0.17810788521399865,0.030916351180237073,-0.0009460210477790007,0.6291848764969752,1.4188131460776696,39.84813836904672,-0.00026150658959522843,0.9997385144233704,1e-05
+2,42.74799112173227,2.1646786561379066,-0.4820975134005913,-0.28690536549458134,0.0011654225572084005,0.6261767286520737,1.4185487765532274,37.447567866398735,-0.0005296582821756601,0.9994704723358154,1e-05
+3,37.105497066791244,2.469459451161898,-0.7998192447882432,-0.6144183278083801,-0.001528555634789742,0.6290883398973025,1.4182763053820684,35.23962328984187,-0.0008069827454164624,0.9991933107376099,1e-05
+4,31.61755004295936,2.7845399104631863,-1.1390714874634376,-0.959066058580692,0.002699147891969635,0.6258358771984394,1.4179945313013518,33.10237004206731,-0.0010937106562778354,0.9989068508148193,1e-05
+5,26.290072221022385,3.1155088314643273,-1.5048654171136708,-1.327046839090494,0.0018475081485051375,0.6275930977784671,1.4177027253004222,30.809336955730732,-0.0013913859147578478,0.9986096024513245,1e-05
+6,21.219593341533955,3.4597337521039524,-1.898091027369866,-1.72139134315344,0.0015373010806237848,0.6289841761955848,1.41739860864786,28.130129520709698,-0.0017031843308359385,0.9982982873916626,1e-05
+7,16.488187863276554,3.814746563251202,-2.318785383151128,-2.1478584683858433,0.004338518788035099,0.6267613974901346,1.4170775505212636,24.934578455411472,-0.0020353635773062706,0.9979666471481323,1e-05
+8,12.24762197641226,4.169880692775433,-2.762000551590553,-2.609083890914917,0.011118383549798567,0.6270796473209674,1.4167324992326589,21.11287366426908,-0.0023964031133800745,0.9976064562797546,1e-05
+9,8.667160474337065,4.516612548094529,-3.2168121337890625,-3.1048747301101685,0.01159355123169147,0.628200161915559,1.4163513550391564,16.64622317827665,-0.0028030485846102238,0.9972008466720581,1e-05
+10,5.9098759064307576,4.827157570765569,-3.6611218452453613,-3.6314597680018497,0.009581334369543653,0.6280465676234319,1.4159116194798396,11.65472041643583,-0.003285819198936224,0.9967195987701416,1e-05
+11,4.029569754233727,5.048460025053758,-4.050841597410349,-4.163992386597854,0.005133922695397184,0.6260099204686972,1.4153749530132,6.790425007159893,-0.0038885080721229315,0.9961190223693848,1e-05
+12,2.889687941624568,5.0439500625316915,-4.304698815712562,-4.589167338151198,0.01999598229303956,0.6251189135588132,1.4147150149712195,3.7241223592024584,-0.004591146484017372,0.9954193830490112,1e-05
+13,2.171265001480396,4.864010847531832,-4.405112211520855,-4.757586607566247,0.019911803045453362,0.6310361463289994,1.4140217808576732,3.03710767856011,-0.005254133604466915,0.9947596788406372,1e-05
+14,1.6649044706271245,4.628310460310716,-4.463024249443641,-4.799018584764921,0.02648855708181285,0.6292459185306842,1.4134019246468177,2.936196859066303,-0.005829635541886091,0.9941872954368591,1e-05
+15,1.3175764175561757,4.378618533794697,-4.504867131893452,-4.831162562737098,0.030521162561714076,0.6291343661455008,1.4128565192222595,2.923917981294485,-0.006342696025967598,0.9936773777008057,1e-05
+16,1.0772182391240046,4.1271930657900295,-4.538192547284639,-4.855170891835139,0.03736648062029137,0.6310162429626172,1.4123687606591444,2.957545491365286,-0.006808740086853504,0.9932143688201904,1e-05
+17,0.8950831890106201,3.8952850653575015,-4.568363501475408,-4.8779030946584845,0.03301330772228539,0.6330874103766221,1.4119204787107615,3.0668328541975756,-0.00724057387560606,0.9927855730056763,1e-05
+18,0.7747641045313615,3.629041836811946,-4.586684465408325,-4.892559913488535,0.041031224420294166,0.6309641897678375,1.4115023704675527,3.245303071462191,-0.007641565520316362,0.9923875331878662,1e-05
+19,0.6897516732032483,3.3515585844333353,-4.604715512349055,-4.9038050541510945,0.04953186249790283,0.629513545678212,1.4111166183765118,3.43721813421983,-0.00801195576786995,0.9920200705528259,1e-05
+20,0.6291370265758954,3.1048542811320377,-4.621269867970393,-4.914389206812932,0.05347364875846184,0.6280597219100366,1.4107569043452923,3.681397346349863,-0.00836184062063694,0.9916729927062988,1e-05
+21,0.5950303559119885,2.830276397558359,-4.630057958456186,-4.923014035591712,0.061667502571183905,0.6337490609058967,1.410417079925537,3.9220578487102804,-0.008689465001225471,0.991348147392273,1e-05
+22,0.5752608489531738,2.5625466933617225,-4.64080733519334,-4.928740354684683,0.06456381698640493,0.6311626801123986,1.41009809420659,4.225572466850281,-0.00899837538599968,0.9910420179367065,1e-05
+23,0.5557121634483337,2.3339621470524716,-4.653871627954336,-4.937420533253596,0.06576334276738074,0.6377143515990331,1.4097975217379057,4.487349436833308,-0.009293651208281517,0.9907494187355042,1e-05
+24,0.5524258590661563,2.0486426720252404,-4.655555743437547,-4.941053152084351,0.07351674984854001,0.6327710105822637,1.4095058624561017,4.826896924238938,-0.009579513221979141,0.9904662370681763,1e-05
+25,0.5513139882913003,1.8246899338868947,-4.663060720150288,-4.944678068161011,0.07161622637739548,0.6361539455560538,1.4092255876614497,5.142190548089834,-0.009855162352323532,0.990193247795105,1e-05
+26,0.5518127496425922,1.573747387299171,-4.671399153195894,-4.951381481610811,0.07982024020300461,0.6357155465162717,1.4089536300072303,5.482489549196684,-0.010122687555849552,0.9899284243583679,1e-05
+27,0.5571590994413083,1.289644548526177,-4.676936553074763,-4.951573115128737,0.08505714097275184,0.6373334527015686,1.4086912824557378,5.853121849206778,-0.01038086786866188,0.9896728992462158,1e-05
+28,0.5715199869412643,1.0340864727130303,-4.677204352158767,-4.952001113158006,0.08720817431234397,0.6398565494097196,1.4084359590823834,6.210744766088633,-0.010634372010827065,0.989422082901001,1e-05
+29,0.5775843056348654,0.7290280713484838,-4.683349957832923,-4.958913362943209,0.0958644413890747,0.6359837972200834,1.4081833545978253,6.597934612861047,-0.010884132236242294,0.9891749620437622,1e-05
+30,0.5866968093010095,0.46038901118131786,-4.6914472579956055,-4.961775192847619,0.09769783340967618,0.6396510646893427,1.4079371461501489,7.0074181373302755,-0.011127148754894733,0.9889346361160278,1e-05
+31,0.5990718603134155,0.21205024535839373,-4.69128603201646,-4.962540223048284,0.10062903122833142,0.6388472822996286,1.4076955364300654,7.442376430218037,-0.011367845349013805,0.9886966943740845,1e-05
+32,0.6128829534237201,-0.08512266324116634,-4.690737687624418,-4.963507413864136,0.11299526433532055,0.6381654945703653,1.4075139806820796,7.685819754233727,-0.011485489085316658,0.9885803461074829,5e-06
+33,0.6157614256326969,-0.2119203255726741,-4.695799185679509,-4.964871058097253,0.11483683666357627,0.6365706370427058,1.4073969767643855,8.004449954399696,-0.011602304875850677,0.9884649515151978,5e-06
+34,0.627229283635433,-0.32526657673028797,-4.695009029828585,-4.965286108163687,0.11324915719720033,0.6403103929299575,1.4072802662849426,8.241463404435377,-0.011719164438545704,0.988349437713623,5e-06
+35,0.6344186457303854,-0.49247077795175404,-4.697483502901518,-4.965652099022498,0.11873088232599772,0.6403781129763677,1.4071630514585054,8.378714946600107,-0.011835871264338493,0.9882341027259827,5e-06
+36,0.6371987324494582,-0.6735117389605596,-4.703614565042349,-4.9667448447300835,0.12879732251167297,0.6405166318783393,1.4070473634279692,8.60957827934852,-0.011950071901082993,0.9881212711334229,5e-06
+37,0.648114011837886,-0.7660538920989404,-4.701089730629554,-4.967068598820613,0.12191488880377549,0.6440253968422229,1.4069330600591807,8.855720923497127,-0.012065980583429337,0.9880067110061646,5e-06
+38,0.6587906250586877,-0.8829761560146625,-4.7007990433619575,-4.968175392884475,0.12370234469954784,0.6422467598548303,1.4068445471616893,9.025869772984432,-0.012124095112085342,0.9879493713378906,2.5e-06
+39,0.6612329563269248,-1.0011040980999286,-4.70237895158621,-4.967853931280283,0.12900512493573701,0.6425491754825299,1.4067862584040716,9.045703961299015,-0.01218264177441597,0.9878914952278137,2.5e-06
+40,0.6619314001156733,-1.0220045080551734,-4.704121773059551,-4.968282974683321,0.12544840488296288,0.6452476726128504,1.406727983401372,9.143460273742676,-0.012240858748555183,0.9878339767456055,2.5e-06
+41,0.6655064248121701,-1.0948008390573354,-4.704842934241662,-4.9692637370182915,0.12714576320006296,0.6389343830255362,1.4066696350391095,9.358961178706242,-0.012299198657274246,0.9877763986587524,2.5e-06
+42,0.6698991885552039,-1.195504789169018,-4.705471662374643,-4.969758492249709,0.1293922129731912,0.6417050109459803,1.40661117205253,9.4004336137038,-0.012357627972960472,0.9877187013626099,2.5e-06
+43,0.6756797994558628,-1.2711659578176646,-4.705601343741784,-4.969992802693294,0.13107957117832625,0.6453113487133613,1.4065526815561147,9.407719025245079,-0.012416308745741844,0.9876607060432434,2.5e-06
+44,0.6776889127034408,-1.3873735116078303,-4.707410427240225,-4.9697831594027,0.13793536820090735,0.6424969297188979,1.4065081889812763,9.637886487520658,-0.012445289641618729,0.9876320958137512,1.25e-06
+45,0.6787693729767432,-1.3932943894312932,-4.7074151405921345,-4.970233880556547,0.13491240831521842,0.6401103093073919,1.4064793632580683,9.719529371995192,-0.01247425377368927,0.987603485584259,1.25e-06
+46,0.683182989175503,-1.387885148708637,-4.707432783566988,-4.970314282637376,0.13146505447534415,0.6433572769165039,1.4064500286028936,9.812847907726582,-0.012503748759627342,0.9875743389129639,1.25e-06
+47,0.6845697519870905,-1.4872853893500109,-4.708836665520301,-4.9703901914449835,0.1377519741654396,0.6428446769714355,1.406420959876134,9.779376690204327,-0.012532589957118034,0.9875458478927612,1.25e-06
+48,0.6855237827851222,-1.5157882708769579,-4.709759968977708,-4.970507199947651,0.1368959445792895,0.642321547636619,1.4063917765250573,9.86510493205144,-0.01256206538528204,0.9875167608261108,1.25e-06
+49,0.6856481914336865,-1.4685819515815148,-4.709227048433744,-4.969973839246309,0.1281849486896625,0.643506455879945,1.4063622447160573,9.924289630009579,-0.012591816484928131,0.9874874353408813,1.25e-06
+50,0.6930036934522482,-1.5667992646877582,-4.7078920511099005,-4.97058917925908,0.13486810515706354,0.6450020349942721,1.4063395170065074,9.875884276169996,-0.01260673813521862,0.987472653388977,6.25e-07
+51,0.6908904142104663,-1.5806709023622365,-4.709743609795203,-4.970752972822923,0.13596665372069067,0.6424859005671281,1.4063246387701769,10.035176093761738,-0.01262154895812273,0.9874580502510071,6.25e-07
+52,0.6923070870912992,-1.6175930224932158,-4.710187875307524,-4.970600476631751,0.14064965196526968,0.6406887059028332,1.4063099255928626,10.132172181056095,-0.012636225670576096,0.9874435663223267,6.25e-07
+53,0.6956806652821027,-1.621866991886726,-4.70940454189594,-4.970676440459031,0.13696883160334367,0.6406707167625427,1.4062952215854938,10.020891483013447,-0.012650985270738602,0.9874289631843567,6.25e-07
+54,0.6934175307934101,-1.534504349415119,-4.710014746739314,-4.970828294754028,0.12510046764062002,0.6427266483123486,1.4062802608196552,10.149632600637583,-0.012666184455156326,0.9874140024185181,6.25e-07
+55,0.695332904274647,-1.6641068091759315,-4.710228754923894,-4.9717895434452934,0.13901207137566346,0.6435876144812658,1.406265112069937,10.104337068704458,-0.012681221589446068,0.9873991012573242,6.25e-07
+56,0.6976788273224463,-1.704794874558082,-4.710450740960928,-4.9713664054870605,0.13979780559356397,0.6421045569273142,1.4062537550926208,10.09784592114962,-0.012688616290688515,0.9873918294906616,3.125e-07
+57,0.695394937808697,-1.6598692811452425,-4.711275100708008,-4.971142457081721,0.13706650986121252,0.6436491310596466,1.4062463136819692,10.14700306378878,-0.012696107849478722,0.9873844385147095,3.125e-07
+58,0.6969600526186136,-1.6938561109396129,-4.710821518531213,-4.971375483732957,0.14032975383676016,0.6414050689110389,1.4062387943267822,10.18459466787485,-0.012703601270914078,0.9873770475387573,3.125e-07
+59,0.6970435014137855,-1.6896111094034636,-4.7125795254340535,-4.971474189024705,0.13718473997253638,0.6444623080583719,1.4062313024814312,10.16913362649771,-0.01271112821996212,0.9873696565628052,3.125e-07
+60,0.6978389827104715,-1.7288418687306917,-4.712342427327083,-4.971216990397527,0.1395869332437332,0.6414153873920441,1.4062237785412715,10.241812705993652,-0.012718653306365013,0.9873621463775635,3.125e-07
+61,0.6982782150690372,-1.713108452466818,-4.711428568913386,-4.971904112742497,0.13654995910250223,0.6413909655350906,1.4062161904114943,10.311742342435396,-0.012726260349154472,0.9873546361923218,3.125e-07
+62,0.7001137045713571,-1.741628137918619,-4.711308387609629,-4.9712156515855055,0.13916092623884863,0.6446555509017065,1.4062104316858144,10.129141587477465,-0.012730039656162262,0.9873509407043457,1.5625e-07
+63,0.6989898383617401,-1.753194822714879,-4.71162042251,-4.971909119532659,0.1384739947433655,0.6458074886065263,1.406206644498385,10.071607112884521,-0.01273384876549244,0.9873471260070801,1.5625e-07
+64,0.6996910915924952,-1.684629385287945,-4.712037453284631,-4.97142217709468,0.1332069099522554,0.6449600824942956,1.4062028114612286,10.206939477186937,-0.012737693265080452,0.9873433709144592,1.5625e-07
+65,0.6983300630862896,-1.7484252865497882,-4.712471595177283,-4.97123907162593,0.13860649730150515,0.6402134390977713,1.4061989784240723,10.402705742762638,-0.012741507962346077,0.9873396158218384,1.5625e-07
+66,0.7012107601532569,-1.7268659701714149,-4.712489494910607,-4.971675432645357,0.13740914314985275,0.6435850377266223,1.4061951820666974,10.239790329566368,-0.01274531427770853,0.9873358607292175,1.5625e-07
+67,0.7015385696521172,-1.71540105342865,-4.711571895159208,-4.971559322797335,0.13380694790528372,0.6475734298045819,1.4061913581994863,10.268535760732798,-0.01274915225803852,0.9873321056365967,1.5625e-07
+68,0.7000190168619156,-1.7763255146833568,-4.71206703552833,-4.97156625527602,0.14268538986261076,0.6457628905773163,1.4061884467418377,10.183358449202318,-0.012751035392284393,0.9873301982879639,7.8125e-08
+69,0.699504201228802,-1.739743553675138,-4.713063918627226,-4.971929971988384,0.1382594294846058,0.6415524024229783,1.406186566903041,10.297208932729868,-0.012752950191497803,0.987328290939331,7.8125e-08
+70,0.703253285242961,-1.7711177101502051,-4.71248014156635,-4.971560386510996,0.13532225501078826,0.6443013755174783,1.4061846274595995,10.212985845712515,-0.012754879891872406,0.9873263835906982,7.8125e-08
+71,0.6999450005017794,-1.7335725289124708,-4.71328643652109,-4.971561945401705,0.13562355018579042,0.6418883273234735,1.4061827155259938,10.253135937910814,-0.012756818905472755,0.9873244762420654,7.8125e-08
+72,0.6995046597260696,-1.7697031314556415,-4.713019334352934,-4.971132058363694,0.1379573981349285,0.6424807539353004,1.406180780667525,10.286979308495155,-0.012758753262460232,0.9873225688934326,7.8125e-08
+73,0.7013234106393961,-1.804710566997528,-4.712448101777297,-4.971505293479333,0.1411106910269994,0.6423554030748514,1.4061788412240834,10.212209701538086,-0.01276068203151226,0.9873206615447998,7.8125e-08
+74,0.7002658545970917,-1.7625916737776537,-4.713129171958337,-4.971716972497793,0.14017911685200837,0.6441108813652625,1.4061773694478548,10.244056224822998,-0.012761642225086689,0.9873197078704834,3.90625e-08
+75,0.7012534118615664,-1.763185666157649,-4.712661926562969,-4.971637634130625,0.13480086939839217,0.6431701091619638,1.4061764157735384,10.381322347200834,-0.01276260707527399,0.987318754196167,3.90625e-08
+76,0.7017162488057063,-1.7949166664710412,-4.711675222103413,-4.972068383143498,0.13994856436665243,0.6444529294967651,1.4061754529292767,10.356022578019362,-0.012763562612235546,0.9873178005218506,3.90625e-08
+77,0.6996564200291266,-1.7331810822853675,-4.713643000676082,-4.971778667890108,0.1347871574644859,0.6431864179097689,1.4061744855000422,10.346962525294376,-0.012764533050358295,0.9873168468475342,3.90625e-08
+78,0.7001752303196833,-1.7462623898799603,-4.713778459108793,-4.971478884036724,0.13664408257374397,0.6420943484856532,1.4061735226557806,10.340524600102352,-0.01276550441980362,0.9873158931732178,3.90625e-08
+79,0.7004002951658689,-1.7770576843848596,-4.71308420254634,-4.972114581328172,0.13854040721288094,0.6434919513188876,1.406172541471628,10.205430581019474,-0.012766474857926369,0.9873149394989014,3.90625e-08
+80,0.700161704650292,-1.72645765542984,-4.7131709685692424,-4.972065210342407,0.13719732858813727,0.6448848453851846,1.4061718124609728,10.34166380075308,-0.012766960076987743,0.9873144626617432,1.953125e-08
+81,0.7010206488462595,-1.7068027670566852,-4.712895668469942,-4.972093802232009,0.13308028446940276,0.6426706680884728,1.4061713172839239,10.304774027604322,-0.01276745181530714,0.987313985824585,1.953125e-08
+82,0.7011966200975271,-1.8123575357290416,-4.712834468254676,-4.972159954217764,0.14508171007037163,0.6420200971456674,1.406170835861793,10.23363421513484,-0.012767938897013664,0.9873135089874268,1.953125e-08
+83,0.7007293758484033,-1.795530942770151,-4.713703852433425,-4.9720596716954155,0.14203304510850173,0.6442550535385425,1.4061703498546894,10.221892283512997,-0.012768423184752464,0.9873130321502686,1.953125e-08
+84,0.7021827422655545,-1.793592645571782,-4.712788068331205,-4.971627913988554,0.1417514678950493,0.6440355319243211,1.406169863847586,10.158100384932299,-0.012768907472491264,0.9873125553131104,1.953125e-08
+85,0.7025913023031675,-1.7744757395524244,-4.713026450230525,-4.971901746896597,0.13877187554652876,0.6448675027260413,1.4061693778404822,10.258671870598427,-0.012769391760230064,0.9873120784759521,1.953125e-08
+86,0.7000596271111414,-1.757405019723452,-4.713505066358126,-4.971824407577515,0.13416763309102792,0.6460099220275879,1.4061688918333788,10.225694839770977,-0.01276988536119461,0.987311601638794,1.953125e-08
+87,0.7022679425202883,-1.7369011778097887,-4.7131562599769,-4.97202310195336,0.13284015082395995,0.6446897295805124,1.4061684012413025,10.283921718597412,-0.012770377099514008,0.9873111248016357,1.953125e-08
+88,0.7023863540245936,-1.7465528799937322,-4.7130482747004585,-4.971803903579712,0.1348364376104795,0.6444777571238004,1.4061679060642536,10.316034720494198,-0.012770871631801128,0.9873106479644775,1.953125e-08
+89,0.702348544047429,-1.7632271601603582,-4.7129645714393025,-4.971744482333843,0.13364267291931006,0.6421087200825031,1.4061674017172594,10.361144395974966,-0.012771360576152802,0.9873101711273193,1.953125e-08
+90,0.699845573076835,-1.733741324681502,-4.71246077464177,-4.972032106839693,0.13414701991356337,0.6424210644685305,1.4061669248801012,10.377868212186373,-0.01277184672653675,0.9873096942901611,1.953125e-08
+91,0.7029451063046088,-1.8143507196353033,-4.712428239675669,-4.971956858268151,0.14250674270666563,0.6440073939470145,1.4061664388729975,10.174899541414701,-0.012772337533533573,0.9873092174530029,1.953125e-08
+92,0.7009659329285989,-1.7889294119981618,-4.712766849077665,-4.971683630576501,0.138615337988505,0.6427389291616586,1.4061659482809215,10.274140101212721,-0.012772824615240097,0.9873087406158447,1.953125e-08
+93,0.703208143894489,-1.777096858391395,-4.712599240816557,-4.9722191370450535,0.1381658436014102,0.6424930347846105,1.4061654485188997,10.30868647648738,-0.012773323804140091,0.9873082041740417,1.953125e-08
+94,0.7043121640498822,-1.7741612104269175,-4.712299291904156,-4.971850010064932,0.13860498110835368,0.6438096876327808,1.4061649579268236,10.257520602299618,-0.012773810885846615,0.9873076677322388,1.953125e-08
+95,0.7002837016032293,-1.8091890445122352,-4.713192591300378,-4.972089070540208,0.1410900534918675,0.6445028139994695,1.4061644765046926,10.223846398867094,-0.012774300761520863,0.9873071908950806,1.953125e-08
+96,0.7028887031170038,-1.7739147819005525,-4.712405186433059,-4.97194609275231,0.13830088250912154,0.6415780599300678,1.4061639721576984,10.34738614008977,-0.012774791568517685,0.9873067736625671,1.953125e-08
+97,0.7023204289949857,-1.8015102744102478,-4.713496721707857,-4.9719579219818115,0.1388537362217903,0.6421132935927465,1.4061634953205402,10.322833464695858,-0.012775273993611336,0.9873062372207642,1.953125e-08
+98,0.6993490927494489,-1.7614992123383741,-4.7135323377755975,-4.971341444895818,0.13389196562079284,0.6435363728266495,1.406163004728464,10.325426321763258,-0.012775769457221031,0.987305760383606,1.953125e-08
+99,0.7019780255281008,-1.7617831505261934,-4.7125650736001825,-4.972375924770649,0.13782052065317446,0.643962887617258,1.4061625141363878,10.296109713040865,-0.012776259332895279,0.9873052835464478,1.953125e-08
+100,0.7051317370854892,-1.8017273316016564,-4.711829497263982,-4.9715294654552755,0.14130963614353767,0.6411759211466863,1.4061620235443115,10.378402820000282,-0.012776743620634079,0.9873048067092896,1.953125e-08
+101,0.7013196578392615,-1.810042495910938,-4.713027935761672,-4.971539607414832,0.14050280245450827,0.6419863036045661,1.406161537537208,10.298425087561974,-0.012777234427630901,0.9873043298721313,1.953125e-08
+102,0.7008149188298446,-1.8159752029639025,-4.712922151272114,-4.972128593004667,0.14229049247044784,0.6449987108890827,1.406161056115077,10.316619506249062,-0.012777718715369701,0.9873038530349731,1.953125e-08
+103,0.7020891824593911,-1.7613612780204186,-4.713200569152832,-4.972025156021118,0.1389768272638321,0.6428391131070944,1.406160560938028,10.30735364327064,-0.012778208591043949,0.9873033761978149,1.953125e-08
+104,0.7028424109403903,-1.7003300923567553,-4.712813010582557,-4.971556260035588,0.1294264867901802,0.6466251703409048,1.4061600657609792,10.30552126811101,-0.012778708711266518,0.9873028993606567,1.953125e-08
+105,0.700709751019111,-1.7840571907850413,-4.713591190484854,-4.9720402130713826,0.13814682026322073,0.6437976291546454,1.4061595705839305,10.279361798213078,-0.012779204174876213,0.9873024225234985,1.953125e-08
+106,0.701608637204537,-1.7937558751839857,-4.713062799893892,-4.971701530309824,0.1393363607617525,0.6452140051584977,1.4061590799918542,10.260535460252028,-0.012779701501131058,0.9873019456863403,1.953125e-08
+107,0.7015649011501899,-1.7746716783596919,-4.712178450364333,-4.971549382576575,0.13857745350553438,0.6434961282289945,1.4061585848148053,10.353039668156551,-0.012780187651515007,0.9873014092445374,1.953125e-08
+108,0.7024393242139083,-1.8163175857984102,-4.713773635717539,-4.97181433897752,0.14366217731283262,0.645787924528122,1.4061580850527837,10.10454012797429,-0.012780681252479553,0.9873009324073792,1.953125e-08
+109,0.7011041457836444,-1.709573534818796,-4.714346537223229,-4.971653736554659,0.13267317156379038,0.6445214610833389,1.40615759904568,10.265228821681095,-0.012781183235347271,0.9873003959655762,1.953125e-08
+110,0.7030754937575414,-1.7794869679671068,-4.712615251541138,-4.971982057277973,0.1372160129249096,0.6423678054259374,1.4061570946986859,10.283217980311466,-0.012781684286892414,0.987299919128418,1.953125e-08
+111,0.7023465725091788,-1.711598359621488,-4.713356384864221,-4.971952493374165,0.13041399763180658,0.6411590484472421,1.4061565903516917,10.31390879704402,-0.012782183475792408,0.9872994422912598,1.953125e-08
+112,0.7002862210457141,-1.811475552045382,-4.71405997643104,-4.971911980555608,0.1423731612471434,0.6428131186045133,1.4061560951746428,10.220325506650484,-0.012782678939402103,0.9872989654541016,1.953125e-08
+113,0.7009249455653704,-1.7548752243702228,-4.714185219544631,-4.9719415077796345,0.13582096306177285,0.641888650564047,1.4061555999975939,10.372300954965445,-0.012783173471689224,0.9872984886169434,1.953125e-08
+114,0.7017534868075297,-1.757205293728755,-4.713938621374277,-4.971689242583055,0.1385997854746305,0.6432017179635855,1.406155104820545,10.238951132847713,-0.012783670797944069,0.9872980117797852,1.953125e-08
+115,0.7007421484360328,-1.774416630084698,-4.713473154948308,-4.971550372930674,0.13584513446459404,0.646107017993927,1.4061546004735506,10.25988483428955,-0.012784172780811787,0.987297534942627,1.953125e-08
+116,0.6988673427930245,-1.786327444590055,-4.71425384741563,-4.971924323302049,0.14081212906883314,0.6425795623889337,1.4061541144664471,10.232891082763672,-0.012784669175744057,0.987296998500824,1.953125e-08
+117,0.7023771336445441,-1.7871464307491596,-4.71420170710637,-4.971540982906635,0.13415757967875555,0.6486338995970212,1.4061536147044256,10.116316611950214,-0.0127851702272892,0.987296462059021,1.953125e-08
+118,0.7007234921822181,-1.7755398200108454,-4.713903940640963,-4.971751268093403,0.1355821547599939,0.6448457172283759,1.406153114942404,10.240365248460035,-0.012785663828253746,0.9872959852218628,1.953125e-08
+119,0.7015377787443308,-1.7988196703103871,-4.712496060591477,-4.971970154688909,0.1405474113730284,0.6435273564778842,1.4061526151803823,10.235229969024658,-0.01278616115450859,0.9872955083847046,1.953125e-08
+120,0.7005501183179709,-1.783159498984997,-4.714133721131545,-4.971840583361113,0.1375406117966542,0.6444119581809411,1.4061521154183607,10.320063187525822,-0.012786662206053734,0.9872950315475464,1.953125e-08
+121,0.7005042548363025,-1.7858469302837665,-4.7134695786696215,-4.971768544270442,0.13727466905346283,0.6416051066838778,1.4061516110713665,10.298476512615498,-0.012787164188921452,0.9872945547103882,1.953125e-08
+122,0.7030845284461975,-1.8143943731601422,-4.713145347741934,-4.971523651709924,0.14421206827347094,0.6408214431542617,1.4061511158943176,10.339730776273287,-0.012787654995918274,0.98729407787323,1.953125e-08
+123,0.7010187002328726,-1.7922461537214427,-4.712896090287429,-4.971818208694458,0.14212099462747574,0.6393557351369125,1.4061506207172687,10.395121317643385,-0.012788151390850544,0.9872936010360718,1.953125e-08
+124,0.7032387783894172,-1.8386861773637624,-4.713552199877226,-4.9715639261099005,0.1433771814291294,0.6435802578926086,1.4061501301251924,10.229608132289005,-0.012788644060492516,0.9872930645942688,1.953125e-08
+125,0.700232398051482,-1.7802322369355421,-4.713622533358061,-4.9716509855710544,0.1365026000600595,0.6448803910842309,1.4061496303631709,10.237350940704346,-0.012789146974682808,0.9872925281524658,1.953125e-08
+126,0.6997892879522764,-1.7981071472167969,-4.71307607797476,-4.972394246321458,0.1395831171136636,0.6443570026984582,1.4061491306011493,10.259672201596773,-0.012789648026227951,0.9872920513153076,1.953125e-08
+127,0.7022012082430032,-1.7532421029531038,-4.712694773307214,-4.971545384480403,0.13372439948412088,0.6429971754550934,1.4061486262541552,10.25721682035006,-0.012790156528353691,0.9872915744781494,1.953125e-08
+128,0.7020234740697421,-1.8195153841605554,-4.713398419893705,-4.971820501180796,0.14127671632629174,0.6438035644017733,1.4061481219071608,10.239136219024658,-0.012790655717253685,0.9872910976409912,1.953125e-08
+129,0.7001127508970407,-1.81697279214859,-4.714124257747944,-4.971649426680345,0.14217649256953827,0.6428970900865701,1.4061476221451392,10.232085961561937,-0.01279115304350853,0.987290620803833,1.953125e-08
+130,0.7021975769446447,-1.738131977044619,-4.713786565340483,-4.9720728397369385,0.13809818717149588,0.6419734954833984,1.4061471269680903,10.35787083552434,-0.012791653163731098,0.98729008436203,1.953125e-08
+131,0.7023522532903231,-1.7932511063722463,-4.714036299632146,-4.971442369314341,0.14081104730184263,0.6435797718855051,1.4061466272060688,10.278754417712872,-0.0127921476

# 2025-03-26 22:21:19.288146
+n

# 2025-03-26 22:21:25.658975
+/clear

# 2025-03-26 22:21:31.918213
+is the training working now? if you see some trends that could still be improved, suggest some changes and justify it. All the changes should be to simplify the code, not increase complexity. epoch,critic_loss,actor_loss,q1_value,q2_value,action_mean,action_std,entropy,grad_norm,log_std_mean,alpha,lr
+1,48.344684013953575,1.8699569197801442,-0.17810788521399865,0.030916351180237073,-0.0009460210477790007,0.6291848764969752,1.4188131460776696,39.84813836904672,-0.00026150658959522843,0.9997385144233704,1e-05
+2,42.74799112173227,2.1646786561379066,-0.4820975134005913,-0.28690536549458134,0.0011654225572084005,0.6261767286520737,1.4185487765532274,37.447567866398735,-0.0005296582821756601,0.9994704723358154,1e-05
+3,37.105497066791244,2.469459451161898,-0.7998192447882432,-0.6144183278083801,-0.001528555634789742,0.6290883398973025,1.4182763053820684,35.23962328984187,-0.0008069827454164624,0.9991933107376099,1e-05
+4,31.61755004295936,2.7845399104631863,-1.1390714874634376,-0.959066058580692,0.002699147891969635,0.6258358771984394,1.4179945313013518,33.10237004206731,-0.0010937106562778354,0.9989068508148193,1e-05
+5,26.290072221022385,3.1155088314643273,-1.5048654171136708,-1.327046839090494,0.0018475081485051375,0.6275930977784671,1.4177027253004222,30.809336955730732,-0.0013913859147578478,0.9986096024513245,1e-05
+6,21.219593341533955,3.4597337521039524,-1.898091027369866,-1.72139134315344,0.0015373010806237848,0.6289841761955848,1.41739860864786,28.130129520709698,-0.0017031843308359385,0.9982982873916626,1e-05
+7,16.488187863276554,3.814746563251202,-2.318785383151128,-2.1478584683858433,0.004338518788035099,0.6267613974901346,1.4170775505212636,24.934578455411472,-0.0020353635773062706,0.9979666471481323,1e-05
+8,12.24762197641226,4.169880692775433,-2.762000551590553,-2.609083890914917,0.011118383549798567,0.6270796473209674,1.4167324992326589,21.11287366426908,-0.0023964031133800745,0.9976064562797546,1e-05
+9,8.667160474337065,4.516612548094529,-3.2168121337890625,-3.1048747301101685,0.01159355123169147,0.628200161915559,1.4163513550391564,16.64622317827665,-0.0028030485846102238,0.9972008466720581,1e-05
+10,5.9098759064307576,4.827157570765569,-3.6611218452453613,-3.6314597680018497,0.009581334369543653,0.6280465676234319,1.4159116194798396,11.65472041643583,-0.003285819198936224,0.9967195987701416,1e-05
+11,4.029569754233727,5.048460025053758,-4.050841597410349,-4.163992386597854,0.005133922695397184,0.6260099204686972,1.4153749530132,6.790425007159893,-0.0038885080721229315,0.9961190223693848,1e-05
+12,2.889687941624568,5.0439500625316915,-4.304698815712562,-4.589167338151198,0.01999598229303956,0.6251189135588132,1.4147150149712195,3.7241223592024584,-0.004591146484017372,0.9954193830490112,1e-05
+13,2.171265001480396,4.864010847531832,-4.405112211520855,-4.757586607566247,0.019911803045453362,0.6310361463289994,1.4140217808576732,3.03710767856011,-0.005254133604466915,0.9947596788406372,1e-05
+14,1.6649044706271245,4.628310460310716,-4.463024249443641,-4.799018584764921,0.02648855708181285,0.6292459185306842,1.4134019246468177,2.936196859066303,-0.005829635541886091,0.9941872954368591,1e-05
+15,1.3175764175561757,4.378618533794697,-4.504867131893452,-4.831162562737098,0.030521162561714076,0.6291343661455008,1.4128565192222595,2.923917981294485,-0.006342696025967598,0.9936773777008057,1e-05
+16,1.0772182391240046,4.1271930657900295,-4.538192547284639,-4.855170891835139,0.03736648062029137,0.6310162429626172,1.4123687606591444,2.957545491365286,-0.006808740086853504,0.9932143688201904,1e-05
+17,0.8950831890106201,3.8952850653575015,-4.568363501475408,-4.8779030946584845,0.03301330772228539,0.6330874103766221,1.4119204787107615,3.0668328541975756,-0.00724057387560606,0.9927855730056763,1e-05
+18,0.7747641045313615,3.629041836811946,-4.586684465408325,-4.892559913488535,0.041031224420294166,0.6309641897678375,1.4115023704675527,3.245303071462191,-0.007641565520316362,0.9923875331878662,1e-05
+19,0.6897516732032483,3.3515585844333353,-4.604715512349055,-4.9038050541510945,0.04953186249790283,0.629513545678212,1.4111166183765118,3.43721813421983,-0.00801195576786995,0.9920200705528259,1e-05
+20,0.6291370265758954,3.1048542811320377,-4.621269867970393,-4.914389206812932,0.05347364875846184,0.6280597219100366,1.4107569043452923,3.681397346349863,-0.00836184062063694,0.9916729927062988,1e-05
+21,0.5950303559119885,2.830276397558359,-4.630057958456186,-4.923014035591712,0.061667502571183905,0.6337490609058967,1.410417079925537,3.9220578487102804,-0.008689465001225471,0.991348147392273,1e-05
+22,0.5752608489531738,2.5625466933617225,-4.64080733519334,-4.928740354684683,0.06456381698640493,0.6311626801123986,1.41009809420659,4.225572466850281,-0.00899837538599968,0.9910420179367065,1e-05
+23,0.5557121634483337,2.3339621470524716,-4.653871627954336,-4.937420533253596,0.06576334276738074,0.6377143515990331,1.4097975217379057,4.487349436833308,-0.009293651208281517,0.9907494187355042,1e-05
+24,0.5524258590661563,2.0486426720252404,-4.655555743437547,-4.941053152084351,0.07351674984854001,0.6327710105822637,1.4095058624561017,4.826896924238938,-0.009579513221979141,0.9904662370681763,1e-05
+25,0.5513139882913003,1.8246899338868947,-4.663060720150288,-4.944678068161011,0.07161622637739548,0.6361539455560538,1.4092255876614497,5.142190548089834,-0.009855162352323532,0.990193247795105,1e-05
+26,0.5518127496425922,1.573747387299171,-4.671399153195894,-4.951381481610811,0.07982024020300461,0.6357155465162717,1.4089536300072303,5.482489549196684,-0.010122687555849552,0.9899284243583679,1e-05
+27,0.5571590994413083,1.289644548526177,-4.676936553074763,-4.951573115128737,0.08505714097275184,0.6373334527015686,1.4086912824557378,5.853121849206778,-0.01038086786866188,0.9896728992462158,1e-05
+28,0.5715199869412643,1.0340864727130303,-4.677204352158767,-4.952001113158006,0.08720817431234397,0.6398565494097196,1.4084359590823834,6.210744766088633,-0.010634372010827065,0.989422082901001,1e-05
+29,0.5775843056348654,0.7290280713484838,-4.683349957832923,-4.958913362943209,0.0958644413890747,0.6359837972200834,1.4081833545978253,6.597934612861047,-0.010884132236242294,0.9891749620437622,1e-05
+30,0.5866968093010095,0.46038901118131786,-4.6914472579956055,-4.961775192847619,0.09769783340967618,0.6396510646893427,1.4079371461501489,7.0074181373302755,-0.011127148754894733,0.9889346361160278,1e-05
+31,0.5990718603134155,0.21205024535839373,-4.69128603201646,-4.962540223048284,0.10062903122833142,0.6388472822996286,1.4076955364300654,7.442376430218037,-0.011367845349013805,0.9886966943740845,1e-05
+32,0.6128829534237201,-0.08512266324116634,-4.690737687624418,-4.963507413864136,0.11299526433532055,0.6381654945703653,1.4075139806820796,7.685819754233727,-0.011485489085316658,0.9885803461074829,5e-06
+33,0.6157614256326969,-0.2119203255726741,-4.695799185679509,-4.964871058097253,0.11483683666357627,0.6365706370427058,1.4073969767643855,8.004449954399696,-0.011602304875850677,0.9884649515151978,5e-06
+34,0.627229283635433,-0.32526657673028797,-4.695009029828585,-4.965286108163687,0.11324915719720033,0.6403103929299575,1.4072802662849426,8.241463404435377,-0.011719164438545704,0.988349437713623,5e-06
+35,0.6344186457303854,-0.49247077795175404,-4.697483502901518,-4.965652099022498,0.11873088232599772,0.6403781129763677,1.4071630514585054,8.378714946600107,-0.011835871264338493,0.9882341027259827,5e-06
+36,0.6371987324494582,-0.6735117389605596,-4.703614565042349,-4.9667448447300835,0.12879732251167297,0.6405166318783393,1.4070473634279692,8.60957827934852,-0.011950071901082993,0.9881212711334229,5e-06
+37,0.648114011837886,-0.7660538920989404,-4.701089730629554,-4.967068598820613,0.12191488880377549,0.6440253968422229,1.4069330600591807,8.855720923497127,-0.012065980583429337,0.9880067110061646,5e-06
+38,0.6587906250586877,-0.8829761560146625,-4.7007990433619575,-4.968175392884475,0.12370234469954784,0.6422467598548303,1.4068445471616893,9.025869772984432,-0.012124095112085342,0.9879493713378906,2.5e-06
+39,0.6612329563269248,-1.0011040980999286,-4.70237895158621,-4.967853931280283,0.12900512493573701,0.6425491754825299,1.4067862584040716,9.045703961299015,-0.01218264177441597,0.9878914952278137,2.5e-06
+40,0.6619314001156733,-1.0220045080551734,-4.704121773059551,-4.968282974683321,0.12544840488296288,0.6452476726128504,1.406727983401372,9.143460273742676,-0.012240858748555183,0.9878339767456055,2.5e-06
+41,0.6655064248121701,-1.0948008390573354,-4.704842934241662,-4.9692637370182915,0.12714576320006296,0.6389343830255362,1.4066696350391095,9.358961178706242,-0.012299198657274246,0.9877763986587524,2.5e-06
+42,0.6698991885552039,-1.195504789169018,-4.705471662374643,-4.969758492249709,0.1293922129731912,0.6417050109459803,1.40661117205253,9.4004336137038,-0.012357627972960472,0.9877187013626099,2.5e-06
+43,0.6756797994558628,-1.2711659578176646,-4.705601343741784,-4.969992802693294,0.13107957117832625,0.6453113487133613,1.4065526815561147,9.407719025245079,-0.012416308745741844,0.9876607060432434,2.5e-06
+44,0.6776889127034408,-1.3873735116078303,-4.707410427240225,-4.9697831594027,0.13793536820090735,0.6424969297188979,1.4065081889812763,9.637886487520658,-0.012445289641618729,0.9876320958137512,1.25e-06
+45,0.6787693729767432,-1.3932943894312932,-4.7074151405921345,-4.970233880556547,0.13491240831521842,0.6401103093073919,1.4064793632580683,9.719529371995192,-0.01247425377368927,0.987603485584259,1.25e-06
+46,0.683182989175503,-1.387885148708637,-4.707432783566988,-4.970314282637376,0.13146505447534415,0.6433572769165039,1.4064500286028936,9.812847907726582,-0.012503748759627342,0.9875743389129639,1.25e-06
+47,0.6845697519870905,-1.4872853893500109,-4.708836665520301,-4.9703901914449835,0.1377519741654396,0.6428446769714355,1.406420959876134,9.779376690204327,-0.012532589957118034,0.9875458478927612,1.25e-06
+48,0.6855237827851222,-1.5157882708769579,-4.709759968977708,-4.970507199947651,0.1368959445792895,0.642321547636619,1.4063917765250573,9.86510493205144,-0.01256206538528204,0.9875167608261108,1.25e-06
+49,0.6856481914336865,-1.4685819515815148,-4.709227048433744,-4.969973839246309,0.1281849486896625,0.643506455879945,1.4063622447160573,9.924289630009579,-0.012591816484928131,0.9874874353408813,1.25e-06
+50,0.6930036934522482,-1.5667992646877582,-4.7078920511099005,-4.97058917925908,0.13486810515706354,0.6450020349942721,1.4063395170065074,9.875884276169996,-0.01260673813521862,0.987472653388977,6.25e-07
+51,0.6908904142104663,-1.5806709023622365,-4.709743609795203,-4.970752972822923,0.13596665372069067,0.6424859005671281,1.4063246387701769,10.035176093761738,-0.01262154895812273,0.9874580502510071,6.25e-07
+52,0.6923070870912992,-1.6175930224932158,-4.710187875307524,-4.970600476631751,0.14064965196526968,0.6406887059028332,1.4063099255928626,10.132172181056095,-0.012636225670576096,0.9874435663223267,6.25e-07
+53,0.6956806652821027,-1.621866991886726,-4.70940454189594,-4.970676440459031,0.13696883160334367,0.6406707167625427,1.4062952215854938,10.020891483013447,-0.012650985270738602,0.9874289631843567,6.25e-07
+54,0.6934175307934101,-1.534504349415119,-4.710014746739314,-4.970828294754028,0.12510046764062002,0.6427266483123486,1.4062802608196552,10.149632600637583,-0.012666184455156326,0.9874140024185181,6.25e-07
+55,0.695332904274647,-1.6641068091759315,-4.710228754923894,-4.9717895434452934,0.13901207137566346,0.6435876144812658,1.406265112069937,10.104337068704458,-0.012681221589446068,0.9873991012573242,6.25e-07
+56,0.6976788273224463,-1.704794874558082,-4.710450740960928,-4.9713664054870605,0.13979780559356397,0.6421045569273142,1.4062537550926208,10.09784592114962,-0.012688616290688515,0.9873918294906616,3.125e-07
+57,0.695394937808697,-1.6598692811452425,-4.711275100708008,-4.971142457081721,0.13706650986121252,0.6436491310596466,1.4062463136819692,10.14700306378878,-0.012696107849478722,0.9873844385147095,3.125e-07
+58,0.6969600526186136,-1.6938561109396129,-4.710821518531213,-4.971375483732957,0.14032975383676016,0.6414050689110389,1.4062387943267822,10.18459466787485,-0.012703601270914078,0.9873770475387573,3.125e-07
+59,0.6970435014137855,-1.6896111094034636,-4.7125795254340535,-4.971474189024705,0.13718473997253638,0.6444623080583719,1.4062313024814312,10.16913362649771,-0.01271112821996212,0.9873696565628052,3.125e-07
+60,0.6978389827104715,-1.7288418687306917,-4.712342427327083,-4.971216990397527,0.1395869332437332,0.6414153873920441,1.4062237785412715,10.241812705993652,-0.012718653306365013,0.9873621463775635,3.125e-07
+61,0.6982782150690372,-1.713108452466818,-4.711428568913386,-4.971904112742497,0.13654995910250223,0.6413909655350906,1.4062161904114943,10.311742342435396,-0.012726260349154472,0.9873546361923218,3.125e-07
+62,0.7001137045713571,-1.741628137918619,-4.711308387609629,-4.9712156515855055,0.13916092623884863,0.6446555509017065,1.4062104316858144,10.129141587477465,-0.012730039656162262,0.9873509407043457,1.5625e-07
+63,0.6989898383617401,-1.753194822714879,-4.71162042251,-4.971909119532659,0.1384739947433655,0.6458074886065263,1.406206644498385,10.071607112884521,-0.01273384876549244,0.9873471260070801,1.5625e-07
+64,0.6996910915924952,-1.684629385287945,-4.712037453284631,-4.97142217709468,0.1332069099522554,0.6449600824942956,1.4062028114612286,10.206939477186937,-0.012737693265080452,0.9873433709144592,1.5625e-07
+65,0.6983300630862896,-1.7484252865497882,-4.712471595177283,-4.97123907162593,0.13860649730150515,0.6402134390977713,1.4061989784240723,10.402705742762638,-0.012741507962346077,0.9873396158218384,1.5625e-07
+66,0.7012107601532569,-1.7268659701714149,-4.712489494910607,-4.971675432645357,0.13740914314985275,0.6435850377266223,1.4061951820666974,10.239790329566368,-0.01274531427770853,0.9873358607292175,1.5625e-07
+67,0.7015385696521172,-1.71540105342865,-4.711571895159208,-4.971559322797335,0.13380694790528372,0.6475734298045819,1.4061913581994863,10.268535760732798,-0.01274915225803852,0.9873321056365967,1.5625e-07
+68,0.7000190168619156,-1.7763255146833568,-4.71206703552833,-4.97156625527602,0.14268538986261076,0.6457628905773163,1.4061884467418377,10.183358449202318,-0.012751035392284393,0.9873301982879639,7.8125e-08
+69,0.699504201228802,-1.739743553675138,-4.713063918627226,-4.971929971988384,0.1382594294846058,0.6415524024229783,1.406186566903041,10.297208932729868,-0.012752950191497803,0.987328290939331,7.8125e-08
+70,0.703253285242961,-1.7711177101502051,-4.71248014156635,-4.971560386510996,0.13532225501078826,0.6443013755174783,1.4061846274595995,10.212985845712515,-0.012754879891872406,0.9873263835906982,7.8125e-08
+71,0.6999450005017794,-1.7335725289124708,-4.71328643652109,-4.971561945401705,0.13562355018579042,0.6418883273234735,1.4061827155259938,10.253135937910814,-0.012756818905472755,0.9873244762420654,7.8125e-08
+72,0.6995046597260696,-1.7697031314556415,-4.713019334352934,-4.971132058363694,0.1379573981349285,0.6424807539353004,1.406180780667525,10.286979308495155,-0.012758753262460232,0.9873225688934326,7.8125e-08
+73,0.7013234106393961,-1.804710566997528,-4.712448101777297,-4.971505293479333,0.1411106910269994,0.6423554030748514,1.4061788412240834,10.212209701538086,-0.01276068203151226,0.9873206615447998,7.8125e-08
+74,0.7002658545970917,-1.7625916737776537,-4.713129171958337,-4.971716972497793,0.14017911685200837,0.6441108813652625,1.4061773694478548,10.244056224822998,-0.012761642225086689,0.9873197078704834,3.90625e-08
+75,0.7012534118615664,-1.763185666157649,-4.712661926562969,-4.971637634130625,0.13480086939839217,0.6431701091619638,1.4061764157735384,10.381322347200834,-0.01276260707527399,0.987318754196167,3.90625e-08
+76,0.7017162488057063,-1.7949166664710412,-4.711675222103413,-4.972068383143498,0.13994856436665243,0.6444529294967651,1.4061754529292767,10.356022578019362,-0.012763562612235546,0.9873178005218506,3.90625e-08
+77,0.6996564200291266,-1.7331810822853675,-4.713643000676082,-4.971778667890108,0.1347871574644859,0.6431864179097689,1.4061744855000422,10.346962525294376,-0.012764533050358295,0.9873168468475342,3.90625e-08
+78,0.7001752303196833,-1.7462623898799603,-4.713778459108793,-4.971478884036724,0.13664408257374397,0.6420943484856532,1.4061735226557806,10.340524600102352,-0.01276550441980362,0.9873158931732178,3.90625e-08
+79,0.7004002951658689,-1.7770576843848596,-4.71308420254634,-4.972114581328172,0.13854040721288094,0.6434919513188876,1.406172541471628,10.205430581019474,-0.012766474857926369,0.9873149394989014,3.90625e-08
+80,0.700161704650292,-1.72645765542984,-4.7131709685692424,-4.972065210342407,0.13719732858813727,0.6448848453851846,1.4061718124609728,10.34166380075308,-0.012766960076987743,0.9873144626617432,1.953125e-08
+81,0.7010206488462595,-1.7068027670566852,-4.712895668469942,-4.972093802232009,0.13308028446940276,0.6426706680884728,1.4061713172839239,10.304774027604322,-0.01276745181530714,0.987313985824585,1.953125e-08
+82,0.7011966200975271,-1.8123575357290416,-4.712834468254676,-4.972159954217764,0.14508171007037163,0.6420200971456674,1.406170835861793,10.23363421513484,-0.012767938897013664,0.9873135089874268,1.953125e-08
+83,0.7007293758484033,-1.795530942770151,-4.713703852433425,-4.9720596716954155,0.14203304510850173,0.6442550535385425,1.4061703498546894,10.221892283512997,-0.012768423184752464,0.9873130321502686,1.953125e-08
+84,0.7021827422655545,-1.793592645571782,-4.712788068331205,-4.971627913988554,0.1417514678950493,0.6440355319243211,1.406169863847586,10.158100384932299,-0.012768907472491264,0.9873125553131104,1.953125e-08
+85,0.7025913023031675,-1.7744757395524244,-4.713026450230525,-4.971901746896597,0.13877187554652876,0.6448675027260413,1.4061693778404822,10.258671870598427,-0.012769391760230064,0.9873120784759521,1.953125e-08
+86,0.7000596271111414,-1.757405019723452,-4.713505066358126,-4.971824407577515,0.13416763309102792,0.6460099220275879,1.4061688918333788,10.225694839770977,-0.01276988536119461,0.987311601638794,1.953125e-08
+87,0.7022679425202883,-1.7369011778097887,-4.7131562599769,-4.97202310195336,0.13284015082395995,0.6446897295805124,1.4061684012413025,10.283921718597412,-0.012770377099514008,0.9873111248016357,1.953125e-08
+88,0.7023863540245936,-1.7465528799937322,-4.7130482747004585,-4.971803903579712,0.1348364376104795,0.6444777571238004,1.4061679060642536,10.316034720494198,-0.012770871631801128,0.9873106479644775,1.953125e-08
+89,0.702348544047429,-1.7632271601603582,-4.7129645714393025,-4.971744482333843,0.13364267291931006,0.6421087200825031,1.4061674017172594,10.361144395974966,-0.012771360576152802,0.9873101711273193,1.953125e-08
+90,0.699845573076835,-1.733741324681502,-4.71246077464177,-4.972032106839693,0.13414701991356337,0.6424210644685305,1.4061669248801012,10.377868212186373,-0.01277184672653675,0.9873096942901611,1.953125e-08
+91,0.7029451063046088,-1.8143507196353033,-4.712428239675669,-4.971956858268151,0.14250674270666563,0.6440073939470145,1.4061664388729975,10.174899541414701,-0.012772337533533573,0.9873092174530029,1.953125e-08
+92,0.7009659329285989,-1.7889294119981618,-4.712766849077665,-4.971683630576501,0.138615337988505,0.6427389291616586,1.4061659482809215,10.274140101212721,-0.012772824615240097,0.9873087406158447,1.953125e-08
+93,0.703208143894489,-1.777096858391395,-4.712599240816557,-4.9722191370450535,0.1381658436014102,0.6424930347846105,1.4061654485188997,10.30868647648738,-0.012773323804140091,0.9873082041740417,1.953125e-08
+94,0.7043121640498822,-1.7741612104269175,-4.712299291904156,-4.971850010064932,0.13860498110835368,0.6438096876327808,1.4061649579268236,10.257520602299618,-0.012773810885846615,0.9873076677322388,1.953125e-08
+95,0.7002837016032293,-1.8091890445122352,-4.713192591300378,-4.972089070540208,0.1410900534918675,0.6445028139994695,1.4061644765046926,10.223846398867094,-0.012774300761520863,0.9873071908950806,1.953125e-08
+96,0.7028887031170038,-1.7739147819005525,-4.712405186433059,-4.97194609275231,0.13830088250912154,0.6415780599300678,1.4061639721576984,10.34738614008977,-0.012774791568517685,0.9873067736625671,1.953125e-08
+97,0.7023204289949857,-1.8015102744102478,-4.713496721707857,-4.9719579219818115,0.1388537362217903,0.6421132935927465,1.4061634953205402,10.322833464695858,-0.012775273993611336,0.9873062372207642,1.953125e-08
+98,0.6993490927494489,-1.7614992123383741,-4.7135323377755975,-4.971341444895818,0.13389196562079284,0.6435363728266495,1.406163004728464,10.325426321763258,-0.012775769457221031,0.987305760383606,1.953125e-08
+99,0.7019780255281008,-1.7617831505261934,-4.7125650736001825,-4.972375924770649,0.13782052065317446,0.643962887617258,1.4061625141363878,10.296109713040865,-0.012776259332895279,0.9873052835464478,1.953125e-08
+100,0.7051317370854892,-1.8017273316016564,-4.711829497263982,-4.9715294654552755,0.14130963614353767,0.6411759211466863,1.4061620235443115,10.378402820000282,-0.012776743620634079,0.9873048067092896,1.953125e-08
+101,0.7013196578392615,-1.810042495910938,-4.713027935761672,-4.971539607414832,0.14050280245450827,0.6419863036045661,1.406161537537208,10.298425087561974,-0.012777234427630901,0.9873043298721313,1.953125e-08
+102,0.7008149188298446,-1.8159752029639025,-4.712922151272114,-4.972128593004667,0.14229049247044784,0.6449987108890827,1.406161056115077,10.316619506249062,-0.012777718715369701,0.9873038530349731,1.953125e-08
+103,0.7020891824593911,-1.7613612780204186,-4.713200569152832,-4.972025156021118,0.1389768272638321,0.6428391131070944,1.406160560938028,10.30735364327064,-0.012778208591043949,0.9873033761978149,1.953125e-08
+104,0.7028424109403903,-1.7003300923567553,-4.712813010582557,-4.971556260035588,0.1294264867901802,0.6466251703409048,1.4061600657609792,10.30552126811101,-0.012778708711266518,0.9873028993606567,1.953125e-08
+105,0.700709751019111,-1.7840571907850413,-4.713591190484854,-4.9720402130713826,0.13814682026322073,0.6437976291546454,1.4061595705839305,10.279361798213078,-0.012779204174876213,0.9873024225234985,1.953125e-08
+106,0.701608637204537,-1.7937558751839857,-4.713062799893892,-4.971701530309824,0.1393363607617525,0.6452140051584977,1.4061590799918542,10.260535460252028,-0.012779701501131058,0.9873019456863403,1.953125e-08
+107,0.7015649011501899,-1.7746716783596919,-4.712178450364333,-4.971549382576575,0.13857745350553438,0.6434961282289945,1.4061585848148053,10.353039668156551,-0.012780187651515007,0.9873014092445374,1.953125e-08
+108,0.7024393242139083,-1.8163175857984102,-4.713773635717539,-4.97181433897752,0.14366217731283262,0.645787924528122,1.4061580850527837,10.10454012797429,-0.012780681252479553,0.9873009324073792,1.953125e-08
+109,0.7011041457836444,-1.709573534818796,-4.714346537223229,-4.971653736554659,0.13267317156379038,0.6445214610833389,1.40615759904568,10.265228821681095,-0.012781183235347271,0.9873003959655762,1.953125e-08
+110,0.7030754937575414,-1.7794869679671068,-4.712615251541138,-4.971982057277973,0.1372160129249096,0.6423678054259374,1.4061570946986859,10.283217980311466,-0.012781684286892414,0.987299919128418,1.953125e-08
+111,0.7023465725091788,-1.711598359621488,-4.713356384864221,-4.971952493374165,0.13041399763180658,0.6411590484472421,1.4061565903516917,10.31390879704402,-0.012782183475792408,0.9872994422912598,1.953125e-08
+112,0.7002862210457141,-1.811475552045382,-4.71405997643104,-4.971911980555608,0.1423731612471434,0.6428131186045133,1.4061560951746428,10.220325506650484,-0.012782678939402103,0.9872989654541016,1.953125e-08
+113,0.7009249455653704,-1.7548752243702228,-4.714185219544631,-4.9719415077796345,0.13582096306177285,0.641888650564047,1.4061555999975939,10.372300954965445,-0.012783173471689224,0.9872984886169434,1.953125e-08
+114,0.7017534868075297,-1.757205293728755,-4.713938621374277,-4.971689242583055,0.1385997854746305,0.6432017179635855,1.406155104820545,10.238951132847713,-0.012783670797944069,0.9872980117797852,1.953125e-08
+115,0.7007421484360328,-1.774416630084698,-4.713473154948308,-4.971550372930674,0.13584513446459404,0.646107017993927,1.4061546004735506,10.25988483428955,-0.012784172780811787,0.987297534942627,1.953125e-08
+116,0.6988673427930245,-1.786327444590055,-4.71425384741563,-4.971924323302049,0.14081212906883314,0.6425795623889337,1.4061541144664471,10.232891082763672,-0.012784669175744057,0.987296998500824,1.953125e-08
+117,0.7023771336445441,-1.7871464307491596,-4.71420170710637,-4.971540982906635,0.13415757967875555,0.6486338995970212,1.4061536147044256,10.116316611950214,-0.0127851702272892,0.987296462059021,1.953125e-08
+118,0.7007234921822181,-1.7755398200108454,-4.713903940640963,-4.971751268093403,0.1355821547599939,0.6448457172283759,1.406153114942404,10.240365248460035,-0.012785663828253746,0.9872959852218628,1.953125e-08
+119,0.7015377787443308,-1.7988196703103871,-4.712496060591477,-4.971970154688909,0.1405474113730284,0.6435273564778842,1.4061526151803823,10.235229969024658,-0.01278616115450859,0.9872955083847046,1.953125e-08
+120,0.7005501183179709,-1.783159498984997,-4.714133721131545,-4.971840583361113,0.1375406117966542,0.6444119581809411,1.4061521154183607,10.320063187525822,-0.012786662206053734,0.9872950315475464,1.953125e-08
+121,0.7005042548363025,-1.7858469302837665,-4.7134695786696215,-4.971768544270442,0.13727466905346283,0.6416051066838778,1.4061516110713665,10.298476512615498,-0.012787164188921452,0.9872945547103882,1.953125e-08
+122,0.7030845284461975,-1.8143943731601422,-4.713145347741934,-4.971523651709924,0.14421206827347094,0.6408214431542617,1.4061511158943176,10.339730776273287,-0.012787654995918274,0.98729407787323,1.953125e-08
+123,0.7010187002328726,-1.7922461537214427,-4.712896090287429,-4.971818208694458,0.14212099462747574,0.6393557351369125,1.4061506207172687,10.395121317643385,-0.012788151390850544,0.9872936010360718,1.953125e-08
+124,0.7032387783894172,-1.8386861773637624,-4.713552199877226,-4.9715639261099005,0.1433771814291294,0.6435802578926086,1.4061501301251924,10.229608132289005,-0.012788644060492516,0.9872930645942688,1.953125e-08
+125,0.700232398051482,-1.7802322369355421,-4.713622533358061,-4.9716509855710544,0.1365026000600595,0.6448803910842309,1.4061496303631709,10.237350940704346,-0.012789146974682808,0.9872925281524658,1.953125e-08
+126,0.6997892879522764,-1.7981071472167969,-4.71307607797476,-4.972394246321458,0.1395831171136636,0.6443570026984582,1.4061491306011493,10.259672201596773,-0.012789648026227951,0.9872920513153076,1.953125e-08
+127,0.7022012082430032,-1.7532421029531038,-4.712694773307214,-4.971545384480403,0.13372439948412088,0.6429971754550934,1.4061486262541552,10.25721682035006,-0.012790156528353691,0.9872915744781494,1.953125e-08
+128,0.7020234740697421,-1.8195153841605554,-4.713398419893705,-4.971820501180796,0.14127671632629174,0.6438035644017733,1.4061481219071608,10.239136219024658,-0.012790655717253685,0.9872910976409912,1.953125e-08
+129,0.7001127508970407,-1.81697279214859,-4.714124257747944,-4.971649426680345,0.14217649256953827,0.6428970900865701,1.4061476221451392,10.232085961561937,-0.01279115304350853,0.987290620803833,1.953125e-08
+130,0.7021975769446447,-1.738131977044619,-4.713786565340483,-4.9720728397369385,0.13809818717149588,0.6419734954833984,1.4061471269680903,10.35787083552434,-0.012791653163731098,0.98729008436203,1.953125e-08
+131,0.7023522532903231,-1.7932511063722463,-4.714036299632146,-4.971442369314341,0.14081104730184263,0.6435797718855051,1.4061466272060688,10.278754417712872,-0.0127921476

# 2025-03-26 22:23:41.448440
+y

# 2025-03-26 22:28:25.317212
+/undo

# 2025-03-26 22:36:16.740519
+I dont like the scheduler for the learning rate because i dont understand what it does. Can we remove it? or it would make things more complicated? The last training seemed to work and the learning rate that was logged converged to 1.953125e-08 with alpha 0.9871055483818054. Also, I get this error C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
+  warnings.warn(

# 2025-03-26 22:37:27.208726
+y

# 2025-03-26 22:39:16.121726
+/undo

# 2025-03-26 22:48:57.570159
+I am getting this warning C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
+  warnings.warn(=. Please fix it and explain me what the learning rate scheduler does. Should the learning rate converge to zero at the end of the training?

# 2025-03-26 22:50:57.371547
+n

# 2025-03-26 22:53:45.760678
+/clear

# 2025-03-26 23:01:24.331114
+Update code_documentation to explain how SACCQL_training works step by step and explain the choices of the paramaters. Update SACCQL_training.py to save the trained model. And save the log and the analysis of the log in the same folder with a name related to the just trained model. Create an SACCQL_testing.py file to load test data of the same for as the train data and the train model to test whether it works or not. 

# 2025-03-26 23:05:15.457920
+y

# 2025-03-26 23:05:22.082952
+n

# 2025-03-26 23:06:14.667419
+y

# 2025-03-26 23:19:38.783148
+/add SACCQL_testing.py

# 2025-03-26 23:20:32.070338
+The name of the model and the logs should include datetime of the training so that when a new model is trained it does not eraase the previous one. I aslo get this error during testing emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_testing.py
+Loading SAC model (epoch 500)
+Evaluating: 100%|| 104/104 [00:00<00:00, 301.10it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 358, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 347, in main
+    metrics = evaluate_model(model, test_dataset, args.output_dir)
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 182, in evaluate_model
+    json.dump(metrics, f, indent=4)
+    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 179, in dump
+    for chunk in iterable:
+                 ^^^^^^^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 432, in _iterencode
+    yield from _iterencode_dict(o, _current_indent_level)
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 406, in _iterencode_dict
+    yield from chunks
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 439, in _iterencode
+    o = _default(o)
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
+    raise TypeError(f'Object of type {o.__class__.__name__} '
+                    f'is not JSON serializable')
+TypeError: Object of type float32 is not JSON serializable

# 2025-03-26 23:23:43.578117
+y

# 2025-03-26 23:29:36.511210
+create testing_doc.md to explain how SACCQL_testing.py works, including how to interpret all the obtained graphs

# 2025-03-26 23:32:45.267947
+y

# 2025-03-26 23:49:37.997849
+/clear

# 2025-03-26 23:49:39.686889
+/drop

# 2025-03-26 23:49:58.098427
+/add data_curation.py

# 2025-03-26 23:50:15.167696
+I get this error when running evaluate_dataset_coverage File c:\Users\emili\procoding\Gloop\data_curation.py:75, in evaluate_dataset_coverage(dataset, n_samples)
+     72 """Evaluate state and action coverage of DiabetesDataset"""
+     73 # Convert to pandas DataFrame for easier analysis
+     74 df = pd.DataFrame({
+---> 75     'state': [s.numpy() for s in dataset.states],
+     76     'action': [a.numpy() for a in dataset.actions],
+     77     'reward': dataset.rewards.numpy(),
+     78     'done': dataset.dones.numpy()
+     79 })
+     81 # Random sample for visualization
+     82 sample_idx = np.random.choice(len(df), size=min(n_samples, len(df)), replace=False)
+
+AttributeError: 'numpy.ndarray' object has no attribute 'numpy'

# 2025-03-26 23:51:29.368500
+y

# 2025-03-27 01:43:12.221725
+/drop

# 2025-03-27 01:43:14.372450
+/clear

# 2025-03-27 01:45:31.507687
+/add SACCQL_training.py

# 2025-03-27 01:45:39.030870
+I have modified my dataset (i provide you an example of data below) and now I include an action column instead of two (bolus and basal). Therefore, the state dimension is only one. Please modify t
+. he neural network componenents and the rest of the code accordingly time,glu_raw,glu,glu_d,glu_t,hr,hr_d,hr_t,iob,hour,basal,bolus,tia,action,done2021-09-17 14:40:00,149.0,0.05933688830190088,0.88127138418
+. 52395,-0.12850660140264478,-0.022378046554612323,0.09932619602509997,-0.3263928760838276,0.3930364987877983,0.5833333333333334,0.7,0.0,4.196544217687075,-0.06738027334239183,0 
+. 2021-09-17 14:45:00,154.0,0.15996379097539434,1.101693794942535,0.3512830876725578,-0.30804317608601844,-0.383178614674967,-0.3100662624639462,0.3534607218105369,0.5833333333333334,0.7,0.0,3.90579980563654
+. 24,-0.08532997413810883,0

# 2025-03-27 01:47:43.286267
+Y

# 2025-03-27 01:53:17.516924
+Something is not working correctly in the code epoch,critic_loss,actor_loss,q1_value,q2_value,action_mean,action_std,entropy,grad_norm,log_std_mean,alpha,lr
+1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1e-05
+2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1e-05
+3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1e-05
+4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1e-05
+5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1e-05 Please fix it but do not play too much with the hyper paramaters

# 2025-03-27 01:55:23.883098
+y

# 2025-03-27 01:58:03.681105
+/clear

# 2025-03-27 02:01:10.158137
+there is still a bug in the code. the training logs are still all zeros and I get the message NaN detected in loss, skipping update. I guess by changing the action dimension from 2 to 1 there are tsill parts of the code that are not adapated? Please revise again the code to verify that there are no bugs. Add prints that could maybe help us understand better the problem

# 2025-03-27 02:03:04.747701
+y

# 2025-03-27 02:05:56.081246
+I still get the nan on the loss emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_training.py
+Training on device: cpu
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): ReLU()
+  (2): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): ReLU()
+  (2): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): ReLU()
+  (2): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): ReLU()
+  (2): Linear(in_features=128, out_features=1, bias=True)
+)
+Training:   0%|                                                                                                                                                                        | 0/500 [00:00<?, ?it/s]State shape: torch.Size([512, 8])
+Action shape: torch.Size([512, 1])
+State-action input shape: torch.Size([512, 9])
+Q1 values: min=-0.0346, max=0.0095, mean=-0.0068
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Entropy: 1.4189, Target entropy: -1.0000
+Alpha: 1.0000
+Alpha loss: -0.0000
+NaN gradient in q2.2.bias!
+NaN gradient in q2.2.weight!
+NaN gradient in q2.0.bias!
+NaN gradient in q2.0.weight!
+NaN gradient in q1.2.bias!
+NaN gradient in q1.2.weight!
+NaN gradient in q1.0.bias!
+NaN gradient in q1.0.weight!
+NaN detected in critic gradients, skipping critic update
+Gradient norms - Critic: nan, Actor: 0.5001, Alpha: 2.4189
+State shape: torch.Size([512, 8])
+Action shape: torch.Size([512, 1])
+State-action input shape: torch.Size([512, 9])
+Q1 values: min=-0.0188, max=0.0249, mean=0.0018
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Entropy: 1.4189, Target entropy: -1.0000
+Alpha: 1.0001
+Alpha loss: -0.0002
+NaN gradient in q2.2.bias!
+NaN gradient in q2.2.bias!
+NaN gradient in q2.2.weight!
+NaN gradient in q2.2.weight!

# 2025-03-27 02:07:55.756864
+y

# 2025-03-27 02:13:42.506717
+/clear

# 2025-03-27 02:14:53.757920
+Training on device: cpu
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Training:   0%|                                                                                                                                                                        | 0/500 [00:00<?, ?it/s]State shape: torch.Size([512, 8])
+Action shape: torch.Size([512, 1])
+State stats: mean=0.00 1.00
+Action stats: mean=-0.24 0.24
+State-action input shape: torch.Size([512, 9])
+Q1 values: min=-2.5732, max=1.5612, mean=-0.5255
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Target Q stats: min=nan, max=nan
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Entropy: 1.4189, Target entropy: -1.0000
+Alpha: 1.0000
+Alpha loss: -0.0000
+NaN gradient in q2.6.bias!
+NaN gradient in q2.6.weight!
+NaN gradient in q2.4.weight!
+NaN gradient in q2.4.bias!
+NaN gradient in q2.3.bias!
+NaN gradient in q2.3.weight!
+NaN gradient in q2.1.weight!
+NaN gradient in q2.1.bias!
+NaN gradient in q2.0.bias!
+NaN gradient in q2.0.weight!
+NaN gradient in q1.6.bias!
+NaN gradient in q1.6.weight!
+NaN gradient in q1.4.weight!
+NaN gradient in q1.4.bias!
+NaN gradient in q1.3.bias!
+NaN gradient in q1.3.weight!
+NaN gradient in q1.1.weight!
+NaN gradient in q1.1.bias!
+NaN gradient in q1.0.bias!
+NaN gradient in q1.0.weight!
+NaN gradients in ['q1.0.weight']
+Skipping critic update due to NaN gradients
+Gradient norms - Critic: nan, Actor: 0.5181, Alpha: 2.4189
+State shape: torch.Size([512, 8])
+Action shape: torch.Size([512, 1])
+State stats: mean=-0.00 1.00
+Action stats: mean=-0.23 0.24
+State-action input shape: torch.Size([512, 9])
+Q1 values: min=-2.4202, max=1.7456, mean=-0.4941
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Target Q stats: min=nan, max=nan
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Entropy: 1.4189, Target entropy: -1.0000
+Alpha: 1.0000
+Alpha loss: -0.0000
+NaN gradient in q2.6.bias!
+NaN gradient in q2.6.bias!
+NaN gradient in q2.6.weight!
+NaN gradient in q2.6.weight!
+NaN gradient in q2.4.weight! i get these problems, could it have something to do with the data? should we print the state action reward and next state at every iteration too? just for double checking that everything is correct.

# 2025-03-27 02:16:52.926973
+y

# 2025-03-27 02:19:03.932355
+Actually the actions must not be between 0 and 5 but rather between -1 and 1

# 2025-03-27 02:22:06.208261
+y

# 2025-03-27 02:27:00.683740
+You should now that actually all values are normalised beforehand. I still get this error emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_training.py
+Training on device: cpu
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 622, in <module>
+    agent = train_sac(
+        dataset_path=args.dataset,
+    ...<3 lines>...
+        log_dir=args.log_dir
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 261, in train_sac
+    dataset = DiabetesDataset(dataset_path)
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 46, in __init__
+    self.rewards = self._compute_rewards(df["glu_raw"].values)
+                   ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 70, in _compute_rewards
+    raise ValueError(f"NaN rewards at indices: {nan_indices}")
+ValueError: NaN rewards at indices: [  169   170   171 ... 13197 13198 13199]

# 2025-03-27 02:29:02.341102
+y

# 2025-03-27 02:30:20.663144
+Sample glu_raw inputs: [219. 219. 219. 219. 219.]
+Sample rewards: [-0.46292475 -0.46292475 -0.46292475 -0.46292475 -0.46292475]
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Creating Q-network with state_dim:8 action_dim:1
+Q-network structure: Sequential(
+  (0): Linear(in_features=9, out_features=128, bias=True)
+  (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (2): LeakyReLU(negative_slope=0.01)
+  (3): Linear(in_features=128, out_features=128, bias=True)
+  (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
+  (5): LeakyReLU(negative_slope=0.01)
+  (6): Linear(in_features=128, out_features=1, bias=True)
+)
+Training:   0%|                                                                                                                                                                        | 0/500 [00:00<?, ?it/s]State shape: torch.Size([512, 8])
+Action shape: torch.Size([512, 1])
+State stats: mean=-0.00 1.00
+Action stats: mean=-0.25 0.22
+State range: -6.0 to 6.5
+Action range: -0.6 to 0.2
+State-action input shape: torch.Size([512, 9])
+Q1 values: min=-3.2867, max=1.9967, mean=-0.3982
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Target Q stats: min=-7.43, max=0.55
+Q1 outputs: [[-1.5904075]
+ [-0.5908849]
+ [ 0.4105487]
+ [ 1.9967474]
+ [-2.1355383]]
+Target Q: [[-1.6322966]
+ [-0.7100004]
+ [-0.4733916]
+ [-1.0347567]
+ [-1.5107052]]
+Rewards: [[-0.5       ]
+ [-0.5       ]
+ [-0.44773737]
+ [-0.5       ]
+ [-0.5       ]]
+Dones: [[0.]
+ [0.]
+ [0.]
+ [0.]
+ [0.]]
+Mean shape: torch.Size([512, 1]), Log_std shape: torch.Size([1, 1])
+Entropy: 1.4189, Target entropy: -1.0000
+Alpha: 1.0000
+Alpha loss: -0.0000
+Training:   0%|                                                                                                                                                                        | 0/500 [00:00<?, ?it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 663, in <module>
+    agent = train_sac(
+        dataset_path=args.dataset,
+    ...<3 lines>...
+        log_dir=args.log_dir
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 485, in train_sac
+    actor_loss.backward()
+    ~~~~~~~~~~~~~~~~~~~^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\_tensor.py", line 626, in backward
+    torch.autograd.backward(
+    ~~~~~~~~~~~~~~~~~~~~~~~^
+        self, gradient, retain_graph, create_graph, inputs=inputs
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\autograd\__init__.py", line 347, in backward
+    _engine_run_backward(
+    ~~~~~~~~~~~~~~~~~~~~^
+        tensors,
+        ^^^^^^^^
+    ...<5 lines>...
+        accumulate_grad=True,
+        ^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\torch\autograd\graph.py", line 823, in _engine_run_backward
+    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
+           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+        t_outputs, *args, **kwargs
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )  # Calls into the C++ engine to run the backward pass
+    ^
+RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

# 2025-03-27 02:37:00.716675
+y

# 2025-03-27 02:37:58.936369
+/undo

# 2025-03-27 02:39:00.160236
+Please simplify the code and make it shorter. Do not touch DiabetesDataset but you can modify the other parts such as the training loop. Remove the debugging prints that we added before

# 2025-03-27 02:41:35.013579
+y

# 2025-03-27 02:43:42.164160
+/clear

# 2025-03-27 02:47:34.895509
+add this function so that I can track the training logs def analyze_training_log(log_path="training_logs/training_log.csv", output_dir="training_analysis"):
+    """Analyze training log and generate visualizations"""
+    Path(output_dir).mkdir(parents=True, exist_ok=True)
+    
+    # Read log data
+    df = pd.read_csv(log_path)
+    
+    # Create dynamic subplot grid
+    metrics = [col for col in df.columns if col != 'epoch']
+    n_metrics = len(metrics)
+    n_cols = 3
+    n_rows = (n_metrics + n_cols - 1) // n_cols  # Ceiling division
+    
+    fig, axs = plt.subplots(n_rows, n_cols, figsize=(18, 4*n_rows))
+    fig.suptitle('Training Metrics Analysis', fontsize=16)
+    
+    # Flatten axes array for easier iteration
+    axs = axs.flatten()
+    
+    for idx, metric in enumerate(metrics):
+        ax = axs[idx]
+        ax.plot(df['epoch'], df[metric])
+        ax.set_title(metric.replace('_', ' ').title())
+        ax.set_xlabel('Epoch')
+        ax.grid(True)
+    
+    # Hide empty subplots
+    for idx in range(n_metrics, len(axs)):
+        axs[idx].axis('off')
+    
+    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
+    plt.savefig(Path(output_dir) / "training_metrics.png")
+    plt.close()
+    
+    print(f"Analysis plots saved to {output_dir}") also, these are the training logs that I have obtained in the last training. having a look at the qvalues and losses, what change in hyper parameters would you suggest? Training:  10%|                                                                                                              | 49/500 [00:23<03:37,  2.07it/s, Critic Loss=0.576, Actor Loss=9.803]Checkpoint saved to models\sac_checkpoint_epoch50.pth
+Training:  20%|                                                                                                 | 99/500 [00:49<03:24,  1.96it/s, Critic Loss=0.550, Actor Loss=9.804]Checkpoint saved to models\sac_checkpoint_epoch100.pth
+Training:  30%|                                                                                     | 149/500 [01:16<03:10,  1.85it/s, Critic Loss=0.529, Actor Loss=9.772]Checkpoint saved to models\sac_checkpoint_epoch150.pth
+Training:  40%|                                                                        | 199/500 [01:44<02:40,  1.87it/s, Critic Loss=0.493, Actor Loss=9.735]Checkpoint saved to models\sac_checkpoint_epoch200.pth
+Training:  50%|                                                            | 249/500 [02:12<02:27,  1.70it/s, Critic Loss=0.455, Actor Loss=9.682]Checkpoint saved to models\sac_checkpoint_epoch250.pth
+Training:  60%|                                                | 299/500 [02:40<01:50,  1.82it/s, Critic Loss=0.420, Actor Loss=9.616]Checkpoint saved to models\sac_checkpoint_epoch300.pth
+Training:  70%|                                    | 349/500 [03:08<01:23,  1.80it/s, Critic Loss=0.399, Actor Loss=9.525]Checkpoint saved to models\sac_checkpoint_epoch350.pth
+Training:  80%|                        | 399/500 [03:36<00:55,  1.80it/s, Critic Loss=0.377, Actor Loss=9.441]Checkpoint saved to models\sac_checkpoint_epoch400.pth
+Training:  81%|                      | 406/500 [03:39<00:51,  1.84it/s, Critic Loss=0.379, Actor Loss=9.427

# 2025-03-27 02:50:10.441776
+y

# 2025-03-27 02:51:38.182907
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 427, in <module>
+    agent = train_sac(
+        dataset_path=args.dataset,
+    ...<2 lines>...
+        save_path=args.save_path
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 320, in train_sac
+    alpha_loss = -(agent.log_alpha * (agent.target_entropy + 0.1).detach()).mean()
+                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+AttributeError: 'float' object has no attribute 'detach'

# 2025-03-27 02:52:52.784405
+y

# 2025-03-27 02:54:59.507448
+Please add all these metrics to the logging                     'critic_loss': 0.0,
+                    'actor_loss': 0.0,
+                    'q1_value': 0.0,
+                    'q2_value': 0.0,
+                    'action_mean': 0.0,
+                    'action_std': 0.0,
+                    'entropy': 0.0,
+                    'grad_norm': 0.0,

# 2025-03-27 02:58:04.170327
+y

# 2025-03-27 02:59:14.022628
+/clear

# 2025-03-27 03:01:23.711354
+I am going to share with you some parts of the training logs. Let me know if the trends make sense and whether we can improve them by tuning some of the parameters. I want to optimise the trainign before launching the big training model on the big dataset epoch,critic_loss,actor_loss,alpha_loss
+0,2.1999052992233863,0.9215599138003129,-0.0022499999031424522
+1,1.3756544314897978,1.2950964294947112,-0.0045899986289441586
+2,1.0065401036005754,1.514365370456989,-0.006929994560778141
+3,0.7791745204191941,1.640039168871366,-0.00926999095827341
+4,0.6430046134270154,1.7787442803382874,-0.011609986424446106
+5,0.5611609312204214,1.919862829721891,-0.013949982821941376
+6,0.5098395863404641,2.057718428281637,-0.016289979219436646
+7,0.47562968272429246,2.2098828829251804,-0.018629973754286766
+8,0.4502480150415347,2.350931011713468,-0.020969970151782036
+9,0.43177887281546223,2.4875554396555972,-0.023309966549277306
+10,0.4181803654019649,2.6271657210129957,-0.025649962946772575
+11,0.40856749220536304,2.761951593252329,-0.027989959344267845
+12,0.40101003876099217,2.8885739216437707,-0.030329996719956398
+13,0.3972702897511996,3.021826138863197,-0.0326700359582901
+14,0.39474377265343297,3.1394157226269064,-0.03501007705926895
+15,0.3916191974511513,3.267082856251643,-0.037350114434957504105,0.5804344137700704,9.712652573218712,-0.2479502409696579
+106,0.5758347921073437,9.70874779040997,-0.2502899169921875
+107,0.5785011895574056,9.712988449976994,-0.2526296079158783
+108,0.580751339546763,9.712605072901798,-0.2549692988395691
+109,0.576468657129086,9.708380919236403,-0.2573089897632599
+110,0.5741521827876568,9.706991269038273,-0.2596486806869507
+111,0.5732062543527439,9.70270718061007,-0.2619883716106415
+112,0.5744718551062621,9.705732088822584,-0.2643280625343323
+113,0.5759209188131186,9.701947872455303,-0.26666775345802307
+114,0.5696706376396693,9.704029009892391,-0.26900744438171387
+115,0.5691175119808087,9.695605057936449,-0.27134713530540466
+116,0.5707639985932753,9.702194177187407,-0.27368682622909546
+117,0.5716019367369322,9.701679046337421,-0.27602651715278625
+118,0.570691782121475,9.69394067617563,-0.27836620807647705
+119,0.569386572505419,9.692556968102089,-0.28070589900016785
+120,0.5690601425102124,9.700058973752535,-0.28304558992385864
+121,0.5672857139546138,9.695607698880709,-0.28538528084754944
+122,0.564824253320694,9.696144287402813,-0.28772497177124023
+123,0.5662811857003432,9.695598345536451,-0.29006466269493103
+124,0.5674303082319406,9.692962132967436,-0.2924043536186218
+125,0.5687898076497592,9.697732118459848,-0.2947440445423126
+126,0.5647081567977483,9.693687145526592,-0.2970837354660034485,0.48572035449055523,9.65097713470459,-1.137237548828125
+486,0.48716077810296643,9.652647752028246,-1.139577865600586
+487,0.4848732188917123,9.648202749399038,-1.1419183015823364
+488,0.48733243575462926,9.648292174706093,-1.1442586183547974
+489,0.484819196164608,9.645361166733961,-1.1465990543365479
+490,0.49142782046244693,9.651422794048603,-1.1489394903182983
+491,0.4847153236086552,9.64562408740704,-1.1512798070907593
+492,0.4875197307421611,9.648416409125694,-1.1536202430725098
+493,0.4844756117806985,9.63961201447707,-1.1559605598449707
+494,0.48774150443764835,9.645555386176476,-1.1583009958267212
+495,0.485980477470618,9.648083686828613,-1.1606414318084717
+496,0.48674721786609065,9.646007721240704,-1.1629817485809326
+497,0.48649603529618335,9.64143180847168,-1.165322184562683
+498,0.4845544111270171,9.643453854780931,-1.167662501335144
+499,0.4848356983409478,9.643127734844501,-1.1700029373168945
+

# 2025-03-27 03:02:44.418107
+y

# 2025-03-27 03:12:27.729439
+/chat-mode what should be the behaviour of every trend? Should they oscillate? COnverge? Final and initial value? How can i know from these metrics if the model is going to perform well with thetest data

# 2025-03-27 03:12:41.713303
+/ask what should be the behaviour of every trend? Should they oscillate? COnverge? Final and initial value? How can i know from these metrics if the model is going to perform well with thete 
+. st data

# 2025-03-27 03:15:14.840978
+create a file training_metrics.md where you explain everything that you mentioned above about how to interpret the different training metrics 

# 2025-03-27 03:16:45.896724
+y

# 2025-03-27 14:00:41.163888
+/add SACCQL_training.py

# 2025-03-27 14:02:05.027492
+Please update the script so that the model and the trainin logs is saved with names containing the datetime of the training. That way, the model will not get erased when the script runs again

# 2025-03-27 14:04:09.973330
+y

# 2025-03-27 14:32:08.763949
+/add SACCQL_testing.py

# 2025-03-27 14:32:49.311581
+/drop SACCQL_testing.py

# 2025-03-27 14:33:00.594194
+/add SACCQL_testing.py

# 2025-03-27 14:33:04.970695
+/drop SACCQL_training.py

# 2025-03-27 14:33:10.481159
+/read-only SACCQL_training.py

# 2025-03-27 14:34:30.250804
+Update SACCQL_testing.py so that I can test the model obtained after running SACCQL_training.py. Be aware of the change in the action space. Now we only have one continuous actions instead of two. Provide metrics to understand if our model is well trained or it is overtrained or undertrain.

# 2025-03-27 14:36:12.525606
+y

# 2025-03-27 15:25:59.696174
+/drop

# 2025-03-27 15:26:04.187124
+/add SACCQL_training.py

# 2025-03-27 15:26:20.098871
+I get this error Training complete. Model saved to models\20250327_144446\sac_final_model_20250327_144446.pth
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 661, in <module>
+    analyze_training_log(log_path=latest_dir / "training_log.csv")
+    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 552, in analyze_training_log
+    generate_readiness_report(df, output_dir)
+    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 624, in generate_readiness_report
+    f.write('\n'.join(report))
+    ~~~~~~~^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\encodings\cp1252.py", line 19, in encode
+    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
+           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+UnicodeEncodeError: 'charmap' codec can't encode character '\u03b1' in position 145: character maps to <undefined>
+

# 2025-03-27 15:26:46.280455
+y

# 2025-03-27 15:27:04.769061
+/clear

# 2025-03-27 15:27:10.349457
+/drop

# 2025-03-27 15:27:13.458349
+/add SACCQL_testing.py

# 2025-03-27 15:27:18.681630
+/read-only SACCQL_training.py

# 2025-03-27 15:30:35.385593
+/drop

# 2025-03-27 15:30:41.706856
+/read-only SACCQL_training.py

# 2025-03-27 15:30:51.425147
+/add SACCQL_testing.py

# 2025-03-27 15:31:10.259113
+Fix these errors emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [239. 238. 235. 233. 231.]
+Sample rewards: [-0.4376941  -0.43894523 -0.44270617 -0.44521952 -0.44773737]
+Evaluating:   0%|                                                                                                                                                                       | 0/23 [00:00<?, ?it/s] 
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 783, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 700, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 256, in evaluate_model
+    actions_pred = model.act(states_norm, deterministic=True)
+TypeError: SACAgent.act() got an unexpected keyword argument 'deterministic'

# 2025-03-27 15:32:52.496094
+y

# 2025-03-27 15:35:34.607941
+Fix these errors emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [239. 238. 235. 233. 231.]
+Sample rewards: [-0.4376941  -0.43894523 -0.44270617 -0.44521952 -0.44773737]
+Evaluating:   0%|                                                                                                                                                                       | 0/23 [00:00<?, ?it/s] 
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 783, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 700, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 256, in evaluate_model
+    actions_pred = model.act(states_norm, deterministic=True)
+TypeError: SACAgent.act() got an unexpected keyword argument 'deterministic'

# 2025-03-27 15:36:11.220300
+y

# 2025-03-27 15:42:09.398408
+/clear

# 2025-03-27 15:42:14.533544
+/exit

# 2025-03-27 15:42:41.931265
+/add SACCQL_testing.py

# 2025-03-27 15:43:04.224642
+/read-only SACCQL_training.py

# 2025-03-27 15:43:12.311610
+ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [239. 238. 235. 233. 231.]
+Sample rewards: [-0.4376941  -0.43894523 -0.44270617 -0.44521952 -0.44773737]
+Evaluating:   0%|                                                                                                                                                                       | 0/23 [00:00<?, ?it/s] 
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 783, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 700, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 256, in evaluate_model
+    actions_pred = model.act(states_norm, deterministic=True)
+TypeError: SACAgent.act() got an unexpected keyword argument 'deterministic'
+

# 2025-03-27 15:44:03.153101
+Y

# 2025-03-27 15:45:30.351787
+n

# 2025-03-27 15:46:46.138663
+ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [239. 238. 235. 233. 231.]
+Sample rewards: [-0.4376941  -0.43894523 -0.44270617 -0.44521952 -0.44773737]
+Evaluating:   0%|                                                                                                                                                                       | 0/23 [00:00<?, ?it/s] 
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 783, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 700, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 256, in evaluate_model
+    actions_pred = model.act(states_norm, deterministic=True)
+TypeError: SACAgent.act() got an unexpected keyword argument 'deterministic'
+

# 2025-03-27 15:47:31.517989
+y

# 2025-03-27 15:52:21.234058
+/drop SACCQL_testing.py

# 2025-03-27 15:52:32.210028
+/read-only SACCQL_testing.py

# 2025-03-27 15:53:14.345894
+/add training_doc.md testing_doc.md

# 2025-03-27 15:54:32.427430
+Update these code documentation codes explaining how the code works step by step, what algorithms and neural networks are implemented, and what graphs are obtained after running the scripts. Explain what every graph means and how we should interpret them. Be detailed

# 2025-03-27 15:56:00.684643
+y

# 2025-03-27 17:52:33.953338
+n

# 2025-03-27 19:48:55.288404
+/drop testing_doc.md training_doc.md

# 2025-03-27 19:49:16.528555
+/read-only dataset_analysis.py

# 2025-03-27 19:49:31.368486
+/read-only dataset_creation.py

# 2025-03-27 19:49:54.311872
+/read-only data_processing.ipynb

# 2025-03-27 19:50:03.333093
+docs\Midterm_instructions.pdf

# 2025-03-27 19:50:09.101627
+n

# 2025-03-27 19:50:43.111622
+/drop data_processing.ipynb

# 2025-03-27 19:50:48.348847
+/clear

# 2025-03-27 19:51:02.540251
+/read-only docs\Midterm_instructions.pdf

# 2025-03-27 19:51:27.159977
+/add README.md

# 2025-03-27 19:52:55.752334
+Update the README.md providing documentation on how the code works and is organised. Explain the main characteristics of the dataset (with actions and states), the neural network architecture, the RL algorithm and the metrics used for evaluating the dataset coverage, the model training and the model testing

# 2025-03-27 19:54:30.278849
+n

# 2025-03-27 19:54:48.808519
+y

# 2025-03-27 19:55:44.752978
+/drop dataset_analysis.py

# 2025-03-27 19:55:59.312025
+/drop dataset_creation.py

# 2025-03-27 19:56:09.544493
+/drop README.md

# 2025-03-27 19:56:17.342002
+/read-only README.md

# 2025-03-27 19:56:36.624353
+/read-only docs\Midterm_instructions.pdf

# 2025-03-27 19:59:33.352748
+/exit

# 2025-03-27 20:00:03.806622
+/read-only README.md SACCQL_testing.py SACCQL_training.py

# 2025-03-27 20:00:14.295942
+/read-only docs\Midterm_instructions.pdf

# 2025-03-27 20:01:26.888054
+/add data_processing.ipynb

# 2025-03-27 20:01:32.784131
+/add docs\ProjectMidterm.txt

# 2025-03-27 20:01:37.120099
+/drop data_processing.ipynb

# 2025-03-27 20:03:22.912950
+Please following the guidelines of docs\Midterm_instructions.pdf and having a look at the code, write the skeleton of the docs\ProjectMidterm.txt making sure you complete stuff in every section of the report. Dont invent things, if you dont know what to add in some parts just write questions at the end of the report that I will try to answer

# 2025-03-27 23:27:38.848606
+/drop

# 2025-03-27 23:28:08.097255
+/clear

# 2025-03-27 23:28:50.131606
+/exit

# 2025-03-27 23:31:00.841253
+/add SACCQL_testing.py

# 2025-03-27 23:31:30.114025
+$ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [179. 183. 187. 191. 195.]
+Sample rewards: [-0.5        -0.5079023  -0.503207   -0.49832824 -0.4933549 ]
+Evaluating:  90%|    | 122/135 [00:00<00:00, 164.94it/s]C:\Users\emili\procoding\Gloop\SACCQL_testing.py:253: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1831.)
+  states_norm = (states - states.mean(0)) / (states.std(0) + 1e-8)
+Evaluating: 100%|| 135/135 [00:00<00:00, 165.37it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 783, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 700, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 288, in evaluate_model
+    metrics["rmse"] = float(np.sqrt(mean_squared_error(all_actions_true, all_actions_pred)))
+                                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\_param_validation.py", line 216, in wrapper
+    return func(*args, **kwargs)
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_regression.py", line 565, in mean_squared_error
+    _check_reg_targets_with_floating_dtype(
+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
+        y_true, y_pred, sample_weight, multioutput, xp=xp
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_regression.py", line 198, in _check_reg_targets_with_floating_dtype
+    y_type, y_true, y_pred, multioutput = _check_reg_targets(
+                                          ~~~~~~~~~~~~~~~~~~^
+        y_true, y_pred, multioutput, dtype=dtype_name, xp=xp
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_regression.py", line 106, in _check_reg_targets
+    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\validation.py", line 1107, in check_array
+    _assert_all_finite(
+    ~~~~~~~~~~~~~~~~~~^
+        array,
+        ^^^^^^
+    ...<2 lines>...
+        allow_nan=ensure_all_finite == "allow-nan",
+        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\validation.py", line 120, in _assert_all_finite
+    _assert_all_finite_element_wise(
+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
+        X,
+        ^^
+    ...<4 lines>...
+        input_name=input_name,
+        ^^^^^^^^^^^^^^^^^^^^^^
+    )
+    ^
+  File "C:\Users\emili\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\validation.py", line 169, in _assert_all_finite_element_wise
+    raise ValueError(msg_err)
+ValueError: Input contains NaN.

# 2025-03-27 23:33:19.839008
+Y

# 2025-03-27 23:34:22.254297
+emili@Emilio-DELL-PC MINGW64 ~/procoding/Gloop (emilio-simple)
+$ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [179. 183. 187. 191. 195.]
+Sample rewards: [-0.5        -0.5079023  -0.503207   -0.49832824 -0.4933549 ]
+C:\Users\emili\procoding\Gloop\SACCQL_testing.py:63: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
+  cleaned = pd.Series(values).fillna(method='ffill').fillna(method='bfill').values
+Evaluating:   0%|                                                                                                  | 0/135 [00:00<?, ?it/s]
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 814, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 731, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 259, in evaluate_model
+    sample = test_dataset[j]
+             ~~~~~~~~~~~~^^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_training.py", line 140, in __getitem__
+    'action': torch.FloatTensor(self.actions[idx]),
+              ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
+TypeError: new(): data must be a sequence (got numpy.float32)
+

# 2025-03-27 23:35:44.123379
+N

# 2025-03-27 23:35:46.402450
+Y

# 2025-03-27 23:37:58.358540
+$ python SACCQL_testing.py
+Using device: cpu
+Loading model (legacy format)
+NaN values in glu_raw - filling with forward/backward fill
+Sample glu_raw inputs: [179. 183. 187. 191. 195.]
+Sample rewards: [-0.5        -0.5079023  -0.503207   -0.49832824 -0.4933549 ]
+C:\Users\emili\procoding\Gloop\SACCQL_testing.py:86: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
+  cleaned = pd.Series(values).fillna(method='ffill').fillna(method='bfill').values
+Evaluating:  88%|          | 119/135 [00:00<00:00, 172.94it/s]C:\Users\emili\procoding\Gloop\SACCQL_testing.py:293: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1831.)
+  states_norm = (states - states.mean(0)) / (states.std(0) + 1e-8)
+Evaluating: 100%|| 135/135 [00:00<00:00, 169.49it/s]
+Warning: Filtering -294191103 invalid samples with NaN values
+Traceback (most recent call last):
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 837, in <module>
+    main()
+    ~~~~^^
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 754, in main
+    metrics = evaluate_model(
+        model,
+    ...<3 lines>...
+        noise_test=args.noise_test
+    )
+  File "C:\Users\emili\procoding\Gloop\SACCQL_testing.py", line 321, in evaluate_model
+    all_actions_true = all_actions_true[valid_mask]
+                       ~~~~~~~~~~~~~~~~^^^^^^^^^^^^
+IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

# 2025-03-27 23:40:59.451355
+Y

# 2025-03-27 23:46:17.764669
+/DROP

# 2025-03-27 23:46:22.018596
+/drop

# 2025-03-27 23:46:29.599732
+/add SACCQL_training.py

# 2025-03-27 23:50:15.303597
+This is what I obstain from the model_readiness.md. # Model Readiness Report
+
+## Training Summary
+- Total Epochs: 500
+- Final Critic Loss: 0.6055
+- Final Actor Loss: 10.0685
+- Final Entropy (alpha): 0.3207
+
+## Readiness Checklist
+-  Critic loss stabilized for last 100 epochs (std=0.0105)
+-  alpha > 0.1 maintained in final 20% of training
+-  Action std between 0.2-0.6 (current=0.7592)
+-  Q-values within [-4.5, -0.5] range (Q1=-9.9496, Q2=-9.9497)
+-  <1% difference between final Q1/Q2 values (0.0019251153657021175%)
+
+## Recommendations
+- Decrease exploration by lowering target entropy
+- Adjust reward scaling or critic network architecture Please adapt the hyperameters, create a callback function that stops training if for the last 50 epochs there has not been significant improvement in the critic loss.

# 2025-03-27 23:51:42.602419
+y

# 2025-03-28 00:07:17.012990
+/drop

# 2025-03-28 00:07:32.820953
+/read-only data_processing.ipynb SACCQL_testing.py

# 2025-03-28 00:07:44.537242
+/read-only SACCQL_training.py

# 2025-03-28 00:09:24.770717
+/read-only docs\Midterm_instructions.pdf

# 2025-03-28 00:10:02.087031
+/exit

# 2025-03-28 00:10:30.312242
+/read-only data_processing.ipynb SACCQL_testing.py

# 2025-03-28 00:10:51.910680
+/read-only docs\Midterm_instructions.pdf

# 2025-03-28 00:11:20.533172
+/read-only SACCQL_training.py

# 2025-03-28 00:11:47.246356
+/add docs\ProjectMidterm.txt

# 2025-03-28 00:14:41.170763
+Update ProjectMidterm.txt following the guidelines of docs\Midterm_instructions.pdf. Write as if you were writing an academic paper. Be detailed in your speech. Take into account the code that is already written. If you have questions that would help with the writing of docs\ProjectMidterm.txt, write them at the end of the document so that I can answer them

# 2025-03-28 00:14:50.810917
+n

# 2025-03-28 00:22:50.988990
+/clear

# 2025-03-28 00:23:21.075350
+/drop data_processing.ipynb

# 2025-03-28 00:23:24.957772
+Update ProjectMidterm.txt following the guidelines of docs\Midterm_instructions.pdf. Write as if you were writing an academic paper. Be detailed in your speech. Take into account the code that is already written. If you have questions that would help with the writing of docs\ProjectMidterm.txt, write them at the end of the document so that I can answer them

# 2025-03-28 00:24:47.493676
+y

# 2025-03-28 00:24:54.425424
+n

# 2025-03-28 00:25:05.368518
+/drop data_processing.ipynb

# 2025-03-28 00:25:09.644546
+Update ProjectMidterm.txt following the guidelines of docs\Midterm_instructions.pdf. Write as if you were writing an academic paper. Be detailed in your speech. Take into account the code that is already written. If you have questions that would help with the writing of docs\ProjectMidterm.txt, write them at the end of the document so that I can answer them

# 2025-03-28 00:25:49.869980
+n

# 2025-03-28 00:25:52.772520
+y

# 2025-03-28 00:28:32.811933
+/drop

# 2025-03-28 00:28:38.311486
+/clear

# 2025-03-28 00:28:54.232898
+/add SACCQL_testing.py SACCQL_training.py

# 2025-03-28 00:31:28.517258
+Having a look at the action distrubution during testing, I see that the predicted actions are mostly -1 and 1 whereas the true ones are more next to the middle. Please update the hyperparameters accordingly to solve this issue. Also check for potential bugs in the training and in the testing phase. Also, add the possibility of training with cql penalty so that we apply conservative q learning. Run the code with gpu if it is available

# 2025-03-28 00:32:25.223043
+y

# 2025-03-28 00:34:35.070744
+/ask should I have cuda installed to run the code?

# 2025-03-28 00:52:55.897712
+remove the callbacks from the code of training please. 

# 2025-03-28 00:53:41.441765
+y

# 2025-03-31 15:31:28.339378
+/add typic_SAC.py

# 2025-03-31 15:33:23.817229
+Please simplify the code to the maximum. It should still work but I want a basic didactical version that would allow me to understand how SAC and CQL work. Reduce the complexity to the max. 

# 2025-03-31 15:34:13.454816
+y

# 2025-03-31 15:36:10.588374
+/undo

# 2025-03-31 15:38:13.319565
+Keeping `DiabetesDataset` and `SACAgent` as they are, simpligy a bit the rest of the code. Especially the training loop. I just want to run 10 iterations to understand if the code is working. For every iteration, show the state action reward next state in the print line to debug

# 2025-03-31 15:40:06.731766
+y

# 2025-04-01 19:14:52.528778
+/add data_processing.ipynb dataset_helpers.py 

# 2025-04-01 19:16:41.993278
+Create a markdown document data.md describing the dataset used, the methods/steps to process it and the resulting features. Explain the results observed after running `evaluate_dataset_coverage`

# 2025-04-01 19:16:50.623642
+n

# 2025-04-01 19:18:55.988703
+/drop data_processing.ipynb

# 2025-04-01 19:20:33.833897
+/clear

# 2025-04-01 19:20:38.307254
+/exit

# 2025-04-01 19:58:36.005674
+/add model_helpers.py

# 2025-04-01 20:00:35.757957
+I am trying to document the architecture of my RL agent and how it is trained. Please create a markdown document  training.md where you explain both the architecture and the training process 

# 2025-04-01 20:01:14.376615
+y

# 2025-04-01 20:02:55.359074
+/add results\featuresensitivity_nHR.png results\featuresensitivity_noHR.png results\models_comparison.png

# 2025-04-01 20:03:02.904184
+/drop training.md

# 2025-04-01 20:08:27.665675
+Create a new markdown file evaluation.md where you explain the different metrics used to evaluate the model. Discuss the results of training both models (full and reduced) where we see that at first sight the models have similar action distribition but looking at the last graph on the model_comparison.png we see that the insulin action actually decreases for higher glucose level in the case of the the reduced model, which actually is not physically or clinically correct. Therefore we can conclude that heart rate is helping the model learn, so the full model is better. Create also a table where the evaluation metrics of both models are displayed Model 1 Evaluation:
+  Avg Q: -9.9713
+  MSE: 0.0175
+  Pearson Correlation: 0.7830
+  R Score: 0.5947
+  Regression Line: slope=0.5847, intercept=-0.1551
+
+Model 2 Evaluation:
+  Avg Q: -9.9678
+  MSE: 0.0178
+  Pearson Correlation: 0.7856
+  R Score: 0.5875
+  Regression Line: slope=0.5884, intercept=-0.1618

# 2025-04-01 20:08:57.450080
+y
