{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/563-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([ 1.4681e+00, -4.1826e-04, -1.5250e-01, -6.6512e-01,  2.8252e-03,\n",
      "         9.5525e-01, -1.3011e+00,  0.0000e+00])\n",
      "Action: tensor([ 0.1000, -1.0000])\n",
      "Reward: tensor(-1.)\n",
      "Next State: tensor([ 1.4681e+00, -4.1826e-04, -1.5250e-01, -6.6512e-01,  2.8252e-03,\n",
      "         9.5525e-01, -1.2861e+00,  0.0000e+00])\n",
      "Done: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(\"State:\", sample[\"state\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"reward\"])\n",
    "print(\"Next State:\", sample[\"next_state\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# --- Critic Update ---\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Get policy actions for next states\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     next_mean, next_log_std = model.actor(next_states)\n\u001b[32m     33\u001b[39m     next_std = next_log_std.exp()\n\u001b[32m     34\u001b[39m     next_normal = torch.distributions.Normal(next_mean, next_std)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100\n",
    "writer = SummaryWriter()\n",
    "csv_file = 'training_stats1.csv'\n",
    "\n",
    "\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['Epoch', 'Iteration', 'TD Loss', 'CQL Penalty', \n",
    "                        'Critic Loss', 'Actor Loss', 'Q1 Value', 'Q2 Value',\n",
    "                        'Action_Mean', 'Action_Std', 'Entropy'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1000), desc=\"Training Progress\"):\n",
    "    metrics = {\n",
    "        'td': 0.0, 'cql': 0.0, 'critic': 0.0, 'actor': 0.0,\n",
    "        'q1': 0.0, 'q2': 0.0, 'action_mean': 0.0, 'action_std': 0.0,\n",
    "        'entropy': 0.0, 'count': 0\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # --- Data Preparation ---\n",
    "        states = batch[\"state\"].to(device)\n",
    "        dataset_actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # --- Critic Update ---\n",
    "        with torch.no_grad():\n",
    "            # Get policy actions for next states\n",
    "            next_mean, next_log_std = model.actor(next_states)\n",
    "            next_std = next_log_std.exp()\n",
    "            next_normal = torch.distributions.Normal(next_mean, next_std)\n",
    "            next_actions = torch.tanh(next_normal.rsample()) * model.action_scale\n",
    "            \n",
    "            # Target Q calculation\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], 1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], 1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "\n",
    "        # Current Q estimates\n",
    "        current_q1 = model.q1(torch.cat([states, dataset_actions], 1))\n",
    "        current_q2 = model.q2(torch.cat([states, dataset_actions], 1))\n",
    "        \n",
    "        # TD Loss\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # CQL Penalty\n",
    "        cql_penalty = compute_cql_penalty(states, dataset_actions, model)\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # Critic optimization\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # --- Actor Update ---\n",
    "        # Generate actions from current policy\n",
    "        mean, log_std = model.actor(states)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)  # Squashed to [-1, 1]\n",
    "        pred_actions = y_t * model.action_scale  # Scaled to insulin range\n",
    "        \n",
    "        # Calculate log probs with tanh correction\n",
    "        log_probs = normal.log_prob(x_t).sum(1)\n",
    "        log_probs -= torch.log(1 - y_t.pow(2) + 1e-6).sum(1)\n",
    "        entropy = -log_probs.mean()\n",
    "\n",
    "        # Q-values for policy actions\n",
    "        q1_pred = model.q1(torch.cat([states, pred_actions], 1))\n",
    "        q2_pred = model.q2(torch.cat([states, pred_actions], 1))\n",
    "        \n",
    "        # Actor loss with entropy regularization\n",
    "        actor_loss = -torch.min(q1_pred, q2_pred).mean() + alpha * entropy\n",
    "\n",
    "        # Actor optimization\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # --- Target Network Update ---\n",
    "        model.update_targets()\n",
    "\n",
    "        # --- Metrics Collection ---\n",
    "        metrics['td'] += td_loss.item()\n",
    "        metrics['cql'] += cql_penalty.item()\n",
    "        metrics['critic'] += critic_loss.item()\n",
    "        metrics['actor'] += actor_loss.item()\n",
    "        metrics['q1'] += q1_pred.mean().item()\n",
    "        metrics['q2'] += q2_pred.mean().item()\n",
    "        metrics['action_mean'] += pred_actions.mean().item()\n",
    "        metrics['action_std'] += pred_actions.std().item()\n",
    "        metrics['entropy'] += entropy.item()\n",
    "        metrics['count'] += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if metrics['count'] > 0:\n",
    "            avg_metrics = {k: v/metrics['count'] for k, v in metrics.items() if k != 'count'}\n",
    "            \n",
    "            # TensorBoard Logging\n",
    "            global_step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/TD', avg_metrics['td'], global_step)\n",
    "            writer.add_scalar('Loss/CQL', avg_metrics['cql'], global_step)\n",
    "            writer.add_scalar('Loss/Critic', avg_metrics['critic'], global_step)\n",
    "            writer.add_scalar('Loss/Actor', avg_metrics['actor'], global_step)\n",
    "            writer.add_scalar('Q_Values/Q1', avg_metrics['q1'], global_step)\n",
    "            writer.add_scalar('Q_Values/Q2', avg_metrics['q2'], global_step)\n",
    "            writer.add_scalar('Actions/Mean', avg_metrics['action_mean'], global_step)\n",
    "            writer.add_scalar('Actions/Std', avg_metrics['action_std'], global_step)\n",
    "            writer.add_scalar('Entropy', avg_metrics['entropy'], global_step)\n",
    "\n",
    "            # CSV Logging\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                csv_writer.writerow([\n",
    "                    epoch, i,\n",
    "                    avg_metrics['td'], avg_metrics['cql'],\n",
    "                    avg_metrics['critic'], avg_metrics['actor'],\n",
    "                    avg_metrics['q1'], avg_metrics['q2'],\n",
    "                    avg_metrics['action_mean'], avg_metrics['action_std'],\n",
    "                    avg_metrics['entropy']\n",
    "                ])\n",
    "            \n",
    "            # Reset metrics\n",
    "            metrics = {k: 0.0 for k in metrics}\n",
    "            metrics['count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
