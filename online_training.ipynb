{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 📌 Import your trained model\n",
    "from model.sac_cql import SACCQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Data Handling\n",
    "# --------------------------\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\"Processed diabetes management dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file):\n",
    "        # Load data and fill missing values\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.ffill().bfill()\n",
    "        \n",
    "        # Ensure key features have no missing values\n",
    "        if df[[\"glu\", \"glu_d\", \"glu_t\", \"iob\", \"hour\"]].isna().any().any():\n",
    "            raise ValueError(\"Dataset contains NaN values after preprocessing\")\n",
    "        \n",
    "        # Verify that action values are within [-1, 1]\n",
    "        assert df[\"action\"].between(-1, 1).all(), \"Actions must be between -1 and 1\"\n",
    "        \n",
    "        # Prepare state features and action values\n",
    "        self.states = df[[\"glu\", \"glu_d\", \"glu_t\", \"iob\", \"hour\"]].values.astype(np.float32)\n",
    "        self.actions = df[\"action\"].values.astype(np.float32).reshape(-1, 1)\n",
    "        \n",
    "        # Compute rewards from the glu_raw values\n",
    "        self.rewards = self._compute_rewards(df[\"glu_raw\"].values)\n",
    "        \n",
    "        # Create transitions: next_states via roll and done flags\n",
    "        self.next_states = np.roll(self.states, -1, axis=0)\n",
    "        self.dones = df[\"done\"].values.astype(np.float32)\n",
    "        \n",
    "        # Remove last transition (invalid next state)\n",
    "        self._sanitize_transitions()\n",
    "    \n",
    "    def _compute_rewards(self, glucose_next):\n",
    "        \"\"\"\n",
    "        Compute rewards using a rescaled Risk Index (RI)-based function.\n",
    "        Based on Kovatchev et al. (2005), extended with a severe hypoglycemia penalty.\n",
    "        \"\"\"\n",
    "        glucose = np.clip(glucose_next.astype(np.float32), 10, 400)  # Clamp extreme values\n",
    "\n",
    "        # Step 1: Risk transformation function\n",
    "        log_glucose = np.log(glucose)\n",
    "        f = 1.509 * (np.power(log_glucose, 1.084) - 5.381)\n",
    "        r = 10 * np.square(f)\n",
    "\n",
    "        # Step 2: LBGI and HBGI\n",
    "        lbgi = np.where(f < 0, r, 0)\n",
    "        hbgi = np.where(f > 0, r, 0)\n",
    "\n",
    "        # Step 3: Total Risk Index (RI)\n",
    "        ri = lbgi + hbgi\n",
    "\n",
    "        # Step 4: Rescale RI and convert to reward\n",
    "        normalized_ri = -ri / 10.0  # Stronger signal than /100\n",
    "        rewards = np.clip(normalized_ri, -5.0, 0.0)\n",
    "\n",
    "        # Step 5: Severe hypoglycemia penalty\n",
    "        severe_hypo_penalty = np.where(glucose <= 39, -15.0, 0.0)\n",
    "        rewards += severe_hypo_penalty\n",
    "\n",
    "        # Step 6: Optional time penalty\n",
    "        rewards -= 0.01  # Encourage faster correction\n",
    "\n",
    "        return np.clip(rewards, -15.0, 0.0).astype(np.float32)\n",
    "\n",
    "\n",
    "    \n",
    "    def _sanitize_transitions(self):\n",
    "        \"\"\"Remove the last transition which lacks a valid next state.\"\"\"\n",
    "        valid_mask = np.ones(len(self.states), dtype=bool)\n",
    "        valid_mask[-1] = False\n",
    "        self.states = self.states[valid_mask]\n",
    "        self.actions = self.actions[valid_mask]\n",
    "        self.rewards = self.rewards[valid_mask]\n",
    "        self.next_states = self.next_states[valid_mask]\n",
    "        self.dones = self.dones[valid_mask]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'state': torch.FloatTensor(self.states[idx]),\n",
    "            'action': torch.FloatTensor(self.actions[idx]),\n",
    "            'reward': torch.FloatTensor([self.rewards[idx]]),\n",
    "            'next_state': torch.FloatTensor(self.next_states[idx]),\n",
    "            'done': torch.FloatTensor([self.dones[idx]])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Actor: Gaussian Policy Network\n",
    "# --------------------------\n",
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=32):\n",
    "        super().__init__()\n",
    "        # Three dense layers with 32 units each\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = nn.Linear(hidden_units, hidden_units)\n",
    "        # Separate output heads for mean and log_std\n",
    "        self.mean_head = nn.Linear(hidden_units, action_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_units, action_dim)\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        \n",
    "        # Initialize weights\n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.mean_head, self.log_std_head]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = self.log_std_head(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(x_t)  # Squash to [-1, 1]\n",
    "        # Compute log probability with tanh correction\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "# --------------------------\n",
    "# Critic: Q-Network\n",
    "# --------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.fc3 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.out = nn.Linear(hidden_units, 1)\n",
    "        \n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.out]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        q = self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# SAC Agent\n",
    "# --------------------------\n",
    "class SACAgent(nn.Module):\n",
    "    \"\"\"SAC agent with a Gaussian policy and twin Q-networks.\"\"\"\n",
    "    def __init__(self, state_dim=8, action_dim=1,\n",
    "                 actor_lr=3e-4, critic_lr=3e-4, alpha_lr=3e-4,\n",
    "                 target_entropy=-1, gamma=0.997, tau=0.005):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.target_entropy = target_entropy\n",
    "        \n",
    "        # Actor (policy) network\n",
    "        self.actor = GaussianActor(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # Twin Q-networks\n",
    "        self.q1 = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.q2 = QNetwork(state_dim, action_dim).to(device)\n",
    "        # Target networks\n",
    "        self.q1_target = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.q2_target = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "        \n",
    "        # Entropy temperature (initialized to 0.1) as log_alpha for numerical stability\n",
    "        self.log_alpha = torch.tensor([0.1], requires_grad=True, device=device)\n",
    "        \n",
    "        # Optimizers for actor, critic, and temperature\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = optim.Adam(list(self.q1.parameters()) + list(self.q2.parameters()), lr=critic_lr)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"Deterministic action for evaluation: use the mean and apply tanh.\"\"\"\n",
    "        mean, _ = self.actor.forward(state)\n",
    "        return torch.tanh(mean)\n",
    "    \n",
    "    def update_targets(self):\n",
    "        \"\"\"Soft-update target networks.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ➕ Dynamically add Gloop repo to path\n",
    "GLOOP_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../../Gloop\"))\n",
    "if GLOOP_PATH not in sys.path:\n",
    "    sys.path.append(GLOOP_PATH)\n",
    "\n",
    "\n",
    "\n",
    "class Controller:\n",
    "    name = \"GloopController\"\n",
    "\n",
    "    def __init__(self, scenario_instance):\n",
    "        print(\">> GloopController initialized\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # 🔁 Load SAC-CQL model\n",
    "        self.model = SACCQL().to(self.device)\n",
    "        checkpoint_path = os.path.join(GLOOP_PATH, \"checkpoints/saccql_trained.pt\")\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"❌ Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "        self.model.eval()\n",
    "        print(\"✅ SAC-CQL model loaded\")\n",
    "\n",
    "    def convert_to_dose(self, x, min_dose=0.0, max_dose=10.0):\n",
    "        \"\"\"Scale [-1, 1] model output to actual insulin range.\"\"\"\n",
    "        return float(np.clip((x + 1) / 2 * max_dose, min_dose, max_dose))\n",
    "\n",
    "    def run(self, measurements, states, inputs, sample):\n",
    "        print(f\"[step={sample}] GloopController running...\")\n",
    "\n",
    "        if sample >= states.shape[0]:\n",
    "            print(f\"[step={sample}] sample out of bounds\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # ⚙️ Build 8D input vector (match model expectations)\n",
    "            state = np.array([\n",
    "                states[sample, 0],  # glucose\n",
    "                states[sample, 1],  # glucose_derivative\n",
    "                               # glucose_trend (placeholder)\n",
    "                states[sample, 2],  # heart_rate\n",
    "                states[sample, 3],  # hr_derivative\n",
    "                0.0,                # heart_rate_trend (placeholder)\n",
    "                states[sample, 4],  # insulin_on_board\n",
    "                (sample % 1440) / 60.0  # hour of day\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            state_tensor = torch.tensor(state).unsqueeze(0).to(self.device)\n",
    "\n",
    "            # 🧠 Inference\n",
    "            with torch.no_grad():\n",
    "                action = self.model.act(state_tensor)[0]\n",
    "            dose = self.convert_to_dose(action)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[step={sample}] ❌ Model inference error: {e}\")\n",
    "            dose = 0.0\n",
    "\n",
    "        try:\n",
    "            # 💉 Inject into simulator\n",
    "            if isinstance(inputs, dict):\n",
    "                target = None\n",
    "                if \"u_insulin\" in inputs and hasattr(inputs[\"u_insulin\"], \"sampled_signal\"):\n",
    "                    target = inputs[\"u_insulin\"]\n",
    "                elif \"uInsulin\" in inputs and hasattr(inputs[\"uInsulin\"], \"sampled_signal\"):\n",
    "                    target = inputs[\"uInsulin\"]\n",
    "\n",
    "                if target:\n",
    "                    target.sampled_signal[sample, 0] = dose\n",
    "                    print(f\"[step={sample}] ✅ Dose injected: {dose:.2f} U/hr\")\n",
    "                else:\n",
    "                    print(f\"[step={sample}] ❌ No insulin input signal found in dict\")\n",
    "\n",
    "            elif isinstance(inputs, np.ndarray):\n",
    "                inputs[sample, 0] = dose\n",
    "                print(f\"[step={sample}] ✅ Dose injected via array: {dose:.2f} U/hr\")\n",
    "\n",
    "            else:\n",
    "                print(f\"[step={sample}] ❌ Unknown input structure\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[step={sample}] ❌ Injection failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
