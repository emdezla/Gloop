{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 5\n",
    "action_dim = 1  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 5.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, sequence_length=1):\n",
    "        # Assume you have loaded your time-series data into these arrays:\n",
    "        self.glucose = np.random.randn(10000)            # (num_timesteps,)\n",
    "        self.glucose_deriv = np.random.randn(10000)      # (num_timesteps,)\n",
    "        self.heart_rate = np.random.randn(10000)         # (num_timesteps,)\n",
    "        self.hr_deriv = np.random.randn(10000)           # (num_timesteps,)\n",
    "        self.iob = np.random.randn(10000)                # (num_timesteps,)\n",
    "        self.insulin_doses = np.random.randn(10000)      # (num_timesteps,)\n",
    "        \n",
    "        # Compute rewards (example: penalize deviations from target glucose)\n",
    "        self.rewards = -np.abs(self.glucose - 100)  # Target = 100 mg/dL\n",
    "        \n",
    "        # States: Stack all 5 time-series features\n",
    "        self.states = np.column_stack([\n",
    "            self.glucose,\n",
    "            self.glucose_deriv,\n",
    "            self.heart_rate,\n",
    "            self.hr_deriv,\n",
    "            self.iob\n",
    "        ])  # Shape: (num_timesteps, 5)\n",
    "        \n",
    "        # Next states: Shift states by 1 timestep\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "        \n",
    "        # \"Done\" flags (0 = episode continues, 1 = episode ends)\n",
    "        # Assume episodes never terminate (modify for real data)\n",
    "        self.dones = np.zeros(len(self.states))\n",
    "        self.dones[-1] = 1  # Mark the end of the dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states) - 1  # Ignore last next_state (no future)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"state\": self.states[idx],          # Shape: (5,)\n",
    "            \"action\": self.insulin_doses[idx],  # Shape: (1,)\n",
    "            \"reward\": self.rewards[idx],        # Shape: (1,)\n",
    "            \"next_state\": self.next_states[idx],# Shape: (5,)\n",
    "            \"done\": self.dones[idx]             # Shape: (1,)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your historical dataset (replace with your data)\n",
    "class DiabetesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.states = np.random.randn(10000, state_dim)  # Replace with real data\n",
    "        self.actions = np.random.randn(10000, action_dim) * 0.05  # Insulin doses\n",
    "        self.rewards = np.random.randn(10000)  # Reward = f(glucose)\n",
    "        self.next_states = np.random.randn(10000, state_dim)\n",
    "        self.dones = np.zeros(10000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.states[idx], self.actions[idx], self.rewards[idx],\n",
    "            self.next_states[idx], self.dones[idx]\n",
    "        )\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for states, actions, rewards, next_states, dones in dataloader:\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device).unsqueeze(1)\n",
    "        \n",
    "        # Critic loss (CQL + TD error)\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1(torch.cat([next_states, next_actions], 1))\n",
    "            q2_next = model.q2(torch.cat([next_states, next_actions], 1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q1 = model.q1(torch.cat([states, actions], 1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], 1))\n",
    "        \n",
    "        # TD loss\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "        \n",
    "        # CQL penalty: logsumexp(Q(s, a')) - Q(s, a)\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1  # Random actions in [-1, 1]\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], 1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], 1))\n",
    "        cql_penalty = (\n",
    "            torch.logsumexp(torch.cat([q1_rand, q2_rand], 1), dim=1).mean() -\n",
    "            (current_q1.mean() + current_q2.mean()) / 2\n",
    "        )\n",
    "        \n",
    "        # Total critic loss\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "        \n",
    "        # Actor loss (maximize Q-value + entropy)\n",
    "        pred_actions = model.actor(states)\n",
    "        q1_pred = model.q1(torch.cat([states, pred_actions], 1))\n",
    "        q2_pred = model.q2(torch.cat([states, pred_actions], 1))\n",
    "        actor_loss = -torch.min(q1_pred, q2_pred).mean()\n",
    "        \n",
    "        # Update critic\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        # Update actor\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Move batch data to device\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. Compute target Q for Critic\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            # clamp is fine, but do it as a separate (non-inplace) assignment\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. Critic forward pass\n",
    "        # -----------------------------\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # 2a. TD Loss (MSE between current Q and target Q)\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 2b. CQL penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = q_rand.clamp(-100, 100)  # not in-place\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4. Actor forward pass\n",
    "        # -----------------------------\n",
    "        # For actor training, we want the Q-value for the action predicted by the current actor\n",
    "        pred_actions = model.actor(states)  # no .clone() or .detach()\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa_actor)\n",
    "        q2_pred = model.q2(sa_actor)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Soft-update target networks\n",
    "        # -----------------------------\n",
    "        model.update_targets()\n",
    "\n",
    "        # Print stats\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100\n",
    "writer = SummaryWriter()\n",
    "csv_file = 'training_stats1.csv'\n",
    "\n",
    "# Write CSV header\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['Epoch', 'Iteration', 'TD Loss', 'CQL Penalty', \n",
    "                        'Critic Loss', 'Actor Loss', 'Q1 Value', 'Q2 Value'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1000), desc=\"Training Progress\"):\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'td': 0.0,\n",
    "        'cql': 0.0,\n",
    "        'critic': 0.0,\n",
    "        'actor': 0.0,\n",
    "        'q1': 0.0,\n",
    "        'q2': 0.0,\n",
    "        'count': 0\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # --- Existing training code ---\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # --- Critic calculations ---\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # TD Loss\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # Inside your training loop:\n",
    "        states = batch[\"state\"].to(device)\n",
    "        dataset_actions = batch[\"action\"].to(device)  # Real actions from dataset\n",
    "\n",
    "        # Compute CQL penalty using DATASET actions (not random ones!)\n",
    "        cql_penalty = compute_cql_penalty(states, dataset_actions, model)\n",
    "        \n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # Critic update\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # --- Actor calculations ---\n",
    "        pred_actions = model.actor(states)\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs detected, skipping actor update\")\n",
    "            q1_pred = torch.tensor(0.0)  # Default values\n",
    "            q2_pred = torch.tensor(0.0)\n",
    "            actor_loss = torch.tensor(0.0)\n",
    "        else:\n",
    "            sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "            q1_pred = model.q1(sa_actor)\n",
    "            q2_pred = model.q2(sa_actor)\n",
    "            actor_loss = -torch.min(q1_pred, q2_pred).mean() + alpha * policy_entropy\n",
    "\n",
    "            # Actor update\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "        # Target network updates\n",
    "        model.update_targets()\n",
    "\n",
    "        # --- Metrics collection ---\n",
    "        metrics['td'] += td_loss.item()\n",
    "        metrics['cql'] += cql_penalty.item()\n",
    "        metrics['critic'] += critic_loss.item()\n",
    "        metrics['actor'] += actor_loss.item() if not torch.isnan(pred_actions).any() else 0\n",
    "        metrics['q1'] += q1_pred.mean().item()\n",
    "        metrics['q2'] += q2_pred.mean().item()\n",
    "        metrics['count'] += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if metrics['count'] > 0:  # Log after every batch\n",
    "            # Calculate averages\n",
    "            avg_td = metrics['td'] / metrics['count']\n",
    "            avg_cql = metrics['cql'] / metrics['count']\n",
    "            avg_critic = metrics['critic'] / metrics['count']\n",
    "            avg_actor = metrics['actor'] / metrics['count']\n",
    "            avg_q1 = metrics['q1'] / metrics['count']\n",
    "            avg_q2 = metrics['q2'] / metrics['count']\n",
    "\n",
    "            # TensorBoard logging\n",
    "            global_step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/TD', avg_td, global_step)\n",
    "            writer.add_scalar('Loss/CQL', avg_cql, global_step)\n",
    "            writer.add_scalar('Loss/Critic', avg_critic, global_step)\n",
    "            writer.add_scalar('Loss/Actor', avg_actor, global_step)\n",
    "            writer.add_scalar('Q_Values/Q1', avg_q1, global_step)\n",
    "            writer.add_scalar('Q_Values/Q2', avg_q2, global_step)\n",
    "            writer.add_scalar('Actions/Mean', pred_actions.mean(), global_step)\n",
    "            writer.add_scalar('Actions/Std', pred_actions.std(), global_step)\n",
    "\n",
    "            # CSV logging\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, i, avg_td, avg_cql, avg_critic,\n",
    "                                    avg_actor, avg_q1, avg_q2])\n",
    "\n",
    "            # Reset metrics\n",
    "            metrics = {k: 0.0 for k in metrics}\n",
    "            metrics['count'] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
