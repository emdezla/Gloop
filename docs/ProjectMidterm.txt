Gloop: Expanding the Loop for Automatic Diabetes Management

Abstract:
[Abstract needs to be filled based on the final results and conclusions of the project. Please provide key findings and a summary of the approach.]

Introduction:
We aim to leverage artificial intelligence to enhance current artificial pancreas systems used by individuals with Type 1 diabetes. Most existing systems utilize physiological models, such as the Oref1 or Oref0, which are deterministic and do not adapt or improve over time. These models determine insulin dose injections based on glucose levels and macronutrient intake. However, we propose to significantly improve these systems using reinforcement learning (RL) to optimize the closed-loop insulin delivery. By eliminating the need for manual meal intake input and replacing it with heart rate data—automatically tracked through smartwatches—we intend to reduce the mental burden on diabetic patients. By integrating various time series data, we aim to refine our reinforcement learning algorithm and potentially pair it with a large language model (LLM) to provide personalized management recommendations for this chronic illness.

Related Work:
Reinforcement learning has been previously explored in the context of artificial pancreas systems in both offline and online settings. Various algorithms such as Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO), Generalized Conservative Policy Critic (GCPC), and Deep Deterministic Policy Gradient (DDPG) have been employed. However, no existing model has thoroughly investigated how additional data sources, such as heart rate, influence the performance and efficacy of the artificial pancreas loop.

Problem Statement:
Deterministic physiological models for artificial pancreas systems provide a foundational framework for insulin delivery but lack the capacity for continuous adaptation and optimization based on real-time physiological data. The primary challenge is to develop an adaptive reinforcement learning-based artificial pancreas that can autonomously optimize insulin dosing without requiring manual input for meal intakes. Mathematically, this involves formulating the insulin delivery process as a Markov Decision Process (MDP) where the state encompasses various physiological measurements, and the action represents insulin dosing decisions aimed at maintaining optimal glucose levels.

Proposed Approach:
Our approach involves a two-tiered reinforcement learning framework. Initially, we employ offline reinforcement learning to develop a robust general model using historical real-world data from six diabetic patients collected via the OhioT1DM dataset. This model serves as a foundational policy. Subsequently, we train an online reinforcement learning agent within a virtual simulation environment to fine-tune the policy for individual patients, allowing for personalization. We utilize the SAC-CQL algorithm, which integrates Soft Actor-Critic with Conservative Q-Learning to enhance policy stability and performance. Comparative analyses will be conducted between models incorporating CQL and those without, as well as between models utilizing heart rate data versus those that do not, to evaluate the impact of these enhancements on system performance.

Experimental Methodology:
Our dataset, OhioT1DM, consists of data from six diabetic patients equipped with insulin pumps, glucose sensors, and health smartwatches over the course of one month, with an additional ten days per patient designated for testing. The data preprocessing pipeline transforms raw physiological measurements into reinforcement learning-compatible datasets comprising (state, action, done) tuples. Actions are derived from total insulin administered, combining both bolus and basal units. States include normalized physiological metrics: glucose levels, glucose derivatives, glucose trends, heart rate, heart rate derivatives, heart rate trends, insulin on board (IOB), and hour of the day. Data points are recorded at five-minute intervals and aligned by selecting the closest available measurements within a five-minute window.

Results and Discussion:
[Provide detailed analysis of the model's performance, including quantitative metrics such as RMSE, MAE, R² score, and clinical metrics like Time-in-Range (TIR), hypoglycemia and hyperglycemia percentages. Discuss the effectiveness of incorporating CQL and heart rate data in improving insulin delivery accuracy and patient outcomes.]

Next Steps and New Research Ideas:
The immediate next step involves fine-tuning our reinforcement learning model using online training within a simulated environment, enabling the model to adapt dynamically to individual patient responses. Additionally, we plan to integrate a large language model (LLM) to work alongside our reinforcement learning agent, offering personalized management recommendations based on real-time data. Future research may explore the incorporation of additional physiological data sources to further enhance model accuracy and responsiveness, as well as the application of this framework to a broader population of diabetic patients.

Questions:
1. What specific results and metrics have been obtained so far that should be included in the 'Results and Discussion' section?
2. Are there any challenges or limitations encountered during the implementation that should be addressed?
3. What are the key findings from the comparative analyses between models with and without CQL or heart rate data?
4. How does the model performance translate to clinical outcomes for diabetic patients?
5. Are there any user feedback or patient-reported outcomes that could be incorporated into the discussion?
