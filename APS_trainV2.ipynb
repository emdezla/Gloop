{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 5\n",
    "action_dim = 1  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 5.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, sequence_length=1):\n",
    "        # Assume you have loaded your time-series data into these arrays:\n",
    "        self.glucose = np.random.randn(10000)            # (num_timesteps,)\n",
    "        self.glucose_deriv = np.random.randn(10000)      # (num_timesteps,)\n",
    "        self.heart_rate = np.random.randn(10000)         # (num_timesteps,)\n",
    "        self.hr_deriv = np.random.randn(10000)           # (num_timesteps,)\n",
    "        self.iob = np.random.randn(10000)                # (num_timesteps,)\n",
    "        self.insulin_doses = np.random.randn(10000)      # (num_timesteps,)\n",
    "        \n",
    "        # Compute rewards (example: penalize deviations from target glucose)\n",
    "        self.rewards = -np.abs(self.glucose - 100)  # Target = 100 mg/dL\n",
    "        \n",
    "        # States: Stack all 5 time-series features\n",
    "        self.states = np.column_stack([\n",
    "            self.glucose,\n",
    "            self.glucose_deriv,\n",
    "            self.heart_rate,\n",
    "            self.hr_deriv,\n",
    "            self.iob\n",
    "        ])  # Shape: (num_timesteps, 5)\n",
    "        \n",
    "        # Next states: Shift states by 1 timestep\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "        \n",
    "        # \"Done\" flags (0 = episode continues, 1 = episode ends)\n",
    "        # Assume episodes never terminate (modify for real data)\n",
    "        self.dones = np.zeros(len(self.states))\n",
    "        self.dones[-1] = 1  # Mark the end of the dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states) - 1  # Ignore last next_state (no future)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"state\": self.states[idx],          # Shape: (5,)\n",
    "            \"action\": self.insulin_doses[idx],  # Shape: (1,)\n",
    "            \"reward\": self.rewards[idx],        # Shape: (1,)\n",
    "            \"next_state\": self.next_states[idx],# Shape: (5,)\n",
    "            \"done\": self.dones[idx]             # Shape: (1,)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your historical dataset (replace with your data)\n",
    "class DiabetesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.states = np.random.randn(10000, state_dim)  # Replace with real data\n",
    "        self.actions = np.random.randn(10000, action_dim) * 0.05  # Insulin doses\n",
    "        self.rewards = np.random.randn(10000)  # Reward = f(glucose)\n",
    "        self.next_states = np.random.randn(10000, state_dim)\n",
    "        self.dones = np.zeros(10000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.states[idx], self.actions[idx], self.rewards[idx],\n",
    "            self.next_states[idx], self.dones[idx]\n",
    "        )\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for states, actions, rewards, next_states, dones in dataloader:\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device).unsqueeze(1)\n",
    "        \n",
    "        # Critic loss (CQL + TD error)\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1(torch.cat([next_states, next_actions], 1))\n",
    "            q2_next = model.q2(torch.cat([next_states, next_actions], 1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q1 = model.q1(torch.cat([states, actions], 1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], 1))\n",
    "        \n",
    "        # TD loss\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "        \n",
    "        # CQL penalty: logsumexp(Q(s, a')) - Q(s, a)\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1  # Random actions in [-1, 1]\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], 1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], 1))\n",
    "        cql_penalty = (\n",
    "            torch.logsumexp(torch.cat([q1_rand, q2_rand], 1), dim=1).mean() -\n",
    "            (current_q1.mean() + current_q2.mean()) / 2\n",
    "        )\n",
    "        \n",
    "        # Total critic loss\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "        \n",
    "        # Actor loss (maximize Q-value + entropy)\n",
    "        pred_actions = model.actor(states)\n",
    "        q1_pred = model.q1(torch.cat([states, pred_actions], 1))\n",
    "        q2_pred = model.q2(torch.cat([states, pred_actions], 1))\n",
    "        actor_loss = -torch.min(q1_pred, q2_pred).mean()\n",
    "        \n",
    "        # Update critic\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        # Update actor\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
