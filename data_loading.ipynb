{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm  # For progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load and clean CSV data\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df.ffill().bfill()\n",
    "\n",
    "        # Extract state features (8 dimensions)\n",
    "        self.states = self.df[[\n",
    "            \"glu\", \"glu_d\", \"glu_t\",\n",
    "            \"hr\", \"hr_d\", \"hr_t\",\n",
    "            \"iob\", \"hour\"\n",
    "        ]].values.astype(np.float32)\n",
    "\n",
    "        # Extract action features (2 dimensions)\n",
    "        self.actions = self.df[[\"basal\", \"bolus\"]].values.astype(np.float32)\n",
    "\n",
    "        # Extract done flags\n",
    "        self.dones = self.df[\"done\"].values.astype(np.float32)\n",
    "\n",
    "        # Compute rewards based on glu_raw at t+1\n",
    "        glucose_next_tensor = torch.tensor(self.df[\"glu_raw\"].values, dtype=torch.float32)\n",
    "        self.rewards = compute_reward_torch(glucose_next_tensor) / 15.0  # Normalize if needed\n",
    "\n",
    "        # Compute next_states using vectorized roll\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "\n",
    "        # Prevent transitions across episode boundaries\n",
    "        self.next_states[self.dones == 1] = self.states[self.dones == 1]\n",
    "\n",
    "        # Slice to make all arrays align: remove last step (no next state), and align reward with t\n",
    "\n",
    "        self.states      = self.states[:-2]\n",
    "        self.actions     = self.actions[:-2]\n",
    "        self.rewards     = self.rewards[1:-1]\n",
    "        self.next_states = self.next_states[:-2]\n",
    "        self.dones       = self.dones[:-2]\n",
    "        self.dones       = torch.tensor(self.dones, dtype=torch.float32)\n",
    "\n",
    "        # Sanity check\n",
    "        L = len(self.states)\n",
    "        assert all(len(arr) == L for arr in [self.actions, self.rewards, self.next_states, self.dones]), \\\n",
    "            f\"Inconsistent lengths in dataset components: {L}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"state\":      torch.from_numpy(self.states[idx]).float(),\n",
    "            \"action\":     torch.from_numpy(self.actions[idx]).float(),\n",
    "            \"reward\":     self.rewards[idx].float(),\n",
    "            \"next_state\": torch.from_numpy(self.next_states[idx]).float(),\n",
    "            \"done\":       self.dones[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.q1_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ) # Same as q1\n",
    "        self.q2_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )  # Same as q2\n",
    "        \n",
    "        # Initialize targets to match main critics\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update_targets(self, tau=0.005):\n",
    "        # Soft update: target = tau * main + (1-tau) * target\n",
    "        with torch.no_grad():\n",
    "            for t, m in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "            for t, m in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_torch(glucose_next):\n",
    "    \"\"\"\n",
    "    Compute RI-based reward in PyTorch.\n",
    "    \"\"\"\n",
    "    glucose_next = torch.clamp(glucose_next, min=1e-6)\n",
    "    log_term = torch.log(glucose_next) ** 1.084\n",
    "    f = 1.509 * (log_term - 5.381)\n",
    "    ri = 10 * f ** 2\n",
    "\n",
    "    reward = -torch.clamp(ri / 100.0, 0, 1)\n",
    "    reward[glucose_next <= 39.0] = -15.0\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def debug_tensor(tensor, name=\"\", check_grad=False, threshold=1e6):\n",
    "    \"\"\"\n",
    "    Prints diagnostic information about a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor to check.\n",
    "        name (str): Optional name for logging.\n",
    "        check_grad (bool): Also check gradients if available.\n",
    "        threshold (float): Warn if values exceed this.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t_min = tensor.min().item()\n",
    "        t_max = tensor.max().item()\n",
    "        t_mean = tensor.mean().item()\n",
    "        t_std = tensor.std().item()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not extract stats for {name}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß™ [{name}] Shape: {tuple(tensor.shape)} | min: {t_min:.4f}, max: {t_max:.4f}, mean: {t_mean:.4f}, std: {t_std:.4f}\")\n",
    "\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"‚ùå NaNs detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"‚ùå Infs detected in {name}\")\n",
    "    if abs(t_min) > threshold or abs(t_max) > threshold:\n",
    "        print(f\"‚ö†Ô∏è Extreme values detected in {name}: values exceed ¬±{threshold}\")\n",
    "\n",
    "    if check_grad and tensor.requires_grad and tensor.grad is not None:\n",
    "        grad = tensor.grad\n",
    "        print(f\"üîÅ [{name}.grad] norm: {grad.norm().item():.4f}\")\n",
    "        if torch.isnan(grad).any():\n",
    "            print(f\"‚ùå NaNs in gradient of {name}\")\n",
    "        if torch.isinf(grad).any():\n",
    "            print(f\"‚ùå Infs in gradient of {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cql_penalty(states, dataset_actions, model, num_action_samples=10):\n",
    "    \"\"\"\n",
    "    Fixed CQL penalty calculation\n",
    "    Args:\n",
    "        states: Current states from batch (batch_size, state_dim)\n",
    "        dataset_actions: Actions taken in the dataset (batch_size, action_dim)\n",
    "        model: Reference to the agent's networks\n",
    "    \"\"\"\n",
    "    batch_size = states.shape[0]\n",
    "    \n",
    "    # 1. Get policy-generated actions (from current actor)\n",
    "    with torch.no_grad():\n",
    "        policy_actions = model.actor(states)  # (batch_size, action_dim)\n",
    "    \n",
    "    # 2. Combine dataset actions + policy actions\n",
    "    # Shape: (2*batch_size, action_dim)\n",
    "    all_actions = torch.cat([dataset_actions, policy_actions], dim=0)\n",
    "    \n",
    "    # 3. Expand states to match action candidates\n",
    "    # Shape: (2*batch_size, state_dim)\n",
    "    expanded_states = states.repeat(2, 1)\n",
    "    \n",
    "    # 4. Compute Q-values for ALL action candidates\n",
    "    q1_all = model.q1(torch.cat([expanded_states, all_actions], dim=1))\n",
    "    q2_all = model.q2(torch.cat([expanded_states, all_actions], dim=1))\n",
    "    \n",
    "    # 5. Compute Q-values for DATASET actions (original batch)\n",
    "    q1_data = model.q1(torch.cat([states, dataset_actions], dim=1))\n",
    "    q2_data = model.q2(torch.cat([states, dataset_actions], dim=1))\n",
    "    \n",
    "    # 6. CQL Penalty = logsumexp(Q_all) - mean(Q_dataset)\n",
    "    logsumexp = torch.logsumexp(\n",
    "        torch.cat([q1_all, q2_all], dim=1),  # Shape: (2*batch_size, 2)\n",
    "        dim=0\n",
    "    ).mean()\n",
    "    \n",
    "    dataset_q_mean = 0.5 * (q1_data.mean() + q2_data.mean())\n",
    "    \n",
    "    return logsumexp - dataset_q_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 0.25  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/563-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([ 1.4681e+00, -4.1826e-04, -1.5250e-01, -6.6512e-01,  2.8252e-03,\n",
      "         9.5525e-01, -1.3011e+00,  0.0000e+00])\n",
      "Action: tensor([ 0.1000, -1.0000])\n",
      "Reward: tensor(-1.)\n",
      "Next State: tensor([ 1.4681e+00, -4.1826e-04, -1.5250e-01, -6.6512e-01,  2.8252e-03,\n",
      "         9.5525e-01, -1.2861e+00,  0.0000e+00])\n",
      "Done: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(\"State:\", sample[\"state\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"reward\"])\n",
    "print(\"Next State:\", sample[\"next_state\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # üß™ Check raw inputs\n",
    "        debug_tensor(states, \"states\")\n",
    "        debug_tensor(actions, \"actions\")\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # Critic Loss (CQL + TD error)\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            # Use TARGET critics for Q_next\n",
    "            debug_tensor(next_actions, \"next_actions\")\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = torch.clamp(target_q, min=-200, max=0)\n",
    "\n",
    "        current_q1 = model.q1(torch.cat([states, actions], dim=1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], dim=1))\n",
    "\n",
    "        debug_tensor(current_q1, \"current_q1\")\n",
    "        debug_tensor(current_q2, \"current_q2\")\n",
    "\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "\n",
    "        # -----------------------------\n",
    "        # CQL Penalty\n",
    "        # -----------------------------\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = torch.clamp(q_rand, min=-100, max=100)\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - (current_q1.mean() + current_q2.mean()) / 2\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        debug_tensor(critic_loss, \"critic_loss\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Actor Loss\n",
    "        # -----------------------------\n",
    "        states_actor = states.clone().detach()# prevent in-place gradient conflict\n",
    "        pred_actions = model.actor(states_actor)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor outputs! Fix this before training.\")\n",
    "            pred_actions = torch.nan_to_num(pred_actions.clone())  # ‚úÖ clone before modifying\n",
    "        \"\"\"\n",
    "        \n",
    "        # üîí Safe skip if NaNs\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        debug_tensor(pred_actions, \"pred_actions\")\n",
    "        \n",
    "\n",
    "        sa = torch.cat([states_actor, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa)\n",
    "        q2_pred = model.q2(sa)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    " \n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        debug_tensor(actor_loss, \"actor_loss\")\n",
    "\n",
    "        if torch.isnan(actor_loss).any():\n",
    "            print(\"‚ùå Skipping batch due to NaN in actor loss.\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # After critic and actor updates:\n",
    "        model.update_targets()  # Add this line\n",
    "\n",
    "        # -----------------------------\n",
    "        # Print Stats\n",
    "        # -----------------------------\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(f\"Current Q1: min={current_q1.min().item():.2f}, max={current_q1.max().item():.2f}, mean={current_q1.mean().item():.2f}\")\n",
    "            print(f\"Current Q2: min={current_q2.min().item():.2f}, max={current_q2.max().item():.2f}, mean={current_q2.mean().item():.2f}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Move batch data to device\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. Compute target Q for Critic\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            # clamp is fine, but do it as a separate (non-inplace) assignment\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. Critic forward pass\n",
    "        # -----------------------------\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # 2a. TD Loss (MSE between current Q and target Q)\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 2b. CQL penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = q_rand.clamp(-100, 100)  # not in-place\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4. Actor forward pass\n",
    "        # -----------------------------\n",
    "        # For actor training, we want the Q-value for the action predicted by the current actor\n",
    "        pred_actions = model.actor(states)  # no .clone() or .detach()\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa_actor)\n",
    "        q2_pred = model.q2(sa_actor)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Soft-update target networks\n",
    "        # -----------------------------\n",
    "        model.update_targets()\n",
    "\n",
    "        # Print stats\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|‚ñà         | 103/1000 [05:47<50:28,  3.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m dataset_actions = batch[\u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m].to(device)  \u001b[38;5;66;03m# Real actions from dataset\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Compute CQL penalty using DATASET actions (not random ones!)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m cql_penalty = \u001b[43mcompute_cql_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m critic_loss = td_loss + cql_weight * cql_penalty\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Critic update\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mcompute_cql_penalty\u001b[39m\u001b[34m(states, dataset_actions, model, num_action_samples)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 5. Compute Q-values for DATASET actions (original batch)\u001b[39;00m\n\u001b[32m     28\u001b[39m q1_data = model.q1(torch.cat([states, dataset_actions], dim=\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m q2_data = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_actions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 6. CQL Penalty = logsumexp(Q_all) - mean(Q_dataset)\u001b[39;00m\n\u001b[32m     32\u001b[39m logsumexp = torch.logsumexp(\n\u001b[32m     33\u001b[39m     torch.cat([q1_all, q2_all], dim=\u001b[32m1\u001b[39m),  \u001b[38;5;66;03m# Shape: (2*batch_size, 2)\u001b[39;00m\n\u001b[32m     34\u001b[39m     dim=\u001b[32m0\u001b[39m\n\u001b[32m     35\u001b[39m ).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize logging tools\n",
    "writer = SummaryWriter()\n",
    "csv_file = 'training_stats1.csv'\n",
    "\n",
    "# Write CSV header\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['Epoch', 'Iteration', 'TD Loss', 'CQL Penalty', \n",
    "                        'Critic Loss', 'Actor Loss', 'Q1 Value', 'Q2 Value'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1000), desc=\"Training Progress\"):\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'td': 0.0,\n",
    "        'cql': 0.0,\n",
    "        'critic': 0.0,\n",
    "        'actor': 0.0,\n",
    "        'q1': 0.0,\n",
    "        'q2': 0.0,\n",
    "        'count': 0\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # --- Existing training code ---\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # --- Critic calculations ---\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # TD Loss\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # Inside your training loop:\n",
    "        states = batch[\"state\"].to(device)\n",
    "        dataset_actions = batch[\"action\"].to(device)  # Real actions from dataset\n",
    "\n",
    "        # Compute CQL penalty using DATASET actions (not random ones!)\n",
    "        cql_penalty = compute_cql_penalty(states, dataset_actions, model)\n",
    "        \n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # Critic update\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # --- Actor calculations ---\n",
    "        pred_actions = model.actor(states)\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs detected, skipping actor update\")\n",
    "            q1_pred = torch.tensor(0.0)  # Default values\n",
    "            q2_pred = torch.tensor(0.0)\n",
    "            actor_loss = torch.tensor(0.0)\n",
    "        else:\n",
    "            sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "            q1_pred = model.q1(sa_actor)\n",
    "            q2_pred = model.q2(sa_actor)\n",
    "            actor_loss = -torch.min(q1_pred, q2_pred).mean() + alpha * policy_entropy\n",
    "\n",
    "            # Actor update\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "        # Target network updates\n",
    "        model.update_targets()\n",
    "\n",
    "        # --- Metrics collection ---\n",
    "        metrics['td'] += td_loss.item()\n",
    "        metrics['cql'] += cql_penalty.item()\n",
    "        metrics['critic'] += critic_loss.item()\n",
    "        metrics['actor'] += actor_loss.item() if not torch.isnan(pred_actions).any() else 0\n",
    "        metrics['q1'] += q1_pred.mean().item()\n",
    "        metrics['q2'] += q2_pred.mean().item()\n",
    "        metrics['count'] += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if metrics['count'] > 0:  # Log after every batch\n",
    "            # Calculate averages\n",
    "            avg_td = metrics['td'] / metrics['count']\n",
    "            avg_cql = metrics['cql'] / metrics['count']\n",
    "            avg_critic = metrics['critic'] / metrics['count']\n",
    "            avg_actor = metrics['actor'] / metrics['count']\n",
    "            avg_q1 = metrics['q1'] / metrics['count']\n",
    "            avg_q2 = metrics['q2'] / metrics['count']\n",
    "\n",
    "            # TensorBoard logging\n",
    "            global_step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/TD', avg_td, global_step)\n",
    "            writer.add_scalar('Loss/CQL', avg_cql, global_step)\n",
    "            writer.add_scalar('Loss/Critic', avg_critic, global_step)\n",
    "            writer.add_scalar('Loss/Actor', avg_actor, global_step)\n",
    "            writer.add_scalar('Q_Values/Q1', avg_q1, global_step)\n",
    "            writer.add_scalar('Q_Values/Q2', avg_q2, global_step)\n",
    "            writer.add_scalar('Actions/Mean', pred_actions.mean(), global_step)\n",
    "            writer.add_scalar('Actions/Std', pred_actions.std(), global_step)\n",
    "\n",
    "            # CSV logging\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, i, avg_td, avg_cql, avg_critic,\n",
    "                                    avg_actor, avg_q1, avg_q2])\n",
    "\n",
    "            # Reset metrics\n",
    "            metrics = {k: 0.0 for k in metrics}\n",
    "            metrics['count'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (3224537314.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtensorboard --logdir=runs\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
