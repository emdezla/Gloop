{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm  # For progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load and clean CSV data\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df.ffill().bfill()\n",
    "\n",
    "        # Extract state features (8 dimensions)\n",
    "        self.states = self.df[[\n",
    "            \"glu\", \"glu_d\", \"glu_t\",\n",
    "            \"hr\", \"hr_d\", \"hr_t\",\n",
    "            \"iob\", \"hour\"\n",
    "        ]].values.astype(np.float32)\n",
    "\n",
    "        # Extract action features (2 dimensions)\n",
    "        self.actions = self.df[[\"basal\", \"bolus\"]].values.astype(np.float32)\n",
    "\n",
    "        # Extract done flags\n",
    "        self.dones = self.df[\"done\"].values.astype(np.float32)\n",
    "\n",
    "        # Compute rewards based on glu_raw at t+1\n",
    "        glucose_next_tensor = torch.tensor(self.df[\"glu_raw\"].values, dtype=torch.float32)\n",
    "        self.rewards = compute_reward_torch(glucose_next_tensor) / 15.0  # Normalize if needed\n",
    "\n",
    "        # Compute next_states using vectorized roll\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "\n",
    "        # Prevent transitions across episode boundaries\n",
    "        self.next_states[self.dones == 1] = self.states[self.dones == 1]\n",
    "\n",
    "        # Slice to make all arrays align: remove last step (no next state), and align reward with t\n",
    "\n",
    "        self.states      = self.states[:-2]\n",
    "        self.actions     = self.actions[:-2]\n",
    "        self.rewards     = self.rewards[1:-1]\n",
    "        self.next_states = self.next_states[:-2]\n",
    "        self.dones       = self.dones[:-2]\n",
    "        self.dones       = torch.tensor(self.dones, dtype=torch.float32)\n",
    "\n",
    "        # Sanity check\n",
    "        L = len(self.states)\n",
    "        assert all(len(arr) == L for arr in [self.actions, self.rewards, self.next_states, self.dones]), \\\n",
    "            f\"Inconsistent lengths in dataset components: {L}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"state\":      torch.from_numpy(self.states[idx]).float(),\n",
    "            \"action\":     torch.from_numpy(self.actions[idx]).float(),\n",
    "            \"reward\":     self.rewards[idx].float(),\n",
    "            \"next_state\": torch.from_numpy(self.next_states[idx]).float(),\n",
    "            \"done\":       self.dones[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_torch(glucose_next):\n",
    "    \"\"\"\n",
    "    Compute RI-based reward in PyTorch.\n",
    "    \"\"\"\n",
    "    glucose_next = torch.clamp(glucose_next, min=1e-6)\n",
    "    log_term = torch.log(glucose_next) ** 1.084\n",
    "    f = 1.509 * (log_term - 5.381)\n",
    "    ri = 10 * f ** 2\n",
    "\n",
    "    reward = -torch.clamp(ri / 100.0, 0, 1)\n",
    "    reward[glucose_next <= 39.0] = -15.0\n",
    "    return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8526,  0.0000])\n",
      "Action: tensor([-1.0147, -0.0992])\n",
      "Reward: tensor(-0.0003)\n",
      "Next State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8258,  0.0000])\n",
      "Done: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(\"State:\", sample[\"state\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"reward\"])\n",
    "print(\"Next State:\", sample[\"next_state\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.q1_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ) # Same as q1\n",
    "        self.q2_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )  # Same as q2\n",
    "        \n",
    "        # Initialize targets to match main critics\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update_targets(self, tau=0.005):\n",
    "        # Soft update: target = tau * main + (1-tau) * target\n",
    "        with torch.no_grad():\n",
    "            for t, m in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "            for t, m in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def debug_tensor(tensor, name=\"\", check_grad=False, threshold=1e6):\n",
    "    \"\"\"\n",
    "    Prints diagnostic information about a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor to check.\n",
    "        name (str): Optional name for logging.\n",
    "        check_grad (bool): Also check gradients if available.\n",
    "        threshold (float): Warn if values exceed this.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t_min = tensor.min().item()\n",
    "        t_max = tensor.max().item()\n",
    "        t_mean = tensor.mean().item()\n",
    "        t_std = tensor.std().item()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not extract stats for {name}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß™ [{name}] Shape: {tuple(tensor.shape)} | min: {t_min:.4f}, max: {t_max:.4f}, mean: {t_mean:.4f}, std: {t_std:.4f}\")\n",
    "\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"‚ùå NaNs detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"‚ùå Infs detected in {name}\")\n",
    "    if abs(t_min) > threshold or abs(t_max) > threshold:\n",
    "        print(f\"‚ö†Ô∏è Extreme values detected in {name}: values exceed ¬±{threshold}\")\n",
    "\n",
    "    if check_grad and tensor.requires_grad and tensor.grad is not None:\n",
    "        grad = tensor.grad\n",
    "        print(f\"üîÅ [{name}.grad] norm: {grad.norm().item():.4f}\")\n",
    "        if torch.isnan(grad).any():\n",
    "            print(f\"‚ùå NaNs in gradient of {name}\")\n",
    "        if torch.isinf(grad).any():\n",
    "            print(f\"‚ùå Infs in gradient of {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cql_penalty(q1, q2, num_action_samples=100):\n",
    "    \"\"\"\n",
    "    q1, q2: Q-values for dataset actions (batch_size, 1)\n",
    "    \"\"\"\n",
    "    # 1. Sample additional actions from current policy\n",
    "    states = ...  # From batch\n",
    "    with torch.no_grad():\n",
    "        policy_actions = model.actor(states)  # (batch_size, 2)\n",
    "    \n",
    "    # 2. Create action candidates (dataset actions + policy actions)\n",
    "    all_actions = torch.cat([dataset_actions, policy_actions], dim=0)\n",
    "    \n",
    "    # 3. Compute Q-values for all actions\n",
    "    q1_all = model.q1(torch.cat([states.expand(num_action_samples, *states.shape), all_actions], dim=-1))\n",
    "    q2_all = model.q2(...)  # Same pattern\n",
    "    \n",
    "    # 4. Compute proper logsumexp\n",
    "    logsumexp_val = torch.logsumexp(0.5*(q1_all + q2_all), dim=0)  # More stable\n",
    "    \n",
    "    # 5. Final penalty\n",
    "    cql_penalty = logsumexp_val - 0.5*(q1.mean() + q2.mean())\n",
    "    \n",
    "    return cql_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose \n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 1.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/559-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # üß™ Check raw inputs\n",
    "        debug_tensor(states, \"states\")\n",
    "        debug_tensor(actions, \"actions\")\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # Critic Loss (CQL + TD error)\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            # Use TARGET critics for Q_next\n",
    "            debug_tensor(next_actions, \"next_actions\")\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = torch.clamp(target_q, min=-200, max=0)\n",
    "\n",
    "        current_q1 = model.q1(torch.cat([states, actions], dim=1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], dim=1))\n",
    "\n",
    "        debug_tensor(current_q1, \"current_q1\")\n",
    "        debug_tensor(current_q2, \"current_q2\")\n",
    "\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "\n",
    "        # -----------------------------\n",
    "        # CQL Penalty\n",
    "        # -----------------------------\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = torch.clamp(q_rand, min=-100, max=100)\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - (current_q1.mean() + current_q2.mean()) / 2\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        debug_tensor(critic_loss, \"critic_loss\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Actor Loss\n",
    "        # -----------------------------\n",
    "        states_actor = states.clone().detach()# prevent in-place gradient conflict\n",
    "        pred_actions = model.actor(states_actor)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor outputs! Fix this before training.\")\n",
    "            pred_actions = torch.nan_to_num(pred_actions.clone())  # ‚úÖ clone before modifying\n",
    "        \"\"\"\n",
    "        \n",
    "        # üîí Safe skip if NaNs\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        debug_tensor(pred_actions, \"pred_actions\")\n",
    "        \n",
    "\n",
    "        sa = torch.cat([states_actor, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa)\n",
    "        q2_pred = model.q2(sa)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    " \n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        debug_tensor(actor_loss, \"actor_loss\")\n",
    "\n",
    "        if torch.isnan(actor_loss).any():\n",
    "            print(\"‚ùå Skipping batch due to NaN in actor loss.\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # After critic and actor updates:\n",
    "        model.update_targets()  # Add this line\n",
    "\n",
    "        # -----------------------------\n",
    "        # Print Stats\n",
    "        # -----------------------------\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(f\"Current Q1: min={current_q1.min().item():.2f}, max={current_q1.max().item():.2f}, mean={current_q1.mean().item():.2f}\")\n",
    "            print(f\"Current Q2: min={current_q2.min().item():.2f}, max={current_q2.max().item():.2f}, mean={current_q2.mean().item():.2f}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Move batch data to device\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. Compute target Q for Critic\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            # clamp is fine, but do it as a separate (non-inplace) assignment\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. Critic forward pass\n",
    "        # -----------------------------\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # 2a. TD Loss (MSE between current Q and target Q)\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 2b. CQL penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = q_rand.clamp(-100, 100)  # not in-place\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4. Actor forward pass\n",
    "        # -----------------------------\n",
    "        # For actor training, we want the Q-value for the action predicted by the current actor\n",
    "        pred_actions = model.actor(states)  # no .clone() or .detach()\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa_actor)\n",
    "        q2_pred = model.q2(sa_actor)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Soft-update target networks\n",
    "        # -----------------------------\n",
    "        model.update_targets()\n",
    "\n",
    "        # Print stats\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [41:50<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging tools\n",
    "writer = SummaryWriter()\n",
    "csv_file = 'training_stats.csv'\n",
    "\n",
    "# Write CSV header\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['Epoch', 'Iteration', 'TD Loss', 'CQL Penalty', \n",
    "                        'Critic Loss', 'Actor Loss', 'Q1 Value', 'Q2 Value'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1000), desc=\"Training Progress\"):\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'td': 0.0,\n",
    "        'cql': 0.0,\n",
    "        'critic': 0.0,\n",
    "        'actor': 0.0,\n",
    "        'q1': 0.0,\n",
    "        'q2': 0.0,\n",
    "        'count': 0\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # --- Existing training code ---\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # --- Critic calculations ---\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # TD Loss\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # CQL Penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1).clamp(-100, 100)\n",
    "        logsumexp_val = torch.logsumexp(q_rand, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # Critic update\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # --- Actor calculations ---\n",
    "        pred_actions = model.actor(states)\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs detected, skipping actor update\")\n",
    "            q1_pred = torch.tensor(0.0)  # Default values\n",
    "            q2_pred = torch.tensor(0.0)\n",
    "            actor_loss = torch.tensor(0.0)\n",
    "        else:\n",
    "            sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "            q1_pred = model.q1(sa_actor)\n",
    "            q2_pred = model.q2(sa_actor)\n",
    "            actor_loss = -torch.min(q1_pred, q2_pred).mean()\n",
    "\n",
    "            # Actor update\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "        # Target network updates\n",
    "        model.update_targets()\n",
    "\n",
    "        # --- Metrics collection ---\n",
    "        metrics['td'] += td_loss.item()\n",
    "        metrics['cql'] += cql_penalty.item()\n",
    "        metrics['critic'] += critic_loss.item()\n",
    "        metrics['actor'] += actor_loss.item() if not torch.isnan(pred_actions).any() else 0\n",
    "        metrics['q1'] += q1_pred.mean().item()\n",
    "        metrics['q2'] += q2_pred.mean().item()\n",
    "        metrics['count'] += 1\n",
    "\n",
    "        # --- Logging ---\n",
    "        if i % print_interval == 0 and metrics['count'] > 0:\n",
    "            # Calculate averages\n",
    "            avg_td = metrics['td'] / metrics['count']\n",
    "            avg_cql = metrics['cql'] / metrics['count']\n",
    "            avg_critic = metrics['critic'] / metrics['count']\n",
    "            avg_actor = metrics['actor'] / metrics['count']\n",
    "            avg_q1 = metrics['q1'] / metrics['count']\n",
    "            avg_q2 = metrics['q2'] / metrics['count']\n",
    "\n",
    "            # TensorBoard logging\n",
    "            global_step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/TD', avg_td, global_step)\n",
    "            writer.add_scalar('Loss/CQL', avg_cql, global_step)\n",
    "            writer.add_scalar('Loss/Critic', avg_critic, global_step)\n",
    "            writer.add_scalar('Loss/Actor', avg_actor, global_step)\n",
    "            writer.add_scalar('Q_Values/Q1', avg_q1, global_step)\n",
    "            writer.add_scalar('Q_Values/Q2', avg_q2, global_step)\n",
    "\n",
    "            # CSV logging\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, i, avg_td, avg_cql, avg_critic,\n",
    "                                    avg_actor, avg_q1, avg_q2])\n",
    "\n",
    "            # Reset metrics\n",
    "            metrics = {k: 0.0 for k in metrics}\n",
    "            metrics['count'] = 0\n",
    "\n",
    "# Visualization code remains the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (3224537314.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtensorboard --logdir=runs\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "¬∑tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
