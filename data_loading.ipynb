{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm  # For progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load and clean CSV data\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df.ffill().bfill()\n",
    "\n",
    "        # Extract state features (8 dimensions)\n",
    "        self.states = self.df[[\n",
    "            \"glu\", \"glu_d\", \"glu_t\",\n",
    "            \"hr\", \"hr_d\", \"hr_t\",\n",
    "            \"iob\", \"hour\"\n",
    "        ]].values.astype(np.float32)\n",
    "\n",
    "        # Extract action features (2 dimensions)\n",
    "        self.actions = self.df[[\"basal\", \"bolus\"]].values.astype(np.float32)\n",
    "\n",
    "        # Extract done flags\n",
    "        self.dones = self.df[\"done\"].values.astype(np.float32)\n",
    "\n",
    "        # Compute rewards based on glu_raw at t+1\n",
    "        glucose_next_tensor = torch.tensor(self.df[\"glu_raw\"].values, dtype=torch.float32)\n",
    "        self.rewards = compute_reward_torch(glucose_next_tensor) / 15.0  # Normalize if needed\n",
    "\n",
    "        # Compute next_states using vectorized roll\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "\n",
    "        # Prevent transitions across episode boundaries\n",
    "        self.next_states[self.dones == 1] = self.states[self.dones == 1]\n",
    "\n",
    "        # Slice to make all arrays align: remove last step (no next state), and align reward with t\n",
    "\n",
    "        self.states      = self.states[:-2]\n",
    "        self.actions     = self.actions[:-2]\n",
    "        self.rewards     = self.rewards[1:-1]\n",
    "        self.next_states = self.next_states[:-2]\n",
    "        self.dones       = self.dones[:-2]\n",
    "        self.dones       = torch.tensor(self.dones, dtype=torch.float32)\n",
    "\n",
    "        # Sanity check\n",
    "        L = len(self.states)\n",
    "        assert all(len(arr) == L for arr in [self.actions, self.rewards, self.next_states, self.dones]), \\\n",
    "            f\"Inconsistent lengths in dataset components: {L}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"state\":      torch.from_numpy(self.states[idx]).float(),\n",
    "            \"action\":     torch.from_numpy(self.actions[idx]).float(),\n",
    "            \"reward\":     self.rewards[idx].float(),\n",
    "            \"next_state\": torch.from_numpy(self.next_states[idx]).float(),\n",
    "            \"done\":       self.dones[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_torch(glucose_next):\n",
    "    \"\"\"\n",
    "    Compute RI-based reward in PyTorch.\n",
    "    \"\"\"\n",
    "    glucose_next = torch.clamp(glucose_next, min=1e-6)\n",
    "    log_term = torch.log(glucose_next) ** 1.084\n",
    "    f = 1.509 * (log_term - 5.381)\n",
    "    ri = 10 * f ** 2\n",
    "\n",
    "    reward = -torch.clamp(ri / 100.0, 0, 1)\n",
    "    reward[glucose_next <= 39.0] = -15.0\n",
    "    return reward\n",
    "\n",
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 1.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/559-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8526,  0.0000])\n",
      "Action: tensor([-1.0147, -0.0992])\n",
      "Reward: tensor(-0.0003)\n",
      "Next State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8258,  0.0000])\n",
      "Done: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(\"State:\", sample[\"state\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"reward\"])\n",
    "print(\"Next State:\", sample[\"next_state\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.q1_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ) # Same as q1\n",
    "        self.q2_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )  # Same as q2\n",
    "        \n",
    "        # Initialize targets to match main critics\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update_targets(self, tau=0.005):\n",
    "        # Soft update: target = tau * main + (1-tau) * target\n",
    "        with torch.no_grad():\n",
    "            for t, m in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "            for t, m in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100\n",
    "\n",
    "def debug_tensor(tensor, name=\"\", check_grad=False, threshold=1e6):\n",
    "    \"\"\"\n",
    "    Prints diagnostic information about a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor to check.\n",
    "        name (str): Optional name for logging.\n",
    "        check_grad (bool): Also check gradients if available.\n",
    "        threshold (float): Warn if values exceed this.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t_min = tensor.min().item()\n",
    "        t_max = tensor.max().item()\n",
    "        t_mean = tensor.mean().item()\n",
    "        t_std = tensor.std().item()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not extract stats for {name}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß™ [{name}] Shape: {tuple(tensor.shape)} | min: {t_min:.4f}, max: {t_max:.4f}, mean: {t_mean:.4f}, std: {t_std:.4f}\")\n",
    "\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"‚ùå NaNs detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"‚ùå Infs detected in {name}\")\n",
    "    if abs(t_min) > threshold or abs(t_max) > threshold:\n",
    "        print(f\"‚ö†Ô∏è Extreme values detected in {name}: values exceed ¬±{threshold}\")\n",
    "\n",
    "    if check_grad and tensor.requires_grad and tensor.grad is not None:\n",
    "        grad = tensor.grad\n",
    "        print(f\"üîÅ [{name}.grad] norm: {grad.norm().item():.4f}\")\n",
    "        if torch.isnan(grad).any():\n",
    "            print(f\"‚ùå NaNs in gradient of {name}\")\n",
    "        if torch.isinf(grad).any():\n",
    "            print(f\"‚ùå Infs in gradient of {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # üß™ Check raw inputs\n",
    "        debug_tensor(states, \"states\")\n",
    "        debug_tensor(actions, \"actions\")\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # Critic Loss (CQL + TD error)\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            # Use TARGET critics for Q_next\n",
    "            debug_tensor(next_actions, \"next_actions\")\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = torch.clamp(target_q, min=-200, max=0)\n",
    "\n",
    "        current_q1 = model.q1(torch.cat([states, actions], dim=1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], dim=1))\n",
    "\n",
    "        debug_tensor(current_q1, \"current_q1\")\n",
    "        debug_tensor(current_q2, \"current_q2\")\n",
    "\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "\n",
    "        # -----------------------------\n",
    "        # CQL Penalty\n",
    "        # -----------------------------\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = torch.clamp(q_rand, min=-100, max=100)\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - (current_q1.mean() + current_q2.mean()) / 2\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        debug_tensor(critic_loss, \"critic_loss\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Actor Loss\n",
    "        # -----------------------------\n",
    "        states_actor = states.clone().detach()# prevent in-place gradient conflict\n",
    "        pred_actions = model.actor(states_actor)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor outputs! Fix this before training.\")\n",
    "            pred_actions = torch.nan_to_num(pred_actions.clone())  # ‚úÖ clone before modifying\n",
    "        \"\"\"\n",
    "        \n",
    "        # üîí Safe skip if NaNs\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"‚ùå NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        debug_tensor(pred_actions, \"pred_actions\")\n",
    "        \n",
    "\n",
    "        sa = torch.cat([states_actor, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa)\n",
    "        q2_pred = model.q2(sa)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    " \n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        debug_tensor(actor_loss, \"actor_loss\")\n",
    "\n",
    "        if torch.isnan(actor_loss).any():\n",
    "            print(\"‚ùå Skipping batch due to NaN in actor loss.\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # After critic and actor updates:\n",
    "        model.update_targets()  # Add this line\n",
    "\n",
    "        # -----------------------------\n",
    "        # Print Stats\n",
    "        # -----------------------------\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(f\"Current Q1: min={current_q1.min().item():.2f}, max={current_q1.max().item():.2f}, mean={current_q1.mean().item():.2f}\")\n",
    "            print(f\"Current Q2: min={current_q2.min().item():.2f}, max={current_q2.max().item():.2f}, mean={current_q2.mean().item():.2f}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "TD Loss: 0.0539, CQL Penalty: 0.6985\n",
      "Critic Loss: 3.5464, Actor Loss: 0.0859\n",
      "--------------------------------------------------\n",
      "Epoch: 1, Iteration: 0\n",
      "TD Loss: 0.1861, CQL Penalty: 0.4708\n",
      "Critic Loss: 2.5403, Actor Loss: 0.0476\n",
      "--------------------------------------------------\n",
      "Epoch: 2, Iteration: 0\n",
      "TD Loss: 0.8476, CQL Penalty: -0.0055\n",
      "Critic Loss: 0.8201, Actor Loss: -0.2196\n",
      "--------------------------------------------------\n",
      "Epoch: 3, Iteration: 0\n",
      "TD Loss: 1.5081, CQL Penalty: -0.8880\n",
      "Critic Loss: -2.9319, Actor Loss: -0.4056\n",
      "--------------------------------------------------\n",
      "Epoch: 4, Iteration: 0\n",
      "TD Loss: 1.8210, CQL Penalty: -2.4938\n",
      "Critic Loss: -10.6482, Actor Loss: -0.4493\n",
      "--------------------------------------------------\n",
      "Epoch: 5, Iteration: 0\n",
      "TD Loss: 1.8667, CQL Penalty: -5.2109\n",
      "Critic Loss: -24.1879, Actor Loss: -0.4857\n",
      "--------------------------------------------------\n",
      "Epoch: 6, Iteration: 0\n",
      "TD Loss: 4.0336, CQL Penalty: -8.2716\n",
      "Critic Loss: -37.3243, Actor Loss: -0.4459\n",
      "--------------------------------------------------\n",
      "Epoch: 7, Iteration: 0\n",
      "TD Loss: 4.3876, CQL Penalty: -12.5658\n",
      "Critic Loss: -58.4416, Actor Loss: -0.2112\n",
      "--------------------------------------------------\n",
      "Epoch: 8, Iteration: 0\n",
      "TD Loss: 3.8626, CQL Penalty: -18.2189\n",
      "Critic Loss: -87.2317, Actor Loss: -0.7384\n",
      "--------------------------------------------------\n",
      "Epoch: 9, Iteration: 0\n",
      "TD Loss: 15.4673, CQL Penalty: -27.4431\n",
      "Critic Loss: -121.7483, Actor Loss: -0.7707\n",
      "--------------------------------------------------\n",
      "Epoch: 10, Iteration: 0\n",
      "TD Loss: 3.3490, CQL Penalty: -37.6893\n",
      "Critic Loss: -185.0974, Actor Loss: -0.1440\n",
      "--------------------------------------------------\n",
      "Epoch: 11, Iteration: 0\n",
      "TD Loss: 3.5834, CQL Penalty: -42.8974\n",
      "Critic Loss: -210.9034, Actor Loss: -0.7652\n",
      "--------------------------------------------------\n",
      "Epoch: 12, Iteration: 0\n",
      "TD Loss: 4.5982, CQL Penalty: -49.9382\n",
      "Critic Loss: -245.0931, Actor Loss: -1.1892\n",
      "--------------------------------------------------\n",
      "Epoch: 13, Iteration: 0\n",
      "TD Loss: 20.1691, CQL Penalty: -56.5176\n",
      "Critic Loss: -262.4190, Actor Loss: -0.7764\n",
      "--------------------------------------------------\n",
      "Epoch: 14, Iteration: 0\n",
      "TD Loss: 71.3213, CQL Penalty: -62.7748\n",
      "Critic Loss: -242.5525, Actor Loss: -1.0690\n",
      "--------------------------------------------------\n",
      "Epoch: 15, Iteration: 0\n",
      "TD Loss: 63.6521, CQL Penalty: -69.7586\n",
      "Critic Loss: -285.1408, Actor Loss: -0.1042\n",
      "--------------------------------------------------\n",
      "Epoch: 16, Iteration: 0\n",
      "TD Loss: 7.3937, CQL Penalty: -71.9175\n",
      "Critic Loss: -352.1940, Actor Loss: -0.8945\n",
      "--------------------------------------------------\n",
      "Epoch: 17, Iteration: 0\n",
      "TD Loss: 27.4332, CQL Penalty: -71.8195\n",
      "Critic Loss: -331.6646, Actor Loss: -0.3150\n",
      "--------------------------------------------------\n",
      "Epoch: 18, Iteration: 0\n",
      "TD Loss: 116.7823, CQL Penalty: -74.2509\n",
      "Critic Loss: -254.4720, Actor Loss: 0.3316\n",
      "--------------------------------------------------\n",
      "Epoch: 19, Iteration: 0\n",
      "TD Loss: 8.7376, CQL Penalty: -77.0018\n",
      "Critic Loss: -376.2712, Actor Loss: -1.4918\n",
      "--------------------------------------------------\n",
      "Epoch: 20, Iteration: 0\n",
      "TD Loss: 52.4897, CQL Penalty: -80.0459\n",
      "Critic Loss: -347.7397, Actor Loss: -0.9596\n",
      "--------------------------------------------------\n",
      "Epoch: 21, Iteration: 0\n",
      "TD Loss: 3.7588, CQL Penalty: -81.3127\n",
      "Critic Loss: -402.8047, Actor Loss: -0.9855\n",
      "--------------------------------------------------\n",
      "Epoch: 22, Iteration: 0\n",
      "TD Loss: 7.3879, CQL Penalty: -79.8671\n",
      "Critic Loss: -391.9475, Actor Loss: -0.9260\n",
      "--------------------------------------------------\n",
      "Epoch: 23, Iteration: 0\n",
      "TD Loss: 158.2682, CQL Penalty: -77.6447\n",
      "Critic Loss: -229.9552, Actor Loss: -1.1811\n",
      "--------------------------------------------------\n",
      "Epoch: 24, Iteration: 0\n",
      "TD Loss: 5.5031, CQL Penalty: -86.3410\n",
      "Critic Loss: -426.2022, Actor Loss: -1.6410\n",
      "--------------------------------------------------\n",
      "Epoch: 25, Iteration: 0\n",
      "TD Loss: 7.3589, CQL Penalty: -84.4713\n",
      "Critic Loss: -414.9978, Actor Loss: 0.3990\n",
      "--------------------------------------------------\n",
      "Epoch: 26, Iteration: 0\n",
      "TD Loss: 26.3104, CQL Penalty: -85.8996\n",
      "Critic Loss: -403.1873, Actor Loss: -0.7390\n",
      "--------------------------------------------------\n",
      "Epoch: 27, Iteration: 0\n",
      "TD Loss: 9.3070, CQL Penalty: -86.6047\n",
      "Critic Loss: -423.7166, Actor Loss: -0.9029\n",
      "--------------------------------------------------\n",
      "Epoch: 28, Iteration: 0\n",
      "TD Loss: 6.6213, CQL Penalty: -86.1150\n",
      "Critic Loss: -423.9539, Actor Loss: -0.8144\n",
      "--------------------------------------------------\n",
      "Epoch: 29, Iteration: 0\n",
      "TD Loss: 7.1725, CQL Penalty: -89.5774\n",
      "Critic Loss: -440.7144, Actor Loss: -0.1744\n",
      "--------------------------------------------------\n",
      "Epoch: 30, Iteration: 0\n",
      "TD Loss: 14.0029, CQL Penalty: -86.3506\n",
      "Critic Loss: -417.7501, Actor Loss: -0.6807\n",
      "--------------------------------------------------\n",
      "Epoch: 31, Iteration: 0\n",
      "TD Loss: 15.0238, CQL Penalty: -85.6465\n",
      "Critic Loss: -413.2087, Actor Loss: -0.6505\n",
      "--------------------------------------------------\n",
      "Epoch: 32, Iteration: 0\n",
      "TD Loss: 7.3516, CQL Penalty: -86.8440\n",
      "Critic Loss: -426.8683, Actor Loss: -0.4948\n",
      "--------------------------------------------------\n",
      "Epoch: 33, Iteration: 0\n",
      "TD Loss: 3.5719, CQL Penalty: -86.8554\n",
      "Critic Loss: -430.7052, Actor Loss: -0.3352\n",
      "--------------------------------------------------\n",
      "Epoch: 34, Iteration: 0\n",
      "TD Loss: 1.9189, CQL Penalty: -87.0914\n",
      "Critic Loss: -433.5381, Actor Loss: 0.4294\n",
      "--------------------------------------------------\n",
      "Epoch: 35, Iteration: 0\n",
      "TD Loss: 65.7402, CQL Penalty: -87.3095\n",
      "Critic Loss: -370.8071, Actor Loss: -0.9104\n",
      "--------------------------------------------------\n",
      "Epoch: 36, Iteration: 0\n",
      "TD Loss: 4.0441, CQL Penalty: -89.0637\n",
      "Critic Loss: -441.2744, Actor Loss: -0.9583\n",
      "--------------------------------------------------\n",
      "Epoch: 37, Iteration: 0\n",
      "TD Loss: 31.7430, CQL Penalty: -87.8988\n",
      "Critic Loss: -407.7510, Actor Loss: -0.6390\n",
      "--------------------------------------------------\n",
      "Epoch: 38, Iteration: 0\n",
      "TD Loss: 8.4128, CQL Penalty: -89.9341\n",
      "Critic Loss: -441.2578, Actor Loss: -0.4797\n",
      "--------------------------------------------------\n",
      "Epoch: 39, Iteration: 0\n",
      "TD Loss: 5.6477, CQL Penalty: -88.1849\n",
      "Critic Loss: -435.2766, Actor Loss: 0.6661\n",
      "--------------------------------------------------\n",
      "Epoch: 40, Iteration: 0\n",
      "TD Loss: 8.0979, CQL Penalty: -91.4167\n",
      "Critic Loss: -448.9856, Actor Loss: 0.0316\n",
      "--------------------------------------------------\n",
      "Epoch: 41, Iteration: 0\n",
      "TD Loss: 9.7778, CQL Penalty: -87.0973\n",
      "Critic Loss: -425.7086, Actor Loss: -0.9244\n",
      "--------------------------------------------------\n",
      "Epoch: 42, Iteration: 0\n",
      "TD Loss: 12.6831, CQL Penalty: -88.0651\n",
      "Critic Loss: -427.6424, Actor Loss: -0.7147\n",
      "--------------------------------------------------\n",
      "Epoch: 43, Iteration: 0\n",
      "TD Loss: 7.3790, CQL Penalty: -90.2288\n",
      "Critic Loss: -443.7652, Actor Loss: 1.8651\n",
      "--------------------------------------------------\n",
      "Epoch: 44, Iteration: 0\n",
      "TD Loss: 10.2307, CQL Penalty: -90.4686\n",
      "Critic Loss: -442.1125, Actor Loss: -0.7894\n",
      "--------------------------------------------------\n",
      "Epoch: 45, Iteration: 0\n",
      "TD Loss: 9.1324, CQL Penalty: -87.9301\n",
      "Critic Loss: -430.5179, Actor Loss: -0.2781\n",
      "--------------------------------------------------\n",
      "Epoch: 46, Iteration: 0\n",
      "TD Loss: 3.8668, CQL Penalty: -90.4097\n",
      "Critic Loss: -448.1819, Actor Loss: 0.2374\n",
      "--------------------------------------------------\n",
      "Epoch: 47, Iteration: 0\n",
      "TD Loss: 11.0835, CQL Penalty: -89.7009\n",
      "Critic Loss: -437.4211, Actor Loss: -0.5981\n",
      "--------------------------------------------------\n",
      "Epoch: 48, Iteration: 0\n",
      "TD Loss: 4.4957, CQL Penalty: -91.1655\n",
      "Critic Loss: -451.3320, Actor Loss: -0.5425\n",
      "--------------------------------------------------\n",
      "Epoch: 49, Iteration: 0\n",
      "TD Loss: 74.2594, CQL Penalty: -87.8000\n",
      "Critic Loss: -364.7404, Actor Loss: 0.2221\n",
      "--------------------------------------------------\n",
      "Epoch: 50, Iteration: 0\n",
      "TD Loss: 5.8327, CQL Penalty: -89.2231\n",
      "Critic Loss: -440.2830, Actor Loss: 0.0822\n",
      "--------------------------------------------------\n",
      "Epoch: 51, Iteration: 0\n",
      "TD Loss: 8.9592, CQL Penalty: -92.5801\n",
      "Critic Loss: -453.9412, Actor Loss: -0.1538\n",
      "--------------------------------------------------\n",
      "Epoch: 52, Iteration: 0\n",
      "TD Loss: 69.0126, CQL Penalty: -89.9633\n",
      "Critic Loss: -380.8037, Actor Loss: 1.0815\n",
      "--------------------------------------------------\n",
      "Epoch: 53, Iteration: 0\n",
      "TD Loss: 4.9614, CQL Penalty: -89.3286\n",
      "Critic Loss: -441.6815, Actor Loss: -0.3913\n",
      "--------------------------------------------------\n",
      "Epoch: 54, Iteration: 0\n",
      "TD Loss: 9.2811, CQL Penalty: -90.2238\n",
      "Critic Loss: -441.8380, Actor Loss: -1.0806\n",
      "--------------------------------------------------\n",
      "Epoch: 55, Iteration: 0\n",
      "TD Loss: 6.1804, CQL Penalty: -92.9353\n",
      "Critic Loss: -458.4959, Actor Loss: -1.1860\n",
      "--------------------------------------------------\n",
      "Epoch: 56, Iteration: 0\n",
      "TD Loss: 2.7190, CQL Penalty: -91.6648\n",
      "Critic Loss: -455.6052, Actor Loss: 0.0865\n",
      "--------------------------------------------------\n",
      "Epoch: 57, Iteration: 0\n",
      "TD Loss: 14.0690, CQL Penalty: -87.7398\n",
      "Critic Loss: -424.6301, Actor Loss: -0.5912\n",
      "--------------------------------------------------\n",
      "Epoch: 58, Iteration: 0\n",
      "TD Loss: 4.4535, CQL Penalty: -92.8613\n",
      "Critic Loss: -459.8531, Actor Loss: -0.0771\n",
      "--------------------------------------------------\n",
      "Epoch: 59, Iteration: 0\n",
      "TD Loss: 5.8898, CQL Penalty: -91.5669\n",
      "Critic Loss: -451.9449, Actor Loss: 1.0108\n",
      "--------------------------------------------------\n",
      "Epoch: 60, Iteration: 0\n",
      "TD Loss: 100.9916, CQL Penalty: -91.5576\n",
      "Critic Loss: -356.7964, Actor Loss: -1.0962\n",
      "--------------------------------------------------\n",
      "Epoch: 61, Iteration: 0\n",
      "TD Loss: 9.7935, CQL Penalty: -90.6392\n",
      "Critic Loss: -443.4026, Actor Loss: 0.3253\n",
      "--------------------------------------------------\n",
      "Epoch: 62, Iteration: 0\n",
      "TD Loss: 106.7436, CQL Penalty: -90.4906\n",
      "Critic Loss: -345.7096, Actor Loss: -0.5249\n",
      "--------------------------------------------------\n",
      "Epoch: 63, Iteration: 0\n",
      "TD Loss: 24.4828, CQL Penalty: -92.0714\n",
      "Critic Loss: -435.8741, Actor Loss: 1.1062\n",
      "--------------------------------------------------\n",
      "Epoch: 64, Iteration: 0\n",
      "TD Loss: 27.3512, CQL Penalty: -89.8143\n",
      "Critic Loss: -421.7204, Actor Loss: 1.6143\n",
      "--------------------------------------------------\n",
      "Epoch: 65, Iteration: 0\n",
      "TD Loss: 17.3667, CQL Penalty: -87.7835\n",
      "Critic Loss: -421.5507, Actor Loss: 0.7436\n",
      "--------------------------------------------------\n",
      "Epoch: 66, Iteration: 0\n",
      "TD Loss: 2140.4600, CQL Penalty: 1.8807\n",
      "Critic Loss: 2149.8633, Actor Loss: 51.6947\n",
      "--------------------------------------------------\n",
      "Epoch: 67, Iteration: 0\n",
      "TD Loss: 903.1804, CQL Penalty: 66.4473\n",
      "Critic Loss: 1235.4167, Actor Loss: 99.6903\n",
      "--------------------------------------------------\n",
      "Epoch: 68, Iteration: 0\n",
      "TD Loss: 1344.2435, CQL Penalty: 43.1294\n",
      "Critic Loss: 1559.8905, Actor Loss: 61.4220\n",
      "--------------------------------------------------\n",
      "Epoch: 69, Iteration: 0\n",
      "TD Loss: 2678.7402, CQL Penalty: -1.3331\n",
      "Critic Loss: 2672.0750, Actor Loss: 44.0031\n",
      "--------------------------------------------------\n",
      "Epoch: 70, Iteration: 0\n",
      "TD Loss: 344.9408, CQL Penalty: -12.8205\n",
      "Critic Loss: 280.8382, Actor Loss: 82.9053\n",
      "--------------------------------------------------\n",
      "Epoch: 71, Iteration: 0\n",
      "TD Loss: 238.4877, CQL Penalty: -28.7423\n",
      "Critic Loss: 94.7763, Actor Loss: 66.5703\n",
      "--------------------------------------------------\n",
      "Epoch: 72, Iteration: 0\n",
      "TD Loss: 157.7411, CQL Penalty: -30.5588\n",
      "Critic Loss: 4.9469, Actor Loss: 63.5253\n",
      "--------------------------------------------------\n",
      "Epoch: 73, Iteration: 0\n",
      "TD Loss: 161.3449, CQL Penalty: -34.6848\n",
      "Critic Loss: -12.0790, Actor Loss: 59.9850\n",
      "--------------------------------------------------\n",
      "Epoch: 74, Iteration: 0\n",
      "TD Loss: 81.7217, CQL Penalty: -36.6141\n",
      "Critic Loss: -101.3486, Actor Loss: 58.1977\n",
      "--------------------------------------------------\n",
      "Epoch: 75, Iteration: 0\n",
      "TD Loss: 74.7155, CQL Penalty: -35.8692\n",
      "Critic Loss: -104.6306, Actor Loss: 58.9670\n",
      "--------------------------------------------------\n",
      "Epoch: 76, Iteration: 0\n",
      "TD Loss: 61.7649, CQL Penalty: -38.1624\n",
      "Critic Loss: -129.0470, Actor Loss: 56.9695\n",
      "--------------------------------------------------\n",
      "Epoch: 77, Iteration: 0\n",
      "TD Loss: 74.4876, CQL Penalty: -36.0393\n",
      "Critic Loss: -105.7089, Actor Loss: 58.7103\n",
      "--------------------------------------------------\n",
      "Epoch: 78, Iteration: 0\n",
      "TD Loss: 48.6629, CQL Penalty: -37.5272\n",
      "Critic Loss: -138.9731, Actor Loss: 57.6645\n",
      "--------------------------------------------------\n",
      "Epoch: 79, Iteration: 0\n",
      "TD Loss: 115.9401, CQL Penalty: -36.7358\n",
      "Critic Loss: -67.7390, Actor Loss: 58.1148\n",
      "--------------------------------------------------\n",
      "Epoch: 80, Iteration: 0\n",
      "TD Loss: 46.5054, CQL Penalty: -39.1216\n",
      "Critic Loss: -149.1028, Actor Loss: 55.5044\n",
      "--------------------------------------------------\n",
      "Epoch: 81, Iteration: 0\n",
      "TD Loss: 63.9373, CQL Penalty: -40.3544\n",
      "Critic Loss: -137.8349, Actor Loss: 55.2484\n",
      "--------------------------------------------------\n",
      "Epoch: 82, Iteration: 0\n",
      "TD Loss: 78.5239, CQL Penalty: -38.1392\n",
      "Critic Loss: -112.1720, Actor Loss: 55.5511\n",
      "--------------------------------------------------\n",
      "Epoch: 83, Iteration: 0\n",
      "TD Loss: 41.5972, CQL Penalty: -39.2306\n",
      "Critic Loss: -154.5557, Actor Loss: 54.6727\n",
      "--------------------------------------------------\n",
      "Epoch: 84, Iteration: 0\n",
      "TD Loss: 54.3786, CQL Penalty: -38.9416\n",
      "Critic Loss: -140.3292, Actor Loss: 53.7238\n",
      "--------------------------------------------------\n",
      "Epoch: 85, Iteration: 0\n",
      "TD Loss: 38.8893, CQL Penalty: -43.3989\n",
      "Critic Loss: -178.1051, Actor Loss: 52.0688\n",
      "--------------------------------------------------\n",
      "Epoch: 86, Iteration: 0\n",
      "TD Loss: 91.3885, CQL Penalty: -41.1671\n",
      "Critic Loss: -114.4469, Actor Loss: 53.2206\n",
      "--------------------------------------------------\n",
      "Epoch: 87, Iteration: 0\n",
      "TD Loss: 30.6811, CQL Penalty: -42.4021\n",
      "Critic Loss: -181.3294, Actor Loss: 53.0079\n",
      "--------------------------------------------------\n",
      "Epoch: 88, Iteration: 0\n",
      "TD Loss: 52.9433, CQL Penalty: -42.4241\n",
      "Critic Loss: -159.1771, Actor Loss: 52.8593\n",
      "--------------------------------------------------\n",
      "Epoch: 89, Iteration: 0\n",
      "TD Loss: 92.5753, CQL Penalty: -41.0965\n",
      "Critic Loss: -112.9073, Actor Loss: 52.7084\n",
      "--------------------------------------------------\n",
      "Epoch: 90, Iteration: 0\n",
      "TD Loss: 33.8775, CQL Penalty: -43.5634\n",
      "Critic Loss: -183.9393, Actor Loss: 50.7384\n",
      "--------------------------------------------------\n",
      "Epoch: 91, Iteration: 0\n",
      "TD Loss: 45.0438, CQL Penalty: -43.7970\n",
      "Critic Loss: -173.9412, Actor Loss: 50.9683\n",
      "--------------------------------------------------\n",
      "Epoch: 92, Iteration: 0\n",
      "TD Loss: 77.3445, CQL Penalty: -43.6660\n",
      "Critic Loss: -140.9857, Actor Loss: 50.6849\n",
      "--------------------------------------------------\n",
      "Epoch: 93, Iteration: 0\n",
      "TD Loss: 67.6304, CQL Penalty: -45.0327\n",
      "Critic Loss: -157.5332, Actor Loss: 48.2211\n",
      "--------------------------------------------------\n",
      "Epoch: 94, Iteration: 0\n",
      "TD Loss: 48.6179, CQL Penalty: -45.6113\n",
      "Critic Loss: -179.4384, Actor Loss: 48.8277\n",
      "--------------------------------------------------\n",
      "Epoch: 95, Iteration: 0\n",
      "TD Loss: 85.5314, CQL Penalty: -45.1889\n",
      "Critic Loss: -140.4133, Actor Loss: 49.7584\n",
      "--------------------------------------------------\n",
      "Epoch: 96, Iteration: 0\n",
      "TD Loss: 26.8505, CQL Penalty: -46.7725\n",
      "Critic Loss: -207.0120, Actor Loss: 47.5973\n",
      "--------------------------------------------------\n",
      "Epoch: 97, Iteration: 0\n",
      "TD Loss: 33.0266, CQL Penalty: -45.5448\n",
      "Critic Loss: -194.6972, Actor Loss: 50.1710\n",
      "--------------------------------------------------\n",
      "Epoch: 98, Iteration: 0\n",
      "TD Loss: 39.9245, CQL Penalty: -48.1994\n",
      "Critic Loss: -201.0726, Actor Loss: 46.5508\n",
      "--------------------------------------------------\n",
      "Epoch: 99, Iteration: 0\n",
      "TD Loss: 81.1662, CQL Penalty: -48.6038\n",
      "Critic Loss: -161.8528, Actor Loss: 46.2151\n",
      "--------------------------------------------------\n",
      "Epoch: 100, Iteration: 0\n",
      "TD Loss: 60.8934, CQL Penalty: -48.1221\n",
      "Critic Loss: -179.7169, Actor Loss: 47.4617\n",
      "--------------------------------------------------\n",
      "Epoch: 101, Iteration: 0\n",
      "TD Loss: 29.9252, CQL Penalty: -47.9686\n",
      "Critic Loss: -209.9176, Actor Loss: 46.9321\n",
      "--------------------------------------------------\n",
      "Epoch: 102, Iteration: 0\n",
      "TD Loss: 37.1884, CQL Penalty: -50.6819\n",
      "Critic Loss: -216.2210, Actor Loss: 44.5248\n",
      "--------------------------------------------------\n",
      "Epoch: 103, Iteration: 0\n",
      "TD Loss: 50.8516, CQL Penalty: -49.3311\n",
      "Critic Loss: -195.8040, Actor Loss: 46.4177\n",
      "--------------------------------------------------\n",
      "Epoch: 104, Iteration: 0\n",
      "TD Loss: 30.1785, CQL Penalty: -50.0586\n",
      "Critic Loss: -220.1143, Actor Loss: 44.5206\n",
      "--------------------------------------------------\n",
      "Epoch: 105, Iteration: 0\n",
      "TD Loss: 37.2563, CQL Penalty: -49.9936\n",
      "Critic Loss: -212.7119, Actor Loss: 44.7425\n",
      "--------------------------------------------------\n",
      "Epoch: 106, Iteration: 0\n",
      "TD Loss: 21.5437, CQL Penalty: -50.9874\n",
      "Critic Loss: -233.3931, Actor Loss: 43.0611\n",
      "--------------------------------------------------\n",
      "Epoch: 107, Iteration: 0\n",
      "TD Loss: 37.5992, CQL Penalty: -51.1717\n",
      "Critic Loss: -218.2591, Actor Loss: 43.0312\n",
      "--------------------------------------------------\n",
      "Epoch: 108, Iteration: 0\n",
      "TD Loss: 30.0731, CQL Penalty: -50.9230\n",
      "Critic Loss: -224.5421, Actor Loss: 42.7367\n",
      "--------------------------------------------------\n",
      "Epoch: 109, Iteration: 0\n",
      "TD Loss: 39.1143, CQL Penalty: -52.3589\n",
      "Critic Loss: -222.6803, Actor Loss: 42.5685\n",
      "--------------------------------------------------\n",
      "Epoch: 110, Iteration: 0\n",
      "TD Loss: 42.3878, CQL Penalty: -52.2194\n",
      "Critic Loss: -218.7095, Actor Loss: 41.4269\n",
      "--------------------------------------------------\n",
      "Epoch: 111, Iteration: 0\n",
      "TD Loss: 47.5896, CQL Penalty: -52.4623\n",
      "Critic Loss: -214.7221, Actor Loss: 40.7781\n",
      "--------------------------------------------------\n",
      "Epoch: 112, Iteration: 0\n",
      "TD Loss: 21.1411, CQL Penalty: -54.4080\n",
      "Critic Loss: -250.8987, Actor Loss: 40.7791\n",
      "--------------------------------------------------\n",
      "Epoch: 113, Iteration: 0\n",
      "TD Loss: 17.4884, CQL Penalty: -54.3007\n",
      "Critic Loss: -254.0153, Actor Loss: 40.8818\n",
      "--------------------------------------------------\n",
      "Epoch: 114, Iteration: 0\n",
      "TD Loss: 33.7934, CQL Penalty: -55.5758\n",
      "Critic Loss: -244.0855, Actor Loss: 40.0325\n",
      "--------------------------------------------------\n",
      "Epoch: 115, Iteration: 0\n",
      "TD Loss: 39.9721, CQL Penalty: -54.3691\n",
      "Critic Loss: -231.8733, Actor Loss: 39.2906\n",
      "--------------------------------------------------\n",
      "Epoch: 116, Iteration: 0\n",
      "TD Loss: 91.4771, CQL Penalty: -55.5139\n",
      "Critic Loss: -186.0924, Actor Loss: 39.5531\n",
      "--------------------------------------------------\n",
      "Epoch: 117, Iteration: 0\n",
      "TD Loss: 34.0543, CQL Penalty: -56.5438\n",
      "Critic Loss: -248.6647, Actor Loss: 39.2128\n",
      "--------------------------------------------------\n",
      "Epoch: 118, Iteration: 0\n",
      "TD Loss: 35.9645, CQL Penalty: -57.8740\n",
      "Critic Loss: -253.4056, Actor Loss: 37.6926\n",
      "--------------------------------------------------\n",
      "Epoch: 119, Iteration: 0\n",
      "TD Loss: 24.4021, CQL Penalty: -55.5244\n",
      "Critic Loss: -253.2199, Actor Loss: 39.0478\n",
      "--------------------------------------------------\n",
      "Epoch: 120, Iteration: 0\n",
      "TD Loss: 39.0251, CQL Penalty: -57.1196\n",
      "Critic Loss: -246.5729, Actor Loss: 37.2851\n",
      "--------------------------------------------------\n",
      "Epoch: 121, Iteration: 0\n",
      "TD Loss: 43.2510, CQL Penalty: -57.8090\n",
      "Critic Loss: -245.7941, Actor Loss: 38.2037\n",
      "--------------------------------------------------\n",
      "Epoch: 122, Iteration: 0\n",
      "TD Loss: 24.0565, CQL Penalty: -58.2774\n",
      "Critic Loss: -267.3307, Actor Loss: 37.3108\n",
      "--------------------------------------------------\n",
      "Epoch: 123, Iteration: 0\n",
      "TD Loss: 19.3914, CQL Penalty: -57.4397\n",
      "Critic Loss: -267.8071, Actor Loss: 36.5950\n",
      "--------------------------------------------------\n",
      "Epoch: 124, Iteration: 0\n",
      "TD Loss: 27.1164, CQL Penalty: -59.7403\n",
      "Critic Loss: -271.5849, Actor Loss: 36.3191\n",
      "--------------------------------------------------\n",
      "Epoch: 125, Iteration: 0\n",
      "TD Loss: 33.2514, CQL Penalty: -58.8819\n",
      "Critic Loss: -261.1578, Actor Loss: 36.1552\n",
      "--------------------------------------------------\n",
      "Epoch: 126, Iteration: 0\n",
      "TD Loss: 64.2964, CQL Penalty: -58.0306\n",
      "Critic Loss: -225.8568, Actor Loss: 35.2051\n",
      "--------------------------------------------------\n",
      "Epoch: 127, Iteration: 0\n",
      "TD Loss: 22.1550, CQL Penalty: -55.8044\n",
      "Critic Loss: -256.8669, Actor Loss: 36.2327\n",
      "--------------------------------------------------\n",
      "Epoch: 128, Iteration: 0\n",
      "TD Loss: 29.5207, CQL Penalty: -60.4481\n",
      "Critic Loss: -272.7198, Actor Loss: 35.7015\n",
      "--------------------------------------------------\n",
      "Epoch: 129, Iteration: 0\n",
      "TD Loss: 73.4576, CQL Penalty: -58.9948\n",
      "Critic Loss: -221.5164, Actor Loss: 34.8910\n",
      "--------------------------------------------------\n",
      "Epoch: 130, Iteration: 0\n",
      "TD Loss: 15.2181, CQL Penalty: -61.3401\n",
      "Critic Loss: -291.4824, Actor Loss: 32.8337\n",
      "--------------------------------------------------\n",
      "Epoch: 131, Iteration: 0\n",
      "TD Loss: 24.4143, CQL Penalty: -61.5784\n",
      "Critic Loss: -283.4775, Actor Loss: 33.1275\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move batch data to device\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mDiabetesDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m:      \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m:     torch.from_numpy(\u001b[38;5;28mself\u001b[39m.actions[idx]).float(),\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m\"\u001b[39m:     \u001b[38;5;28mself\u001b[39m.rewards[idx].float(),\n\u001b[32m     52\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnext_state\u001b[39m\u001b[33m\"\u001b[39m: torch.from_numpy(\u001b[38;5;28mself\u001b[39m.next_states[idx]).float(),\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m:       \u001b[38;5;28mself\u001b[39m.dones[idx]\n\u001b[32m     54\u001b[39m     }\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#WORKING VERSION\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Move batch data to device\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. Compute target Q for Critic\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            # clamp is fine, but do it as a separate (non-inplace) assignment\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. Critic forward pass\n",
    "        # -----------------------------\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # 2a. TD Loss (MSE between current Q and target Q)\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 2b. CQL penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = q_rand.clamp(-100, 100)  # not in-place\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4. Actor forward pass\n",
    "        # -----------------------------\n",
    "        # For actor training, we want the Q-value for the action predicted by the current actor\n",
    "        pred_actions = model.actor(states)  # no .clone() or .detach()\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa_actor)\n",
    "        q2_pred = model.q2(sa_actor)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Soft-update target networks\n",
    "        # -----------------------------\n",
    "        model.update_targets()\n",
    "\n",
    "        # Print stats\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'td_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     25\u001b[39m counter = \u001b[32m0\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# ... [existing training code remains the same until loss calculations] ...\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Accumulate metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     running_td += \u001b[43mtd_loss\u001b[49m.item()\n\u001b[32m     32\u001b[39m     running_cql += cql_penalty.item()\n\u001b[32m     33\u001b[39m     running_critic += critic_loss.item()\n",
      "\u001b[31mNameError\u001b[39m: name 'td_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# First, modify your existing training code to include logging\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize CSV file\n",
    "csv_file = 'training_stats.csv'\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['Epoch', 'Iteration', 'TD Loss', 'CQL Penalty', \n",
    "                        'Critic Loss', 'Actor Loss', 'Q1 Value', 'Q2 Value'])\n",
    "\n",
    "# Training loop with enhanced logging\n",
    "for epoch in tqdm(range(1000), desc=\"Training Progress\"):\n",
    "    # Initialize running averages for this epoch\n",
    "    running_td = 0.0\n",
    "    running_cql = 0.0\n",
    "    running_critic = 0.0\n",
    "    running_actor = 0.0\n",
    "    running_q1 = 0.0\n",
    "    running_q2 = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # ... [existing training code remains the same until loss calculations] ...\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_td += td_loss.item()\n",
    "        running_cql += cql_penalty.item()\n",
    "        running_critic += critic_loss.item()\n",
    "        running_actor += actor_loss.item()\n",
    "        running_q1 += q1_pred.mean().item()\n",
    "        running_q2 += q2_pred.mean().item()\n",
    "        counter += 1\n",
    "\n",
    "        # Log and print metrics at intervals\n",
    "        if i % print_interval == 0 and counter > 0:\n",
    "            # Calculate averages\n",
    "            avg_td = running_td / counter\n",
    "            avg_cql = running_cql / counter\n",
    "            avg_critic = running_critic / counter\n",
    "            avg_actor = running_actor / counter\n",
    "            avg_q1 = running_q1 / counter\n",
    "            avg_q2 = running_q2 / counter\n",
    "\n",
    "            # TensorBoard logging\n",
    "            global_step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/TD', avg_td, global_step)\n",
    "            writer.add_scalar('Loss/CQL', avg_cql, global_step)\n",
    "            writer.add_scalar('Loss/Critic', avg_critic, global_step)\n",
    "            writer.add_scalar('Loss/Actor', avg_actor, global_step)\n",
    "            writer.add_scalar('Q_Values/Q1', avg_q1, global_step)\n",
    "            writer.add_scalar('Q_Values/Q2', avg_q2, global_step)\n",
    "\n",
    "            # CSV logging\n",
    "            with open(csv_file, 'a', newline='') as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, i, avg_td, avg_cql, avg_critic,\n",
    "                                    avg_actor, avg_q1, avg_q2])\n",
    "\n",
    "            # Reset running averages\n",
    "            running_td = 0.0\n",
    "            running_cql = 0.0\n",
    "            running_critic = 0.0\n",
    "            running_actor = 0.0\n",
    "            running_q1 = 0.0\n",
    "            running_q2 = 0.0\n",
    "            counter = 0\n",
    "\n",
    "# After training completes, add visualization code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load logged data\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['TD Loss'], label='TD Loss')\n",
    "plt.plot(df['CQL Penalty'], label='CQL Penalty')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TD and CQL Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['Critic Loss'], label='Critic Loss')\n",
    "plt.plot(df['Actor Loss'], label='Actor Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Critic and Actor Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Q-Values\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['Q1 Value'], label='Q1 Value')\n",
    "plt.plot(df['Q2 Value'], label='Q2 Value')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Q-Value')\n",
    "plt.title('Q-Value Progression')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
