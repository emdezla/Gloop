{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load CSV data\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Extract state features: 8 dimensions\n",
    "        # [glu, glu_d, glu_t, hr, hr_d, hr_t, iob, hour_norm]\n",
    "        self.states = self.df[[\n",
    "            \"glu\", \"glu_d\", \"glu_t\",\n",
    "            \"hr\", \"hr_d\", \"hr_t\",\n",
    "            \"iob\", \"hour\"\n",
    "        ]].values.astype(np.float32)\n",
    "        \n",
    "        # Extract action features: 2 dimensions [basal, bol]\n",
    "        self.actions = self.df[[\"basal\", \"bolus\"]].values.astype(np.float32)\n",
    "        \n",
    "        # Extract done flags (1 at episode boundaries, 0 otherwise)\n",
    "        self.dones = self.df[\"done\"].values.astype(np.float32)\n",
    "        \n",
    "        # Define rewards (example: negative absolute deviation from target)\n",
    "        target_glucose = 0.0  # For normalized glucose, target might be 0 after scaling\n",
    "        self.rewards = -np.abs(self.states[:, 0] - target_glucose)\n",
    "        \n",
    "        # Compute next_states using a vectorized roll\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "        \n",
    "        # For transitions where the current step is an episode end,\n",
    "        # set the next state to be the current state so that transitions do not cross episodes.\n",
    "        self.next_states[self.dones == 1] = self.states[self.dones == 1]\n",
    "        \n",
    "        # Remove the final row since it doesn't have a valid next state\n",
    "        self.states = self.states[:-1]\n",
    "        self.actions = self.actions[:-1]\n",
    "        self.rewards = self.rewards[:-1]\n",
    "        self.next_states = self.next_states[:-1]\n",
    "        self.dones = self.dones[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a dictionary for the transition (s, a, r, s', done)\n",
    "        return {\n",
    "            \"state\":      torch.tensor(self.states[idx],      dtype=torch.float32),\n",
    "            \"action\":     torch.tensor(self.actions[idx],     dtype=torch.float32),\n",
    "            \"reward\":     torch.tensor(self.rewards[idx],     dtype=torch.float32),\n",
    "            \"next_state\": torch.tensor(self.next_states[idx], dtype=torch.float32),\n",
    "            \"done\":       torch.tensor(self.dones[idx],       dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([ 1.5537,     nan,     nan, -0.0855,     nan,     nan, -1.2715,  0.0000])\n",
      "Action: tensor([ 3.1836, -0.1611])\n",
      "Reward: tensor(-1.5537)\n",
      "Next State: tensor([ 1.5320, -0.1646,     nan, -1.0180, -1.3954,     nan, -1.2561,  0.0000])\n",
      "Done: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================\n",
    "# USAGE EXAMPLE\n",
    "# ===================\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = DiabetesDataset(csv_file=\"datasets/processed/563-test.csv\")\n",
    "    \n",
    "    # Peek at the first sample\n",
    "    sample = dataset[0]\n",
    "    print(\"State:\", sample[\"state\"])\n",
    "    print(\"Action:\", sample[\"action\"])\n",
    "    print(\"Reward:\", sample[\"reward\"])\n",
    "    print(\"Next State:\", sample[\"next_state\"])\n",
    "    print(\"Done:\", sample[\"done\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 5.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/563-test.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    for states, actions, rewards, next_states, dones in dataloader:\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device).unsqueeze(1)\n",
    "        \n",
    "        # Critic loss (CQL + TD error)\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1(torch.cat([next_states, next_actions], 1))\n",
    "            q2_next = model.q2(torch.cat([next_states, next_actions], 1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q1 = model.q1(torch.cat([states, actions], 1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], 1))\n",
    "        \n",
    "        # TD loss\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "        \n",
    "        # CQL penalty: logsumexp(Q(s, a')) - Q(s, a)\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1  # Random actions in [-1, 1]\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], 1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], 1))\n",
    "        cql_penalty = (\n",
    "            torch.logsumexp(torch.cat([q1_rand, q2_rand], 1), dim=1).mean() -\n",
    "            (current_q1.mean() + current_q2.mean()) / 2\n",
    "        )\n",
    "        \n",
    "        # Total critic loss\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "        \n",
    "        # Actor loss (maximize Q-value + entropy)\n",
    "        pred_actions = model.actor(states)\n",
    "        q1_pred = model.q1(torch.cat([states, pred_actions], 1))\n",
    "        q2_pred = model.q2(torch.cat([states, pred_actions], 1))\n",
    "        actor_loss = -torch.min(q1_pred, q2_pred).mean()\n",
    "        \n",
    "        # Update critic\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "        \n",
    "        # Update actor\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
