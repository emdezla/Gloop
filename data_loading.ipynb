{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Load CSV data\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.df = self.df.ffill().bfill()\n",
    "        # Extract state features: 8 dimensions\n",
    "        # [glu, glu_d, glu_t, hr, hr_d, hr_t, iob, hour_norm]\n",
    "        self.states = self.df[[\n",
    "            \"glu\", \"glu_d\", \"glu_t\",\n",
    "            \"hr\", \"hr_d\", \"hr_t\",\n",
    "            \"iob\", \"hour\"\n",
    "        ]].values.astype(np.float32)\n",
    "        \n",
    "        # Extract action features: 2 dimensions [basal, bol]\n",
    "        self.actions = self.df[[\"basal\", \"bolus\"]].values.astype(np.float32)\n",
    "        \n",
    "        # Extract done flags (1 at episode boundaries, 0 otherwise)\n",
    "        self.dones = self.df[\"done\"].values.astype(np.float32)\n",
    "       \n",
    "        glucose_next = self.df[\"glu_raw\"].values[1:]  # g_{t+1}\n",
    "        glucose_next_tensor = torch.tensor(glucose_next, dtype=torch.float32)\n",
    "        self.rewards = compute_reward_torch(glucose_next_tensor) / 15.0\n",
    "\n",
    "        \n",
    "        # Compute next_states using a vectorized roll\n",
    "        self.next_states = np.roll(self.states, shift=-1, axis=0)\n",
    "        \n",
    "        # For transitions where the current step is an episode end,\n",
    "        # set the next state to be the current state so that transitions do not cross episodes.\n",
    "        self.next_states[self.dones == 1] = self.states[self.dones == 1]\n",
    "        \n",
    "        # Remove the final row since it doesn't have a valid next state\n",
    "        self.states = self.states[:-1]\n",
    "        self.actions = self.actions[:-1]\n",
    "        self.rewards = self.rewards[:-1]\n",
    "        self.next_states = self.next_states[:-1]\n",
    "        self.dones = self.dones[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a dictionary for the transition (s, a, r, s', done)\n",
    "        assert idx < len(self.states), f\"Index {idx} out of bounds for dataset of size {len(self.states)}\"\n",
    "        return {\n",
    "            \"state\":      torch.tensor(self.states[idx],      dtype=torch.float32),\n",
    "            \"action\":     torch.tensor(self.actions[idx],     dtype=torch.float32),\n",
    "            \"reward\":     torch.tensor(self.rewards[idx],     dtype=torch.float32),\n",
    "            \"next_state\": torch.tensor(self.next_states[idx], dtype=torch.float32),\n",
    "            \"done\":       torch.tensor(self.dones[idx],       dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_torch(glucose_next):\n",
    "    \"\"\"\n",
    "    Compute RI-based reward in PyTorch.\n",
    "    \"\"\"\n",
    "    glucose_next = torch.clamp(glucose_next, min=1e-6)\n",
    "    log_term = torch.log(glucose_next) ** 1.084\n",
    "    f = 1.509 * (log_term - 5.381)\n",
    "    ri = 10 * f ** 2\n",
    "\n",
    "    reward = -torch.clamp(ri / 100.0, 0, 1)\n",
    "    reward[glucose_next <= 39.0] = -15.0\n",
    "    return reward\n",
    "\n",
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 5.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/559-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State: [current_glucose, glucose_trend, heart_rate, heart_rate_trend, insulin_on_board]\n",
    "state_dim = 8\n",
    "action_dim = 2  # Continuous insulin dose (steps of 0.05 units)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.2  # Entropy coefficient\n",
    "cql_weight = 5.0  # CQL penalty strength\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset  = DiabetesDataset(csv_file=\"datasets/processed/559-train.csv\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8526,  0.0000])\n",
      "Action: tensor([-1.0147, -0.0992])\n",
      "Reward: tensor(-0.0003)\n",
      "Next State: tensor([-0.9303, -0.4338,  1.1159,  1.8670, -3.1943, -1.2770, -1.8258,  0.0000])\n",
      "Done: tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\AppData\\Local\\Temp\\ipykernel_15488\\2852020829.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"reward\":     torch.tensor(self.rewards[idx],     dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "print(\"State:\", sample[\"state\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"reward\"])\n",
    "print(\"Next State:\", sample[\"next_state\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCQL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] (rescale to insulin range)\n",
    "        )\n",
    "        \n",
    "        # Critic networks (twin Q-functions)\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.q1_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ) # Same as q1\n",
    "        self.q2_target = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )  # Same as q2\n",
    "        \n",
    "        # Initialize targets to match main critics\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def update_targets(self, tau=0.005):\n",
    "        # Soft update: target = tau * main + (1-tau) * target\n",
    "        with torch.no_grad():\n",
    "            for t, m in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "            for t, m in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                t.data.copy_(tau * m.data + (1 - tau) * t.data)\n",
    "\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "model = SACCQL().to(device)\n",
    "optimizer_actor = optim.Adam(model.actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(list(model.q1.parameters()) + list(model.q2.parameters()), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "print_interval = 100\n",
    "\n",
    "def debug_tensor(tensor, name=\"\", check_grad=False, threshold=1e6):\n",
    "    \"\"\"\n",
    "    Prints diagnostic information about a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): The tensor to check.\n",
    "        name (str): Optional name for logging.\n",
    "        check_grad (bool): Also check gradients if available.\n",
    "        threshold (float): Warn if values exceed this.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t_min = tensor.min().item()\n",
    "        t_max = tensor.max().item()\n",
    "        t_mean = tensor.mean().item()\n",
    "        t_std = tensor.std().item()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not extract stats for {name}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ§ª [{name}] Shape: {tuple(tensor.shape)} | min: {t_min:.4f}, max: {t_max:.4f}, mean: {t_mean:.4f}, std: {t_std:.4f}\")\n",
    "\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"âŒ NaNs detected in {name}\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"âŒ Infs detected in {name}\")\n",
    "    if abs(t_min) > threshold or abs(t_max) > threshold:\n",
    "        print(f\"âš ï¸ Extreme values detected in {name}: values exceed Â±{threshold}\")\n",
    "\n",
    "    if check_grad and tensor.requires_grad and tensor.grad is not None:\n",
    "        grad = tensor.grad\n",
    "        print(f\"ðŸ” [{name}.grad] norm: {grad.norm().item():.4f}\")\n",
    "        if torch.isnan(grad).any():\n",
    "            print(f\"âŒ NaNs in gradient of {name}\")\n",
    "        if torch.isinf(grad).any():\n",
    "            print(f\"âŒ Infs in gradient of {name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # ðŸ§ª Check raw inputs\n",
    "        debug_tensor(states, \"states\")\n",
    "        debug_tensor(actions, \"actions\")\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------------\n",
    "        # Critic Loss (CQL + TD error)\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            # Use TARGET critics for Q_next\n",
    "            debug_tensor(next_actions, \"next_actions\")\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            target_q = torch.clamp(target_q, min=-200, max=0)\n",
    "\n",
    "        current_q1 = model.q1(torch.cat([states, actions], dim=1))\n",
    "        current_q2 = model.q2(torch.cat([states, actions], dim=1))\n",
    "\n",
    "        debug_tensor(current_q1, \"current_q1\")\n",
    "        debug_tensor(current_q2, \"current_q2\")\n",
    "\n",
    "        td_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n",
    "\n",
    "        # -----------------------------\n",
    "        # CQL Penalty\n",
    "        # -----------------------------\n",
    "        random_actions = torch.rand_like(actions) * 2 - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = torch.clamp(q_rand, min=-100, max=100)\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - (current_q1.mean() + current_q2.mean()) / 2\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        debug_tensor(critic_loss, \"critic_loss\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Actor Loss\n",
    "        # -----------------------------\n",
    "        states_actor = states.clone().detach()# prevent in-place gradient conflict\n",
    "        pred_actions = model.actor(states_actor)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"âŒ NaNs in actor outputs! Fix this before training.\")\n",
    "            pred_actions = torch.nan_to_num(pred_actions.clone())  # âœ… clone before modifying\n",
    "        \"\"\"\n",
    "        \n",
    "        # ðŸ”’ Safe skip if NaNs\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"âŒ NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        debug_tensor(pred_actions, \"pred_actions\")\n",
    "        \n",
    "\n",
    "        sa = torch.cat([states_actor, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa)\n",
    "        q2_pred = model.q2(sa)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    " \n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        debug_tensor(actor_loss, \"actor_loss\")\n",
    "\n",
    "        if torch.isnan(actor_loss).any():\n",
    "            print(\"âŒ Skipping batch due to NaN in actor loss.\")\n",
    "            continue\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # After critic and actor updates:\n",
    "        model.update_targets()  # Add this line\n",
    "\n",
    "        # -----------------------------\n",
    "        # Print Stats\n",
    "        # -----------------------------\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(f\"Current Q1: min={current_q1.min().item():.2f}, max={current_q1.max().item():.2f}, mean={current_q1.mean().item():.2f}\")\n",
    "            print(f\"Current Q2: min={current_q2.min().item():.2f}, max={current_q2.max().item():.2f}, mean={current_q2.mean().item():.2f}\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emili\\AppData\\Local\\Temp\\ipykernel_15488\\2852020829.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"reward\":     torch.tensor(self.rewards[idx],     dtype=torch.float32),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m current_q2 = model.q2(sa)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2a. TD Loss (MSE between current Q and target Q)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m td_loss = \u001b[43mF\u001b[49m.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 2b. CQL penalty\u001b[39;00m\n\u001b[32m     34\u001b[39m random_actions = (torch.rand_like(actions) * \u001b[32m2\u001b[39m) - \u001b[32m1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Move batch data to device\n",
    "        states = batch[\"state\"].to(device)\n",
    "        actions = batch[\"action\"].to(device)\n",
    "        rewards = batch[\"reward\"].to(device).unsqueeze(1)\n",
    "        next_states = batch[\"next_state\"].to(device)\n",
    "        dones = batch[\"done\"].to(device).unsqueeze(1)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. Compute target Q for Critic\n",
    "        # -----------------------------\n",
    "        with torch.no_grad():\n",
    "            next_actions = model.actor(next_states)\n",
    "            q1_next = model.q1_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q2_next = model.q2_target(torch.cat([next_states, next_actions], dim=1))\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * q_next\n",
    "            # clamp is fine, but do it as a separate (non-inplace) assignment\n",
    "            target_q = target_q.clamp(-200, 0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. Critic forward pass\n",
    "        # -----------------------------\n",
    "        sa = torch.cat([states, actions], dim=1)\n",
    "        current_q1 = model.q1(sa)\n",
    "        current_q2 = model.q2(sa)\n",
    "\n",
    "        # 2a. TD Loss (MSE between current Q and target Q)\n",
    "        td_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 2b. CQL penalty\n",
    "        random_actions = (torch.rand_like(actions) * 2) - 1\n",
    "        q1_rand = model.q1(torch.cat([states, random_actions], dim=1))\n",
    "        q2_rand = model.q2(torch.cat([states, random_actions], dim=1))\n",
    "        q_rand = torch.cat([q1_rand, q2_rand], dim=1)\n",
    "        q_rand_clamped = q_rand.clamp(-100, 100)  # not in-place\n",
    "        logsumexp_val = torch.logsumexp(q_rand_clamped, dim=1).mean()\n",
    "        cql_penalty = logsumexp_val - 0.5*(current_q1.mean() + current_q2.mean())\n",
    "\n",
    "        critic_loss = td_loss + cql_weight * cql_penalty\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. Update Critic\n",
    "        # -----------------------------\n",
    "        optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4. Actor forward pass\n",
    "        # -----------------------------\n",
    "        # For actor training, we want the Q-value for the action predicted by the current actor\n",
    "        pred_actions = model.actor(states)  # no .clone() or .detach()\n",
    "        if torch.isnan(pred_actions).any():\n",
    "            print(\"NaNs in actor output, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        sa_actor = torch.cat([states, pred_actions], dim=1)\n",
    "        q1_pred = model.q1(sa_actor)\n",
    "        q2_pred = model.q2(sa_actor)\n",
    "        q_pred = torch.min(q1_pred, q2_pred)\n",
    "\n",
    "        actor_loss = -q_pred.mean()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5. Update Actor\n",
    "        # -----------------------------\n",
    "        optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer_actor.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6. Soft-update target networks\n",
    "        # -----------------------------\n",
    "        model.update_targets()\n",
    "\n",
    "        # Print stats\n",
    "        if i % print_interval == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {i}\")\n",
    "            print(f\"TD Loss: {td_loss.item():.4f}, CQL Penalty: {cql_penalty.item():.4f}\")\n",
    "            print(f\"Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "            print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
